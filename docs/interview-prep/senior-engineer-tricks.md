# ğŸ¯ Senior Engineer's Toolbox: Advanced System Design Tricks & Techniques

## The Master Arsenal: Beyond Basic Scale Estimation

This guide compiles **battle-tested mental models, heuristics, and frameworks** that distinguish senior engineers in system design interviews. These are the "invisible tools" that enable rapid, confident decision-making at scale.

---

## ğŸ“š Table of Contents

1. [Mental Models & Frameworks](#mental-models--frameworks)
2. [The Rule of Thumb Library](#the-rule-of-thumb-library)
3. [Pattern Recognition Shortcuts](#pattern-recognition-shortcuts)
4. [CAP Theorem Decision Trees](#cap-theorem-decision-trees)
5. [Database Selection Matrix](#database-selection-matrix)
6. [Caching Strategies Playbook](#caching-strategies-playbook)
7. [The Bottleneck Identification Framework](#the-bottleneck-identification-framework)
8. [Trade-off Communication Templates](#trade-off-communication-templates)
9. [The "What Could Go Wrong?" Checklist](#the-what-could-go-wrong-checklist)
10. [Performance Numbers Every Engineer Should Know](#performance-numbers-every-engineer-should-know)

---

## 1. Mental Models & Frameworks

### ğŸ¯ The STAR Framework (System Thinking Architectural Reasoning)

**S** - Start with the **Story** (User Journey)  
**T** - Think in **Tiers** (Presentation â†’ Logic â†’ Data)  
**A** - Anticipate **Anomalies** (Edge cases & failures)  
**R** - Reason about **Resources** (Bottlenecks & constraints)

**Example Application**:
```
Problem: Design Instagram

S - STORY:
   User uploads photo â†’ Followers see it in feed â†’ Users like/comment
   
T - TIERS:
   - Mobile App (Presentation)
   - API Gateway â†’ Services (Logic)
   - Object Storage + Database (Data)
   
A - ANOMALIES:
   - What if upload fails mid-way?
   - What if user has 10M followers?
   - What if celebrity posts (thundering herd)?
   
R - RESOURCES:
   - Storage bottleneck: Media files (PB scale)
   - Network bottleneck: Image delivery (use CDN)
   - Database bottleneck: Feed generation (pre-compute)
```

---

### ğŸ§  The "Think in Layers" Model

**Always decompose systems into these 7 layers:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. CLIENT LAYER (Web/Mobile/IoT)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  2. EDGE LAYER (CDN, Edge Compute)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  3. GATEWAY LAYER (LB, API Gateway)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  4. SERVICE LAYER (Business Logic)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  5. CACHE LAYER (Redis, Memcached)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  6. DATA LAYER (Databases, Storage)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  7. INFRASTRUCTURE (Monitoring, Logging)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pro Tip**: Walk through each layer systematically. Never skip layers in your explanation!

---

### ğŸª The "Theater Model" for Distributed Systems

Think of distributed systems like a theater production:

```
ğŸ­ THEATER ANALOGY â†’ DISTRIBUTED SYSTEM

Box Office     â†’ API Gateway (single entry point)
Ushers         â†’ Load Balancers (distribute audience)
Multiple Shows â†’ Microservices (independent performances)
Understudies   â†’ Redundancy/Failover
Stage Manager  â†’ Orchestrator (Kubernetes)
Scripts        â†’ Contracts/APIs
Reviews        â†’ Monitoring & Metrics
```

**Memory Hook**: "Every distributed system is a theater - plan for the show to go on even when actors are sick!"

---

## 2. The Rule of Thumb Library

### ğŸ”¢ The "Powers of 2 & 3" Shortcut

Memorize these exact numbers - they appear EVERYWHERE:

```
POWERS OF 2 (Storage & Network):
2^10 = 1,024      â‰ˆ 1 KB      â†’ "Network packet size"
2^16 = 65,536     â‰ˆ 64 KB     â†’ "TCP window size"
2^20 = 1,048,576  â‰ˆ 1 MB      â†’ "Small file size"
2^30 = 1 GB                   â†’ "RAM per process"
2^32 = 4 GB                   â†’ "32-bit address limit"
2^40 = 1 TB                   â†’ "Disk size"

POWERS OF 3 (Time & Latency):
10^3 = 1 ms       â†’ Database query
10^6 = 1 second   â†’ User patience threshold
10^9 = 15 minutes â†’ Cache TTL sweet spot
```

---

### âš¡ The "Latency Numbers Ladder"

**Memorize this progression** (Google's famous latency numbers, updated):

```
Operation                           Time        Mental Model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L1 cache reference                  0.5 ns      Instant
Branch mispredict                   5 ns        Instant
L2 cache reference                  7 ns        Instant
Mutex lock/unlock                   25 ns       Instant
Main memory reference               100 ns      "RAM speed"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SSD random read                     150 Î¼s      ğŸ‘ˆ 1,000x slower!
Read 1 MB sequentially (SSD)        1 ms        
Disk seek (HDD)                     10 ms       ğŸ‘ˆ 100x slower than SSD!
Read 1 MB sequentially (HDD)        20 ms       
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Send packet CA â†’ Netherlands        150 ms      "Transatlantic hop"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ¯ KEY INSIGHT: Each tier is ~100-1000x slower than the previous!
```

**Interview Gold**: When discussing performance, reference these numbers!
```
"Since disk seeks are 10ms, we can handle ~100 seeks/sec per disk.
 For 10K IOPS, we'd need 100 disks OR move to SSD which gives
 ~10K IOPS per drive."
```

---

### ğŸ“Š The "Server Capacity Rule Book"

**What can ONE server handle?** (Baseline: 16 core, 64GB RAM, SSD)

```
COMPUTE LIMITS:
â€¢ CPU-bound tasks:        ~10K ops/sec  (image resize, encryption)
â€¢ Memory ops:             ~1M ops/sec   (simple calculations)
â€¢ Network I/O:            ~10K conn     (C10K problem solved)

STORAGE LIMITS:
â€¢ SSD IOPS:               ~10K IOPS     (random reads)
â€¢ SSD Throughput:         ~500 MB/s     (sequential)
â€¢ HDD IOPS:               ~100 IOPS     (random reads)
â€¢ HDD Throughput:         ~100 MB/s     (sequential)

DATABASE LIMITS (single instance):
â€¢ MySQL:                  ~5K QPS       (read-heavy with cache)
â€¢ PostgreSQL:             ~3K QPS       (write-heavy)
â€¢ MongoDB:                ~10K QPS      (simple queries)
â€¢ Redis:                  ~100K QPS     (in-memory)

NETWORK LIMITS:
â€¢ 1 Gbps NIC:             ~125 MB/s     (theoretical)
â€¢ Realistic throughput:   ~80 MB/s      (TCP overhead)
â€¢ Websocket connections:  ~10K conns    (per server)
```

**Usage**: "Since we need 50K QPS and one server handles 5K, we need **10 servers**."

---

### ğŸ² The "80-20-5-1" Cascade Rule

Beyond basic Pareto, there's a **cascade effect**:

```
100% of data/traffic breaks down as:
â”‚
â”œâ”€ 20% â†’ generates 80% of traffic    (HOT - Redis cache)
â”‚  â””â”€ 5% â†’ generates 64% of traffic  (SUPERHOT - In-memory)
â”‚     â””â”€ 1% â†’ generates 51% traffic  (NUCLEAR - Pre-fetch)
â”‚
â””â”€ 80% â†’ generates 20% of traffic    (COLD - Database)

ğŸ¯ STRATEGY:
â€¢ L1 Cache: Top 1% (pre-computed, edge cache)
â€¢ L2 Cache: Top 5% (Redis cluster)
â€¢ L3 Cache: Top 20% (CDN)
â€¢ Database: Everything (cold storage)
```

**Interview Application**:
```
"For 10M URLs, we don't cache all of them:
 - Top 1% (100K URLs) â†’ Edge cache â†’ Handles 50% of traffic
 - Top 5% (500K URLs) â†’ Redis â†’ Handles 64% of traffic
 - Top 20% (2M URLs) â†’ CDN â†’ Handles 80% of traffic
 - Rest (8M URLs) â†’ Database â†’ Handles 20% of traffic"
```

---

### ğŸ”„ The "N+2 Redundancy Rule"

**Never use N+1 (it's a trap!)**

```
âŒ BAD: N+1 Redundancy
   - Need 10 servers for load
   - Have 11 servers (1 backup)
   - Problem: During deployment, take down 1 â†’ backup kicks in
     If another fails during deployment â†’ OUTAGE!

âœ… GOOD: N+2 Redundancy
   - Need 10 servers for load
   - Have 12 servers (2 backup)
   - Scenario: Deploy to 1, it fails, another has bug â†’ Still OK!

ğŸ¯ RULE: Always have 2 extra for:
   â€¢ Deployments
   â€¢ Unexpected failures
   â€¢ Maintenance windows
```

---

## 3. Pattern Recognition Shortcuts

### ğŸ¨ The "System Design Pattern Matcher"

**Instantly match requirements to patterns:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ REQUIREMENT                    â†’ PATTERN                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ "Real-time updates"            â†’ WebSockets + Pub/Sub       â”‚
â”‚ "High write throughput"        â†’ Message Queue + Workers    â”‚
â”‚ "Global low latency"           â†’ Multi-region + CDN         â”‚
â”‚ "Search functionality"         â†’ Elasticsearch/Solr         â”‚
â”‚ "Analytics/Reporting"          â†’ Data Warehouse (Redshift)  â”‚
â”‚ "File uploads"                 â†’ Object Storage (S3)        â”‚
â”‚ "User sessions"                â†’ Redis/Memcached            â”‚
â”‚ "Geo-location queries"         â†’ Geospatial DB (PostGIS)    â”‚
â”‚ "Time-series data"             â†’ InfluxDB/TimescaleDB       â”‚
â”‚ "Video streaming"              â†’ CDN + Adaptive Bitrate     â”‚
â”‚ "Notifications"                â†’ FCM/SNS + Queue            â”‚
â”‚ "Rate limiting"                â†’ Token Bucket + Redis       â”‚
â”‚ "Idempotency"                  â†’ Unique ID + Dedup table    â”‚
â”‚ "Audit logs"                   â†’ Event Sourcing             â”‚
â”‚ "Eventually consistent OK"     â†’ NoSQL (Cassandra, DynamoDB)â”‚
â”‚ "Strong consistency required"  â†’ SQL (PostgreSQL, MySQL)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Pro Move**: When you hear a requirement, immediately verbalize the pattern:
```
Interviewer: "Users need real-time notifications"
You: "Got it - I'm thinking WebSockets for active users, and 
      a push notification service like FCM for offline users,
      backed by a pub/sub system like Kafka."
```

---

### ğŸ” The "Functional vs Non-Functional Decoder"

**Instantly categorize requirements:**

```
FUNCTIONAL (What it does):          NON-FUNCTIONAL (How well it does it):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Create short URL               â†’  - Handle 10K QPS
- Upload photo                   â†’  - 99.99% availability
- Send message                   â†’  - <100ms latency
- Search products               â†’  - Scale to 1B users
- Process payment               â†’  - ACID compliance
                                    - PCI compliance
                                    - Disaster recovery
```

**Framework**: "Let me separate these into what we'll build (functional) and how we'll build it (non-functional)."

---

### ğŸ§© The "Microservice Boundary Heuristic"

**When to split a service?** Use the **DICE** test:

```
D - Data Model is fundamentally different
I - Independent scaling needs
C - Can deploy separately without coordination
E - Expert team knowledge separation

Example: E-commerce platform

âŒ Don't split:
   - ProductService â†’ ProductListService + ProductDetailService
     (Same data model, same scaling, tight coupling)

âœ… Do split:
   - ProductService (catalog data, read-heavy)
   - InventoryService (stock levels, write-heavy, real-time)
   - PricingService (dynamic pricing, complex algorithms)
   - RecommendationService (ML models, different tech stack)

All pass DICE test!
```

---

## 4. CAP Theorem Decision Trees

### ğŸ¯ The "CAP Picker" Flow Chart

```
START: What's your priority?
â”‚
â”œâ”€ STRONG CONSISTENCY required? (Banking, inventory)
â”‚  â”‚
â”‚  â”œâ”€ Can tolerate downtime during partition?
â”‚  â”‚  â””â”€â†’ CP System (Consistency + Partition Tolerance)
â”‚  â”‚      Examples: MongoDB, HBase, Redis (single master)
â”‚  â”‚      Trade-off: Unavailable during network splits
â”‚  â”‚
â”‚  â””â”€ Cannot tolerate any downtime?
â”‚     â””â”€â†’ CA System (Consistency + Availability)
â”‚         Examples: Traditional RDBMS (single node)
â”‚         Trade-off: Not partition tolerant (doesn't scale)
â”‚
â””â”€ AVAILABILITY required? (Social media, content delivery)
   â”‚
   â””â”€ Can tolerate eventual consistency?
      â””â”€â†’ AP System (Availability + Partition Tolerance)
          Examples: Cassandra, DynamoDB, Riak
          Trade-off: Stale reads possible, conflict resolution

ğŸ¯ REAL WORLD: Pick CP or AP (Network partitions happen!)
```

### ğŸ’¡ The "Consistency Spectrum Selector"

```
STRONG â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ WEAK
â”‚         â”‚              â”‚            â”‚           â”‚
â”‚         â”‚              â”‚            â”‚           â”‚
Linearizable  Sequential   Causal   Eventual   Best Effort
â”‚         â”‚              â”‚            â”‚           â”‚
â”‚         â”‚              â”‚            â”‚           â””â”€â†’ Analytics dashboards
â”‚         â”‚              â”‚            â””â”€â†’ Social media likes
â”‚         â”‚              â””â”€â†’ Chat messages (ordered)
â”‚         â””â”€â†’ Bank account balance
â””â”€â†’ Stock trading, inventory reservation

ğŸ¯ TRICK: Start from the requirement, then pick the weakest
           consistency that satisfies it (better performance!)

Example:
"Do we need real-time inventory accuracy?"
- Yes â†’ Strong consistency (PostgreSQL)
- No, 5 min stale OK â†’ Eventual (DynamoDB with cache)
```

---

## 5. Database Selection Matrix

### ğŸ—„ï¸ The "Database Decision Tree"

```
                    START
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚
    Structured                  Unstructured/
    Relations?                  Flexible Schema?
        â”‚                           â”‚
        â†“                           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”                   NoSQL Path
    â”‚  SQL  â”‚                       â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”˜         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚             â”‚
  High writes?    Key-Value?    Document?     Wide Column?
        â”‚             â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â†“             â†“             â†“
  Yes â”‚   Noâ”‚       Redis        MongoDB      Cassandra
    â”‚     â”‚       Memcached      CouchDB       HBase
    â†“     â†“        DynamoDB
Distributed  Single
    â”‚        â”‚
PostgreSQL  PostgreSQL
 (Citus)    MySQL
Vitess      SQLite
```

### ğŸ“‹ The "Database Characteristics Cheat Sheet"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Database   â”‚ Reads/s  â”‚ Writes/s â”‚ Use Case â”‚ CAP Model  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Redis      â”‚ 100K     â”‚ 100K     â”‚ Cache    â”‚ CP         â”‚
â”‚ PostgreSQL â”‚ 5K       â”‚ 2K       â”‚ OLTP     â”‚ CA/CP      â”‚
â”‚ MySQL      â”‚ 5K       â”‚ 3K       â”‚ OLTP     â”‚ CA/CP      â”‚
â”‚ MongoDB    â”‚ 10K      â”‚ 5K       â”‚ Documentsâ”‚ CP         â”‚
â”‚ Cassandra  â”‚ 50K      â”‚ 50K      â”‚ Time-ser â”‚ AP         â”‚
â”‚ DynamoDB   â”‚ 100K     â”‚ 100K     â”‚ Key-Val  â”‚ AP         â”‚
â”‚ Elasticsearchâ”‚20K     â”‚ 10K      â”‚ Search   â”‚ AP         â”‚
â”‚ Neo4j      â”‚ 5K       â”‚ 2K       â”‚ Graphs   â”‚ CA         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

* Numbers are per-node approximations for typical workloads
```

### ğŸª The "Polyglot Persistence Pattern"

**Use MULTIPLE databases for different needs:**

```
E-commerce Example:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Type        â†’ Database Choice â†’ Reason              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ User accounts    â†’ PostgreSQL     â†’ ACID, relations      â”‚
â”‚ Product catalog  â†’ Elasticsearch  â†’ Full-text search     â”‚
â”‚ Shopping cart    â†’ Redis           â†’ Fast, ephemeral     â”‚
â”‚ Order history    â†’ PostgreSQL     â†’ Transactions         â”‚
â”‚ Product images   â†’ S3              â†’ Object storage      â”‚
â”‚ Click events     â†’ Kafka           â†’ Event streaming     â”‚
â”‚ Analytics        â†’ Redshift        â†’ Data warehouse      â”‚
â”‚ Session data     â†’ DynamoDB        â†’ High throughput     â”‚
â”‚ Recommendations  â†’ Neo4j           â†’ Graph relations     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ KEY INSIGHT: Don't force one database for everything!
```

---

## 6. Caching Strategies Playbook

### ğŸ”¥ The "Cache Strategy Decision Matrix"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Strategy        â”‚ When to Use  â”‚ Pros         â”‚ Cons        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cache-Aside     â”‚ Read-heavy   â”‚ Simple       â”‚ Cache miss  â”‚
â”‚ (Lazy Loading)  â”‚ General use  â”‚ Fault-tol.   â”‚ penalty     â”‚
â”‚                 â”‚              â”‚              â”‚             â”‚
â”‚ Read-Through    â”‚ Read-heavy   â”‚ Transparent  â”‚ Complex     â”‚
â”‚                 â”‚ Consistency  â”‚ Auto-load    â”‚             â”‚
â”‚                 â”‚              â”‚              â”‚             â”‚
â”‚ Write-Through   â”‚ Consistency  â”‚ No stale     â”‚ Write       â”‚
â”‚                 â”‚ critical     â”‚ data         â”‚ latency     â”‚
â”‚                 â”‚              â”‚              â”‚             â”‚
â”‚ Write-Behind    â”‚ Write-heavy  â”‚ Fast writes  â”‚ Data loss   â”‚
â”‚ (Write-Back)    â”‚ Log/metrics  â”‚ Batch DB     â”‚ risk        â”‚
â”‚                 â”‚              â”‚              â”‚             â”‚
â”‚ Refresh-Ahead   â”‚ Predictable  â”‚ No miss      â”‚ Wasted if   â”‚
â”‚                 â”‚ access       â”‚ penalty      â”‚ unused      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ’ The "Cache Invalidation Hierarchy"

**"There are only two hard things in Computer Science: cache invalidation and naming things."**

```
LEVEL 1: TTL (Time To Live)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Use for: Data that changes predictably
âŒ Avoid for: Real-time critical data

Example: Product prices
Cache.set("product:123", data, TTL=300) // 5 minutes

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LEVEL 2: Event-Based Invalidation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Use for: Data with known mutation points
âŒ Avoid for: High-frequency updates

Example: User profile
onUserUpdate(userId) {
  cache.delete("user:" + userId)
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LEVEL 3: Write-Through + Version Tagging
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Use for: Strong consistency needed
âŒ Avoid for: Performance-critical writes

Example: Bank balance
update(balance) {
  db.write(balance)
  cache.set("balance", balance, version++)
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

LEVEL 4: Eventual Consistency + Conflict Resolution
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… Use for: Distributed systems, AP systems
âŒ Avoid for: Financial transactions

Example: Social media post likes
// Different regions may have different counts
// Resolve using Last-Write-Wins or merge
```

### ğŸ¯ The "Cache Hit Ratio Formula"

```
Target: 90%+ cache hit ratio

Formula:
Hit Ratio = Cache Hits / (Cache Hits + Cache Misses)

Optimization Tricks:

1. Pre-warm Cache (Refresh-Ahead)
   - Load popular items at startup
   - Background jobs refresh before expiry

2. Probabilistic Early Expiration
   - Refresh before TTL based on traffic
   expireTime = TTL * (1 - beta * log(random()))

3. Bloom Filters for Negative Caching
   - Avoid DB queries for non-existent items
   if (!bloomFilter.contains(key)) {
     return null; // Don't even check cache/DB
   }

4. Hierarchical Caching (L1 â†’ L2 â†’ L3)
   L1: In-memory map (10ms)
   L2: Redis (100ms)
   L3: Database (10s)
```

---

## 7. The Bottleneck Identification Framework

### ğŸ” The "CRUD Performance Matrix"

**Every system bottlenecks on ONE of these:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Resource â”‚ Symptoms        â”‚ Solution        â”‚ Scaling      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CPU      â”‚ High CPU %      â”‚ - Optimize algo â”‚ Horizontal   â”‚
â”‚          â”‚ Slow processing â”‚ - Async jobs    â”‚ (add servers)â”‚
â”‚          â”‚                 â”‚ - Caching       â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memory   â”‚ OOM errors      â”‚ - Pagination    â”‚ Vertical     â”‚
â”‚          â”‚ High swap usage â”‚ - Streaming     â”‚ (bigger RAM) â”‚
â”‚          â”‚                 â”‚ - Chunking      â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Disk I/O â”‚ High iowait     â”‚ - SSD upgrade   â”‚ Sharding     â”‚
â”‚          â”‚ Slow queries    â”‚ - Indexing      â”‚ Read replicasâ”‚
â”‚          â”‚                 â”‚ - Denormalize   â”‚              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Network  â”‚ Timeouts        â”‚ - CDN           â”‚ Multi-region â”‚
â”‚          â”‚ High latency    â”‚ - Compression   â”‚ Edge compute â”‚
â”‚          â”‚                 â”‚ - Protocol opt  â”‚              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸª The "Little's Law" for Queue Analysis

**Mathematical prediction of bottlenecks:**

```
Little's Law: L = Î» Ã— W

L = Average number of items in system
Î» = Arrival rate (requests/sec)
W = Average time in system (latency)

Example:
- 1000 requests/sec arriving (Î»)
- Each request takes 0.5 sec (W)
- L = 1000 Ã— 0.5 = 500 concurrent requests

ğŸ¯ If your system can only handle 200 concurrent requests:
   â†’ BOTTLENECK! Queue builds up, latency increases!

Solution: Reduce W (faster processing) OR scale capacity
```

### ğŸ“Š The "Back-of-Envelope Bottleneck Calculator"

```
Given: 10,000 QPS, 100ms average latency

Step 1: Calculate concurrent requests
Concurrent = QPS Ã— Latency(sec)
           = 10,000 Ã— 0.1 = 1,000 concurrent

Step 2: Calculate required connections
Assuming 100 requests per connection:
Connections = 1,000 / 100 = 10 active connections

Step 3: Calculate memory needed
Per request: 10 KB
Total = 1,000 Ã— 10 KB = 10 MB (easily fits!)

Step 4: Calculate CPU needed
If each request uses 10ms CPU:
CPU = 10,000 Ã— 0.01 = 100 CPU-seconds/sec
    = Need 100 cores! â† BOTTLENECK FOUND!

Solution: Either optimize (reduce CPU/request) or scale to 100 cores
```

---

## 8. Trade-off Communication Templates

### ğŸ­ The "Trade-off Articulation Framework"

**How to discuss trade-offs like a senior engineer:**

```
TEMPLATE:
"If we choose [OPTION A], we get [BENEFIT], but we trade off [COST].
 Alternatively, [OPTION B] gives us [DIFFERENT BENEFIT] at the cost of
 [DIFFERENT COST]. Given our requirement for [KEY REQUIREMENT],
 I'd recommend [CHOICE] because [REASONING]."

EXAMPLE 1: SQL vs NoSQL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"If we choose PostgreSQL, we get ACID guarantees and strong consistency,
 but we trade off horizontal scalability and write throughput.
 Alternatively, Cassandra gives us massive write throughput and
 multi-datacenter replication at the cost of eventual consistency.
 Given our requirement for handling 100K writes/sec with eventual
 consistency being acceptable (social media likes), I'd recommend
 Cassandra because it naturally handles our write scale and
 geographic distribution needs."

EXAMPLE 2: Microservices vs Monolith
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"If we choose microservices, we get independent deployability and
 technology flexibility, but we trade off operational complexity and
 inter-service latency. Alternatively, a monolith gives us simplicity
 and transactional consistency at the cost of coupling and slower
 deployment velocity. Given we're a 5-person team building an MVP,
 I'd recommend starting with a modular monolith because it minimizes
 operational overhead while letting us move fast, with a clear
 migration path to microservices once we identify scaling bottlenecks."
```

### ğŸ’¡ The "Three Perspectives Technique"

**Always present options from three angles:**

```
TECHNICAL Perspective:
"From a performance standpoint..."

BUSINESS Perspective:
"From a cost and time-to-market perspective..."

OPERATIONAL Perspective:
"From a maintenance and monitoring perspective..."

EXAMPLE: Caching Strategy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Technical: "Cache-aside gives us 90% hit ratio with minimal complexity"
Business: "We can implement it in 2 days vs 2 weeks for write-through"
Operational: "Team already knows Redis, low learning curve"
```

---

## 9. The "What Could Go Wrong?" Checklist

### âš ï¸ The "Failure Modes Checklist" (SPOF â†’ Single Point of Failure)

```
For EVERY component, ask:

â–¡ What if this component crashes?
  â†’ Add redundancy (N+2 instances)

â–¡ What if this component slows down?
  â†’ Add timeouts, circuit breakers, backpressure

â–¡ What if this component fills up (disk/memory)?
  â†’ Add monitoring, auto-scaling, cleanup jobs

â–¡ What if the network between components fails?
  â†’ Add retries, message queues, async processing

â–¡ What if data gets corrupted?
  â†’ Add checksums, backup strategy, audit logs

â–¡ What if we get 10x traffic suddenly?
  â†’ Add rate limiting, auto-scaling, queue buffering

â–¡ What if a deployment introduces a bug?
  â†’ Add canary releases, feature flags, quick rollback

â–¡ What if a dependency (external API) fails?
  â†’ Add circuit breakers, fallback responses, caching

â–¡ What if we lose a datacenter?
  â†’ Multi-region setup, data replication, DNS failover

â–¡ What if there's a security breach?
  â†’ Encryption, authentication, authorization, audit logs
```

### ğŸ¯ The "Cascading Failure Prevention Pattern"

```
PROBLEM: One service fails â†’ All services fail (domino effect)

SOLUTION TOOLKIT:

1. CIRCUIT BREAKER
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ [Closed] â†’ Normal operation     â”‚
   â”‚ [Open]   â†’ Stop calling failed  â”‚
   â”‚ [Half]   â†’ Try again gradually  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   if (failureRate > 50% for 10 sec) {
     circuitBreaker.open()
     return fallbackResponse()
   }

2. BULKHEAD PATTERN
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Isolate thread pools per serviceâ”‚
   â”‚ - Service A: 20 threads         â”‚
   â”‚ - Service B: 20 threads         â”‚
   â”‚ If A fails, B still works!      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. TIMEOUTS
   // Never wait forever!
   request.timeout(3000) // 3 seconds max

4. RETRY WITH EXPONENTIAL BACKOFF
   retries = 0
   while (retries < MAX_RETRIES) {
     try {
       return makeRequest()
     } catch (error) {
       sleep(2^retries * 100) // 100ms, 200ms, 400ms...
       retries++
     }
   }

5. RATE LIMITING
   // Protect downstream services
   if (requestsPerSecond > LIMIT) {
     return 429_TOO_MANY_REQUESTS
   }
```

---

## 10. Performance Numbers Every Engineer Should Know

### âš¡ The "Napkin Math Cheat Sheet" (2024 Edition)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         ESSENTIAL PERFORMANCE NUMBERS (Memorize!)          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

LATENCY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
L1 cache reference                      0.5 ns
Branch mispredict                       5 ns
L2 cache reference                      7 ns
Mutex lock/unlock                       100 ns
Main memory reference                   100 ns
Compress 1KB with Snappy                10 Âµs
Send 2KB over 1 Gbps network            20 Âµs
Read 1 MB sequentially from memory      250 Âµs
Round trip within same datacenter       500 Âµs
Disk seek (SSD)                         150 Âµs
Read 1 MB sequentially from SSD         1 ms
Read 1 MB sequentially from disk        20 ms
Send packet CA â†’ Netherlands            150 ms

THROUGHPUT (What 1 server can handle):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Redis operations                        100,000 ops/sec
Memcached operations                    500,000 ops/sec
Nginx requests (static)                 50,000 req/sec
PostgreSQL reads (indexed)              10,000 QPS
PostgreSQL writes                       5,000 QPS
MySQL reads (indexed)                   12,000 QPS
Kafka messages                          1,000,000 msg/sec
RabbitMQ messages                       50,000 msg/sec
Elasticsearch queries                   20,000 QPS

NETWORK:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1 Gbps network                          125 MB/sec
10 Gbps network                         1.25 GB/sec
Typical AWS inter-region latency        50-150 ms
Typical AWS intra-region latency        <1 ms
HTTP/1.1 request overhead               ~1 KB
HTTP/2 request overhead                 ~100 bytes
WebSocket message overhead              ~10 bytes

STORAGE:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SSD random reads                        10,000 IOPS
SSD sequential reads                    500 MB/sec
HDD random reads                        100 IOPS
HDD sequential reads                    100 MB/sec
NVMe SSD reads                          100,000 IOPS
S3 GET request                          100-200 ms
S3 PUT request                          150-300 ms

COMPRESSION (1 MB data):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Snappy                                  10 ms (fast, lower ratio)
LZ4                                     15 ms (fast, decent ratio)
Gzip                                    50 ms (slower, better ratio)
Brotli                                  100 ms (slowest, best ratio)

SERIALIZATION (1 MB data):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
JSON parse/stringify                    10 ms
Protocol Buffers                        2 ms
MessagePack                             3 ms
Avro                                    2 ms
```

### ğŸ¯ The "Rule of Thumb Calculator"

```
QUICK CONVERSIONS:
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

"How many servers do I need?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Servers = (Required QPS) / (QPS per server) Ã— Safety Factor

Example: Need 50K QPS, Nginx does 25K QPS
Servers = 50K / 25K Ã— 2 = 4 servers (2x safety factor)

"How much bandwidth do I need?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Bandwidth = QPS Ã— Avg Response Size

Example: 10K QPS, 50 KB response
Bandwidth = 10K Ã— 50 KB = 500 MB/sec = 4 Gbps

"How much storage do I need?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Storage = Items Ã— Item Size Ã— Retention Days

Example: 1M items/day, 1 KB each, 365 days
Storage = 1M Ã— 1KB Ã— 365 = 365 GB â‰ˆ 0.5 TB

"How many DB connections?"
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Connections = (QPS Ã— Avg Query Time) / 1000

Example: 5K QPS, 20ms query time
Connections = 5K Ã— 0.02 = 100 connections

Rule: Keep pool at ~100-200 per app server
```

---

## ğŸ“ Bonus: The "Senior Engineer Mindset" Principles

### 1. **"Start Simple, Then Optimize"**

```
âŒ Junior: "Let's use Kubernetes, Kafka, and microservices!"
âœ… Senior: "Let's start with a monolith on EC2, measure bottlenecks,
           then split services where needed. We'll use Kafka when
           our current RabbitMQ hits its limit at 50K msg/sec."

MANTRA: "Make it work â†’ Make it right â†’ Make it fast"
```

### 2. **"Every Decision is a Trade-off"**

```
âŒ Junior: "NoSQL is better than SQL"
âœ… Senior: "NoSQL gives us horizontal scalability and schema flexibility,
           but we lose ACID guarantees and join capabilities. For our
           e-commerce orders, SQL's transactions are worth the trade-off.
           For our product catalog, NoSQL's flexibility wins."

MANTRA: "There are no solutions, only trade-offs" - Thomas Sowell
```

### 3. **"Question the Requirements"**

```
âŒ Junior: "They want 99.999% uptime, let me design for that"
âœ… Senior: "99.999% means 5 min downtime/year. That requires multi-region
           active-active with auto-failover, costing $500K/year vs $50K
           for 99.9%. Do we really need it? What's the business impact
           of 8 hours downtime/year?"

MANTRA: "The cheapest feature is the one you don't build"
```

### 4. **"Think in Probabilities, Not Absolutes"**

```
âŒ Junior: "This component never fails"
âœ… Senior: "With 1000 servers, each with 99.9% uptime, we'll have 1
           server down at any given time. Let's design for graceful
           degradation."

MANTRA: "Hope for the best, design for the worst"
```

### 5. **"Measure, Don't Guess"**

```
âŒ Junior: "Users will definitely want feature X"
âœ… Senior: "Let's A/B test with 5% of users, measure engagement,
           then decide. Metrics: DAU, retention, conversion."

MANTRA: "In God we trust, all others bring data" - W. Edwards Deming
```

---

## ğŸ“š Quick Reference: The "Interview Formula"

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        THE 45-MINUTE SYSTEM DESIGN INTERVIEW FLOW          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHASE 1 (5 min): Requirements & Scale                     â”‚
â”‚   âœ“ Clarify functional requirements                       â”‚
â”‚   âœ“ Clarify non-functional requirements                   â”‚
â”‚   âœ“ Do napkin math (QPS, storage, bandwidth)             â”‚
â”‚   âœ“ State assumptions clearly                             â”‚
â”‚                                                            â”‚
â”‚ PHASE 2 (5 min): High-Level Design                        â”‚
â”‚   âœ“ Draw boxes: Client â†’ API â†’ Service â†’ DB              â”‚
â”‚   âœ“ Identify major components                             â”‚
â”‚   âœ“ Explain data flow (write path & read path)           â”‚
â”‚   âœ“ Call out technologies (REST, Redis, PostgreSQL)      â”‚
â”‚                                                            â”‚
â”‚ PHASE 3 (20 min): Deep Dives                              â”‚
â”‚   âœ“ Database schema design                                â”‚
â”‚   âœ“ API design (2-3 key endpoints)                        â”‚
â”‚   âœ“ Caching strategy                                      â”‚
â”‚   âœ“ Scaling strategy (horizontal, vertical, sharding)    â”‚
â”‚   âœ“ Handle edge cases & failures                          â”‚
â”‚                                                            â”‚
â”‚ PHASE 4 (10 min): Trade-offs & Extensions                 â”‚
â”‚   âœ“ Discuss alternative approaches                        â”‚
â”‚   âœ“ Identify bottlenecks and solutions                    â”‚
â”‚   âœ“ Security considerations                               â”‚
â”‚   âœ“ Monitoring & alerting                                 â”‚
â”‚                                                            â”‚
â”‚ PHASE 5 (5 min): Wrap-up                                  â”‚
â”‚   âœ“ Summarize key decisions                               â”‚
â”‚   âœ“ Acknowledge limitations                               â”‚
â”‚   âœ“ Suggest future improvements                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Practice Exercises

### Exercise 1: Pattern Recognition

For each requirement, identify the appropriate pattern:

1. "System must handle 1M concurrent WebSocket connections"
2. "Users need to search products by text, price, and category"
3. "Video uploads up to 10 GB"
4. "Prevent duplicate payments if user clicks twice"
5. "Generate personalized news feed in <100ms"

<details>
<summary>Answers</summary>

1. **Pattern**: Distributed WebSocket servers + Pub/Sub (Redis/Kafka)
   - Use connection manager, stateless servers, message broker

2. **Pattern**: Elasticsearch with faceted search
   - Index products, use filters for price/category, full-text search

3. **Pattern**: Direct upload to S3 (presigned URLs) + async processing
   - Client uploads to S3, webhook triggers processing pipeline

4. **Pattern**: Idempotency key + deduplication table
   - Generate unique request ID, store in Redis/DB with TTL

5. **Pattern**: Pre-computed feeds + cache (Redis) + fan-out on write
   - Background job generates feeds, cache hot users, paginate results

</details>

---

### Exercise 2: Bottleneck Identification

Given: URL shortener with 10K writes/sec, 100K reads/sec

Identify bottlenecks in this design:
```
[Client] â†’ [Single Nginx] â†’ [Single App Server] â†’ [Single PostgreSQL]
```

<details>
<summary>Analysis</summary>

**Bottlenecks (in order of severity):**

1. **Database** (CRITICAL)
   - PostgreSQL handles ~5K QPS total
   - We need 110K QPS (10K write + 100K read)
   - Solution: Add read replicas (10+ replicas), cache hot URLs (Redis)

2. **Application Server** (HIGH)
   - Single server handles ~10K QPS max
   - We need 110K QPS
   - Solution: Horizontal scaling to 15-20 app servers

3. **Load Balancer** (MEDIUM)
   - Nginx handles 50K+ QPS, but single point of failure
   - Solution: Multiple Nginx instances behind DNS/L4 load balancer

4. **Network** (LOW)
   - 110K QPS Ã— 500 bytes = 55 MB/sec = ~440 Mbps
   - Solution: 1 Gbps NIC is sufficient, no bottleneck

**Optimized Design:**
```
[DNS] â†’ [L4 LB] â†’ [Nginx Pool (3)] â†’ [App Servers (20)] â†’ [Redis Cluster]
                                                          â†’ [PostgreSQL Master]
                                                          â†’ [PostgreSQL Replicas (10)]
```

</details>

---

## ğŸš€ Final Wisdom

### The "Three Questions" Framework

Before ANY design decision, ask:

1. **"What problem am I solving?"** (Requirement)
2. **"What are my constraints?"** (Scale, latency, consistency)
3. **"What are the trade-offs?"** (Cost, complexity, performance)

### The "Explanation Template"

When presenting a design choice:

```
"I'm proposing [TECHNOLOGY/PATTERN] because:

1. It solves [SPECIFIC PROBLEM]
2. It handles [SCALE REQUIREMENT]
3. Trade-offs: [WHAT WE GIVE UP] vs [WHAT WE GAIN]
4. Alternatives considered: [OTHER OPTIONS] but [WHY NOT]
5. Migration path: [HOW TO EVOLVE]"
```

---

## ğŸ“– Recommended Next Steps

1. **Memorize**: Performance numbers, CAP theorem, caching strategies
2. **Practice**: Draw 10+ system designs from memory
3. **Analyze**: Study real-world architectures (Netflix, Uber, Twitter)
4. **Measure**: Benchmark technologies in your projects
5. **Discuss**: Explain designs to peers, get feedback

---

**Remember**: The interviewer isn't testing if you know the "right" answer (there isn't one). They're evaluating:

âœ… **Structured thinking** - Do you have a systematic approach?  
âœ… **Trade-off awareness** - Do you understand pros/cons?  
âœ… **Scale intuition** - Do you know when to use what?  
âœ… **Communication** - Can you explain complex ideas simply?  
âœ… **Adaptability** - Can you adjust based on new requirements?

**Master these tricks, and you'll design systems with the confidence of a 10x engineer!** ğŸš€

---

*This guide complements the POWER Technique. Together they form your complete system design arsenal.*

