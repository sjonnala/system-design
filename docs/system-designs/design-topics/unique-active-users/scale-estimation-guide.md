# ğŸ¯ Unique Active Users System: Scale Estimation Masterclass

## The DATA Framework for Analytics Scale Math
**(D)imensions â†’ (A)ctive users â†’ (T)hroughput â†’ (A)ggregations**

This mental framework applies to ANY data analytics and event processing system.

---

## ğŸ“Š PART 1: Understanding the Analytics Scale

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Registered Users | 500M | Facebook-scale platform |
| | Daily Active Users (DAU) | 100M | ~20% of total (industry standard) |
| | Weekly Active Users (WAU) | 200M | ~40% of total |
| | Monthly Active Users (MAU) | 300M | ~60% of total |
| **Event Generation** | Events per DAU | 50/day | Mix of page views, clicks, actions |
| | Peak Hour Concentration | 30% | Traffic concentrated in 8 peak hours |
| **Event Size** | Average Event Size | 1 KB | JSON payload with metadata |
| | Compressed Size | 200 bytes | LZ4 compression (5:1 ratio) |
| **Retention** | Raw Events | 7 days | Hot storage, then archive |
| | Aggregated Data | 5 years | Historical analytics |

---

## ğŸ§® PART 2: The "Analytics Calculator" - Mental Math Toolkit

### Rule #1: **Events Per Second (EPS) Formula**
```
Remember these anchors:
â€¢ 1 Day = 86,400 seconds â‰ˆ 100K seconds (for quick math)
â€¢ Peak multiplier = 3Ã— average (industry standard)
â€¢ Events/sec = (DAU Ã— Events/User/Day) Ã· 100K

Quick Example:
100M DAU Ã— 50 events/day Ã· 100K = 50,000 events/sec (average)
Peak: 50K Ã— 3 = 150,000 events/sec
```

### Rule #2: **The HyperLogLog Memory Trick**
```
Exact counting memory: 16 bytes Ã— number of unique users
HyperLogLog memory: 12 KB (fixed, regardless of users!)

Example - Storing DAU:
âœ— Exact (HashSet): 100M users Ã— 16 bytes = 1.6 GB per day
âœ“ HyperLogLog: 12 KB per day (99.19% memory savings!)

Trade-off: Â±0.81% accuracy (acceptable for analytics)
```

### Rule #3: **Storage Compression Multiplier**
```
Raw JSON: 1 KB per event
Compressed (LZ4): 200 bytes (5:1 ratio)
Columnar (Parquet): 100 bytes (10:1 ratio)
Aggregated: 50 bytes (20:1 ratio)

Always calculate:
- Raw ingestion storage
- Compressed storage (Ã· 5)
- Aggregated storage (Ã· 20)
```

---

## ğŸ“ˆ PART 3: Scale Math Template for Analytics Systems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ THE ANALYTICS NAPKIN MATH - Universal Template     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: EVENT INGESTION RATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Daily Active Users:        [____] M
Events per User per Day:   [____]
Peak Hour Concentration:   [____]%

â†’ Total Events/Day = DAU Ã— Events/User = [____] B
â†’ Average EPS = Events/Day Ã· 100K       = [____] K
â†’ Peak EPS = Average Ã— 3                = [____] K

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Event Size (Raw):          [____] KB
Compression Ratio:         5:1 (LZ4)
Retention Period:          [____] days

â†’ Daily Raw Storage = Events Ã— Size        = [____] TB
â†’ Daily Compressed = Daily Raw Ã· 5         = [____] TB
â†’ Total Storage = Daily Ã— Retention        = [____] PB

STEP 3: UNIQUE USER COUNTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users to Track:            [____] M
Counting Method:           HyperLogLog

â†’ Memory (Exact): Users Ã— 16 bytes         = [____] GB
â†’ Memory (HLL): 12 KB per dimension        = [____] KB
â†’ Dimensions (country, platform, etc.):    = [____]
â†’ Total HLL Memory = 12KB Ã— Dimensions     = [____] MB

STEP 4: QUERY LOAD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dashboard Queries/Day:     [____]
Analyst Queries/Day:       [____]
Real-time Updates/Min:     [____]

â†’ Total Queries/Day = [____]
â†’ QPS = Total Ã· 100K = [____]
â†’ Average Query Latency Target: < 2s
â†’ Cache Hit Ratio: 70-80%

STEP 5: AGGREGATION STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dimensions to Track:       [____]
Granularity:              Hourly, Daily, Monthly
Retention:                5 years

â†’ Hourly Rollups = 24 Ã— 365 Ã— 5 Ã— Dims    = [____] GB
â†’ Daily Rollups = 365 Ã— 5 Ã— Dims          = [____] MB
â†’ Monthly Rollups = 12 Ã— 5 Ã— Dims         = [____] KB
```

---

## ğŸ’¾ PART 4: Filled Template - Unique Active Users System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    UNIQUE ACTIVE USERS - NAPKIN MATH SOLUTION          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: EVENT INGESTION RATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Daily Active Users:        100 M
Events per User per Day:   50
Peak Hour Concentration:   30% in 8 hours

â†’ Total Events/Day = 100M Ã— 50           = 5,000 M (5 B)
â†’ Average EPS = 5B Ã· 100K                = 50,000 EPS
â†’ Peak EPS = 50K Ã— 3                     = 150,000 EPS

Peak Hour Detail:
- 30% of daily events = 1.5B events
- Concentrated in 8 hours = 28,800 sec
- Peak EPS = 1.5B Ã· 28,800               = 52,000 EPS
- With spikes (2Ã—): 100K+ EPS

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Event Size (Raw):          1 KB
Event Size (Compressed):   200 bytes (LZ4)
Event Size (Parquet):      100 bytes
Retention Period:          7 days (raw), 2 years (archive)

â†’ Daily Raw Storage = 5B Ã— 1KB           = 5 TB/day
â†’ Daily Compressed = 5TB Ã· 5             = 1 TB/day
â†’ 7-day Hot Storage = 1TB Ã— 7            = 7 TB
â†’ 2-year Archive = 1TB Ã— 730 Ã· 10        = 73 TB (Parquet)

Total Storage:
- Hot (7 days): 7 TB (SSD/NVMe)
- Warm (90 days): 90 TB (HDD)
- Cold (2 years): 73 TB (S3/Glacier)

STEP 3: UNIQUE USER COUNTING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users to Track:            100 M (DAU)
Dimensions:
- Countries: 195
- Platforms: 4 (web, ios, android, other)
- Device Types: 3 (mobile, desktop, tablet)
- App Versions: 50 (active versions)
- Combinations: 195 Ã— 4 Ã— 3 = 2,340 dimensions

Using Exact Counting (NOT FEASIBLE):
â†’ Memory per Day = 100M Ã— 16 bytes       = 1.6 GB/day
â†’ 30 days = 1.6GB Ã— 30                   = 48 GB
â†’ With dimensions: 48GB Ã— 2,340          = 112 TB (IMPOSSIBLE!)

Using HyperLogLog (RECOMMENDED):
â†’ Memory per HLL = 12 KB
â†’ HLLs per day = 2,340 dimensions        = 2,340 HLLs
â†’ Memory per day = 2,340 Ã— 12KB          = 28 MB
â†’ 30 days (MAU) = 28MB Ã— 30              = 840 MB
â†’ 90 days (rolling) = 28MB Ã— 90          = 2.5 GB

Memory Savings: 112 TB â†’ 2.5 GB (99.998% reduction!)

STEP 4: QUERY LOAD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dashboard Auto-refresh:    100 dashboards Ã— 12/hour = 1,200/hour
Analyst Queries:           100 analysts Ã— 20/day    = 2,000/day
Real-time Updates:         50 screens Ã— 12/hour     = 600/hour

â†’ Total Queries/Day = (1,200 Ã— 24) + 2,000 + (600 Ã— 24)
                    = 28,800 + 2,000 + 14,400
                    = 45,200 queries/day

â†’ QPS = 45,200 Ã· 86,400                  â‰ˆ 0.5 QPS (very low!)
â†’ Peak Hour = 0.5 Ã— 5                    â‰ˆ 2.5 QPS

Average Query Latency: < 2s (P95)
Cache Hit Ratio: 75% (common dashboards)
â†’ Actual DB QPS = 2.5 Ã— 0.25             â‰ˆ 0.6 QPS

Insight: Query load is LOW compared to ingestion!

STEP 5: AGGREGATION STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Dimensions:                2,340 (see above)
Metrics per Dimension:     5 (DAU, new users, events, sessions, revenue)
Size per Record:           50 bytes

Hourly Rollups:
- Records/hour = 2,340 dimensions Ã— 5 metrics = 11,700 records
- Size/hour = 11,700 Ã— 50 bytes               = 585 KB
- 1 day = 585KB Ã— 24                          = 14 MB
- 90 days = 14MB Ã— 90                         = 1.26 GB

Daily Rollups:
- Records/day = 11,700
- Size/day = 585 KB
- 5 years = 585KB Ã— 365 Ã— 5                   = 1.07 GB

Monthly Rollups:
- Records/month = 11,700
- Size/month = 585 KB
- 5 years = 585KB Ã— 12 Ã— 5                    = 35 MB

Total Aggregated Storage:
- Hourly (90 days): 1.26 GB
- Daily (5 years): 1.07 GB
- Monthly (5 years): 35 MB
- Grand Total: ~2.4 GB (tiny!)

Compare to raw: 2.4 GB vs 73 TB (30,000Ã— reduction!)
```

---

## ğŸ§  PART 6: Mental Math Techniques for Analytics

### **Technique 1: The Power of Approximation**
```
ğŸ¯ ANALYTICS MANTRA:
"Exact counts are impossible at scale. Embrace probabilistic data structures!"

HyperLogLog: Â±0.81% error (acceptable)
Count-Min Sketch: Â±2% error (top-K queries)
Bloom Filter: 1% false positive (existence checks)

Trade accuracy for:
- 1000Ã— memory savings
- Real-time processing
- Distributed computation
```

### **Technique 2: The "Five Nines" Rule**
```
When estimating storage savings:

Exact â†’ HyperLogLog: 99.9% memory reduction
Raw â†’ Compressed: 80% reduction (5:1)
Raw â†’ Aggregated: 95% reduction (20:1)
Raw â†’ Columnar: 90% reduction (10:1)

Quick Math:
1 PB raw events â†’ 50 TB aggregated (98% savings!)
```

### **Technique 3: Dimensional Explosion Calculator**
```
Dimensions grow multiplicatively, not additively!

Example:
- Countries: 200
- Platforms: 4
- Device Types: 3
- Total Combinations = 200 Ã— 4 Ã— 3 = 2,400

With user segments (10): 2,400 Ã— 10 = 24,000 combinations!

Rule: Limit dimensions to < 10 to avoid explosion
```

### **Technique 4: The Lambda Architecture Cost Model**
```
Speed Layer (Real-time):
- Cost: HIGH (Flink cluster, Redis)
- Latency: < 1 minute
- Data: Last 24 hours

Batch Layer (Historical):
- Cost: MEDIUM (Spark jobs, S3)
- Latency: 1 hour
- Data: > 24 hours

Savings: Process recent data differently from old data!
```

---

## ğŸ¨ PART 7: The Visual Analytics Scale Model

```
                    ğŸ“Š ANALYTICS SYSTEM SCALE
                             |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                    |                    |
   ğŸŒŠ INGESTION         ğŸ’¾ STORAGE           ğŸ” QUERY
        |                    |                    |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   EPS    Volume     Hot   Cold   Cache    Latency  QPS
  150K    5B/day    7TB   73TB    20GB      <2s     2.5
```

**Memory Trigger**: Think **"I.S.Q."** = Ingestion, Storage, Query

---

## ğŸ—ï¸ PART 8: Real-World Benchmark Comparisons

### Industry Benchmarks

| **Company** | **DAU** | **Events/Day** | **Storage** | **Query Latency** |
|-------------|---------|----------------|-------------|-------------------|
| Facebook | 2B | 500B+ | 300+ PB | < 1s |
| Google Analytics | 30M sites | 100B+ | 100+ PB | < 2s |
| Mixpanel | 10M tracked | 50B | 10 PB | < 1s |
| Our System | 100M | 5B | 80 TB | < 2s |
| Twitter | 200M | 500B | 50+ PB | < 1s |

---

## ğŸ¯ PART 9: The Interview Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYTICS SYSTEM ESTIMATION - 5 MIN RITUAL      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Clarify: DAU, Events/User, Dimensions
[ ] 2. Calculate EPS: DAU Ã— Events Ã· 100K
[ ] 3. Calculate Storage: Events Ã— Size Ã— Retention
[ ] 4. Choose Counting: HyperLogLog for > 100K users
[ ] 5. Estimate Queries: Usually LOW compared to ingestion
[ ] 6. Plan Aggregations: Pre-compute everything
[ ] 7. Sanity Check: Does this match known systems?
```

---

## ğŸ’¡ Pro Architect Tips

### **Tip 1: The 80-20 Rule for Analytics**
```
80% of queries access:
- 20% of data (last 7 days)
- 20% of dimensions (country, platform)
- 20% of users (most active)

Optimize for the 80%, not the 20%!
```

### **Tip 2: Pre-aggregation is King**
```
Query cost comparison:
- Raw events scan: $10 per query
- Pre-aggregated: $0.01 per query (1000Ã— cheaper!)

Always pre-aggregate:
- Hourly rollups
- Daily summaries
- Monthly totals
```

### **Tip 3: Lambda Architecture Split Point**
```
Question: "When to split speed vs batch layer?"

Answer: At the 24-hour mark!
- < 24 hours: Real-time (Flink, Redis)
- > 24 hours: Batch (Spark, ClickHouse)

Reason: Recent data changes frequently (corrections, late arrivals)
Old data is stable, can be optimized
```

### **Tip 4: Dimension Cardinality Matters**
```
Low Cardinality (< 1000): Store in each record
- Country (200)
- Platform (10)
- Device (10)

High Cardinality (> 1M): Use dimension tables
- User ID (100M)
- Session ID (1B)
- Event ID (10B)

Join at query time, don't denormalize!
```

---

## ğŸ“š Quick Reference: Analytics Scale Benchmarks

| **Metric** | **Small** | **Medium** | **Large** | **Huge** |
|------------|-----------|------------|-----------|----------|
| **DAU** | < 1M | 1M - 10M | 10M - 100M | > 100M |
| **Events/Day** | < 100M | 100M - 1B | 1B - 10B | > 10B |
| **EPS** | < 1K | 1K - 10K | 10K - 100K | > 100K |
| **Storage** | < 100 GB | 100GB - 1TB | 1TB - 10TB | > 10TB |
| **Ingestion Tool** | Logstash | Kafka | Kafka + Flink | Distributed Kafka |
| **OLAP DB** | PostgreSQL | ClickHouse | Druid/ClickHouse | Druid + Pinot |
| **Counting** | Exact (SQL) | HyperLogLog | HyperLogLog | HyperLogLog + Sampling |

---

## ğŸ” Repetition Backed by Emotion

**REPEAT 3 TIMES OUT LOUD:**
1. *"HyperLogLog saves 99.9% memory - it's magic!"*
2. *"Pre-aggregate everything - raw queries don't scale!"*
3. *"Lambda architecture: real-time + batch = best of both!"*

**VISUALIZE:** You're at the whiteboard, the interviewer nods as you confidently say:
> "With 100 million DAU and 50 events per user, we're looking at 50,000 events per second on average, peaking at 150K. We'll use HyperLogLog to track unique users with only 12KB per dimension, down from 1.6GB exact counting..."

---

## ğŸ”§ Practical Application: Adapting This Template

### For a **Video Analytics** System (like YouTube):
```
STEP 1: INGESTION
- Views/day: Billions
- Unique viewers: 100M+
- Video interactions: plays, pauses, seeks

STEP 2: STORAGE
- Video metadata: 1KB per video
- View events: 500 bytes each
- Watch time: aggregate by video, user, country

STEP 3: COUNTING
- Use HyperLogLog for unique viewers
- Exact count for watch time (SUM aggregation)
- Pre-aggregate by hour, day, video

STEP 4: QUERIES
- "Top trending videos" (hot data, cache heavily)
- "Channel analytics" (pre-aggregate by channel)
- "Viewer demographics" (dimension tables)
```

### For a **E-commerce Analytics** System:
```
STEP 1: INGESTION
- Page views, add-to-cart, purchases
- Lower volume than social (10Ã— less)
- Higher value per event (revenue tracking)

STEP 2: STORAGE
- Events with product metadata
- Join with product catalog (dimension table)
- Revenue calculations (SUM, not COUNT)

STEP 3: COUNTING
- Unique visitors: HyperLogLog
- Purchases: Exact count (low volume)
- Revenue: Exact SUM (financial accuracy)

STEP 4: QUERIES
- "Conversion funnel" (pre-computed)
- "Top products" (materialized view)
- "Revenue by country" (hourly rollup)
```

---

## ğŸ“ Professor's Final Wisdom

> **"In analytics, FAST APPROXIMATE beats SLOW EXACT"**

Your interviewer wants to see:
1. âœ… Understanding of scale (millions vs billions)
2. âœ… Trade-offs (accuracy vs memory)
3. âœ… Practical solutions (HyperLogLog, pre-aggregation)
4. âœ… Cost awareness (storage, compute optimization)

**NOT NEEDED:**
- âŒ Exact calculations
- âŒ Memorizing algorithms
- âŒ Over-engineering

---

## ğŸš€ Key Metrics Summary

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Avg EPS** | 50,000 | Kafka partition sizing |
| **Peak EPS** | 150,000 | Autoscaling triggers |
| **Raw Storage** | 5 TB/day | Infrastructure cost |
| **Compressed** | 1 TB/day | Actual storage needed |
| **HLL Memory** | 28 MB/day | Redis sizing |
| **Query Latency** | < 2s | User experience |
| **Aggregation** | 2.4 GB | 30,000Ã— savings! |

---

## ğŸ¯ Mental Math Practice Problem

### Problem: Design Analytics for Uber

```
Given:
- 100M trips/month
- 10M active riders/month
- 1M active drivers/month
- Each trip generates 100 events (GPS, status updates)
- Track: trips by city, driver ratings, surge pricing

Calculate:
1. Events per second
2. Storage (1 year)
3. HyperLogLog memory for MAU
4. Aggregation tables size

[Try it yourself, then check your approach]
```

<details>
<summary>Approach</summary>

```
1. EVENTS PER SECOND:
   - Trips/month = 100M
   - Events/trip = 100
   - Total events/month = 100M Ã— 100 = 10B
   - Events/day = 10B Ã· 30 = 333M
   - EPS = 333M Ã· 100K = 3,330 EPS (avg)
   - Peak = 3,330 Ã— 3 = 10,000 EPS

2. STORAGE (1 year):
   - Events/year = 10B Ã— 12 = 120B
   - Size/event = 500 bytes (GPS data)
   - Raw = 120B Ã— 500B = 60 TB
   - Compressed (5:1) = 12 TB
   - Parquet (10:1) = 6 TB for archive

3. HLL MEMORY (MAU):
   - Riders: 10M
   - Drivers: 1M
   - Cities: 1000
   - HLLs needed = 2 (riders, drivers) Ã— 1000 cities = 2000
   - Memory = 2000 Ã— 12KB = 24 MB

4. AGGREGATIONS:
   - Dimensions: city (1000), hour (24), day (365)
   - Metrics: trips, revenue, avg_rating (3)
   - Records/year = 1000 Ã— 365 Ã— 3 = 1,095,000
   - Size = 1M Ã— 100 bytes = 100 MB
```
</details>

---

**Remember**:
> "Analytics systems are write-heavy, storage-heavy, but query-light. Optimize for ingestion throughput and storage efficiency, not query complexity!"

**Now go design world-class analytics systems!** ğŸš€

---

*Created with the DATA technique: Dimensions â†’ Active users â†’ Throughput â†’ Aggregations*
*Perfect for: FAANG interviews, Analytics System Design, Real-time Data Processing*
