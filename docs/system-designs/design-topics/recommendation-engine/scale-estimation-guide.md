# ğŸ¯ Recommendation Engine: Scale Estimation Masterclass

## The POWER Technique for ML Systems Scale Math
**(P)rinciples â†’ (O)rder of magnitude â†’ (W)rite it down â†’ (E)stimate â†’ (R)ound ruthlessly**

This is a **mental framework** for estimating scale in ML-powered systems.

---

## ğŸ“Š PART 1: Users, Items & Scale Estimation

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 1B | Netflix/Amazon/YouTube scale |
| | Daily Active Users (DAU) | 200M | ~20% of total (Pareto!) |
| | Power Users (Heavy engagement) | 50M | ~25% generate 75% interactions |
| **Item Catalog** | Total Items | 100M | Products/videos/music/articles |
| | New Items/day | 100K | Catalog growth rate |
| | Active Items (< 1 year old) | 30M | 30% of catalog |
| **Interactions** | Interactions per DAU/day | 50 | Views, clicks, purchases, likes |
| | Total Interactions/day | 10B | 200M DAU Ã— 50 |
| | Read:Write Ratio | N/A | Recommendation requests vs events |
| **Recommendations** | Rec requests per DAU/day | 2.5 | Average session count |
| | Total Rec requests/day | 500M | 200M DAU Ã— 2.5 |
| **Time Distribution** | Peak Hours | 6-8 hours/day | Evening + commute times |
| | Peak Traffic Multiplier | 2.5x | Higher than typical systems |

---

## ğŸ§® PART 2: The "ML System Calculator" - Your Mental Math Toolkit

### Rule #1: **The ML Data Scale Ladder**
```
ML System Data Hierarchy:
â€¢ User-Item Matrix: Users Ã— Items interactions
â€¢ Embeddings: Users Ã— Dims + Items Ã— Dims
â€¢ Features: (Users + Items) Ã— Feature_Count
â€¢ Training Data: Interactions Ã— (1 + Negative_Samples)
â€¢ Model Parameters: Layers Ã— Neurons Ã— Weights

Key insight: ML systems have 3 storage layers:
1. Raw Data (events, interactions)
2. Processed Features (engineered, aggregated)
3. Model Artifacts (weights, embeddings)
```

### Rule #2: **The Embedding Math Shortcut**
```
Embedding Storage = Count Ã— Dimensions Ã— 4 bytes (float32)

Example: 1B users Ã— 128 dims Ã— 4 bytes = 512 GB

Quick calc: 1B Ã— 128 Ã— 4 = 1B Ã— 512 = 512 GB
(Think: "half a KB per user" â†’ 1B Ã— 0.5 KB = 500 GB)
```

### Rule #3: **The GPU Hour Conversion**
```
Training Time Estimation:
â€¢ Small dataset (1M samples): ~1 GPU hour
â€¢ Medium dataset (100M samples): ~10-20 GPU hours
â€¢ Large dataset (10B samples): ~100-200 GPU hours

With 8 GPUs in parallel: Divide by 8
With data parallelism overhead: Add 20%
```

---

## ğŸ“ˆ PART 3: Quick Scale Math Template for ML Systems

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    THE ML SYSTEM NAPKIN MATH TEMPLATE - Universal RecSys    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users (DAU):              [____] M
Items in Catalog:         [____] M
Interactions/user/day:    [____]
Rec requests/user/day:    [____]

â†’ Total Interactions/day  = DAU Ã— Interactions   = [____] B
â†’ Interaction Event QPS   = Interactions Ã· 100K  = [____] K
â†’ Rec Request QPS         = Requests Ã· 100K      = [____] K
â†’ Peak QPS                = Average QPS Ã— 2.5    = [____] K

STEP 2: DATA STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Raw Event Storage:
   Events/day:            [____] B
   Bytes per event:       [____] bytes (200-500 typical)
   Retention:             [____] years

   â†’ Daily Storage   = Events Ã— Bytes      = [____] TB
   â†’ Yearly Storage  = Daily Ã— 365         = [____] TB
   â†’ Total Storage   = Yearly Ã— Years      = [____] TB

B. User/Item Profile Storage:
   Users:                 [____] B
   Items:                 [____] M
   Profile size:          [____] KB

   â†’ User Storage    = Users Ã— Profile     = [____] TB
   â†’ Item Storage    = Items Ã— Profile     = [____] GB

C. Embedding Storage:
   User embeddings:       [____] B users Ã— [____] dims Ã— 4 bytes
   Item embeddings:       [____] M items Ã— [____] dims Ã— 4 bytes

   â†’ User Emb Storage = [____] GB
   â†’ Item Emb Storage = [____] GB

STEP 3: COMPUTE REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Inference (Real-time):
   Rec QPS:               [____] K
   Latency budget:        [____] ms
   CPU cores per request: [____]

   â†’ Concurrent Requests  = QPS Ã— Latency_sec     = [____]
   â†’ Total CPU Cores      = Concurrent Ã— Cores    = [____]
   â†’ Servers (32 cores)   = Total Cores Ã· 32      = [____]

B. Training (Offline):
   Training samples:      [____] B
   Epochs:                [____]
   GPU hours per epoch:   [____]

   â†’ Total GPU Hours  = Samples Ã— Epochs Ã· Throughput = [____]
   â†’ With 8 GPUs      = GPU Hours Ã· 8                 = [____] hours
   â†’ Training Freq    = Weekly/Daily                  = [____]

STEP 4: FEATURE STORE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
User Features:         [____] (count)
Item Features:         [____] (count)
Feature Store Size:    Users Ã— Features Ã— Bytes

â†’ Feature Store Total = [____] TB

STEP 5: CACHE (Redis)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Using 80-20 Rule:
â†’ Hot Users (20%) = Total Users Ã— 0.2              = [____] M
â†’ Cache Size      = Hot Users Ã— Rec_Count Ã— Bytes  = [____] GB

Practical cache: 10-20% of hot data               = [____] GB
```

---

## ğŸ’¾ PART 4: Recommendation Engine Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      RECOMMENDATION ENGINE - NAPKIN MATH SOLUTION           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users (DAU):              200 M
Items in Catalog:         100 M
Interactions/user/day:    50 (views, clicks, purchases)
Rec requests/user/day:    2.5 (sessions)

â†’ Total Interactions/day  = 200M Ã— 50         = 10 B events
â†’ Interaction Event QPS   = 10B Ã· 100K        = 100K events/sec
â†’ Rec Request QPS         = 500M Ã· 100K       = 5K requests/sec
â†’ Peak QPS                = 5K Ã— 2.5          = 12.5K requests/sec

STEP 2: DATA STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Raw Event Storage:
   Events/day:            10 B
   Bytes per event:       200 bytes (user_id, item_id, event_type, timestamp, context)
   Retention:             1 year (for training)

   â†’ Daily Storage   = 10B Ã— 200B       = 2 TB/day
   â†’ Yearly Storage  = 2TB Ã— 365        = 730 TB/year

B. User/Item Profile Storage:
   Users:                 1 B
   User profile:          10 KB (demographics, preferences, history summary)
   Items:                 100 M
   Item metadata:         5 KB (title, description, category, tags)

   â†’ User Storage    = 1B Ã— 10KB        = 10 TB
   â†’ Item Storage    = 100M Ã— 5KB       = 500 GB

C. Embedding Storage:
   User embeddings:       1B users Ã— 128 dims Ã— 4 bytes
   Item embeddings:       100M items Ã— 128 dims Ã— 4 bytes

   â†’ User Emb Storage = 1B Ã— 128 Ã— 4    = 512 GB
   â†’ Item Emb Storage = 100M Ã— 128 Ã— 4  = 51 GB
   â†’ Total Embeddings                   = 563 GB

D. Feature Store:
   User features:         1B users Ã— 200 features Ã— 4 bytes (avg)
   Item features:         100M items Ã— 150 features Ã— 4 bytes

   â†’ User Features   = 1B Ã— 200 Ã— 4     = 800 GB
   â†’ Item Features   = 100M Ã— 150 Ã— 4   = 60 GB
   â†’ Total Features                     = 860 GB (compressed ~400 GB)

TOTAL STORAGE SUMMARY:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ Raw Events (1 year):     730 TB
â€¢ User Profiles:           10 TB
â€¢ Item Metadata:           0.5 TB
â€¢ Embeddings:              0.56 TB
â€¢ Feature Store:           0.4 TB (compressed)
â€¢ Model Artifacts:         0.05 TB (models, checkpoints)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:                     ~741 TB for 1 year

STEP 3: COMPUTE REQUIREMENTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Inference (Real-time):
   Rec QPS:               12.5K (peak)
   Latency budget:        100 ms (P95)
   Candidate Gen:         20-30 ms (ANN search)
   Ranking:               50-70 ms (ML model inference)

   Concurrent Requests:
   â†’ QPS Ã— Latency = 12.5K Ã— 0.1 sec = 1,250 concurrent

   CPU Requirements (for 1,250 concurrent):
   â†’ Assuming 2 CPU cores per request
   â†’ Total Cores = 1,250 Ã— 2 = 2,500 cores
   â†’ Servers (32 cores each) = 2,500 Ã· 32 = ~80 servers

   GPU Requirements (for model serving):
   â†’ GPU throughput: ~500 inferences/sec per GPU
   â†’ Total GPUs = 12.5K Ã· 500 = ~25 GPUs (T4/V100)

B. Training (Offline):
   Training samples:      10B interactions (1 year data)
   Negative sampling:     1:4 ratio (4 negatives per positive)
   Total samples:         10B Ã— 5 = 50B samples
   Epochs:                10
   GPU throughput:        1M samples/sec (on A100)

   â†’ Total Samples Ã— Epochs = 50B Ã— 10 = 500B samples
   â†’ GPU Hours = 500B Ã· 1M = 500K seconds = 139 GPU hours
   â†’ With 8 GPUs parallel = 139 Ã· 8 = ~17.5 hours
   â†’ Training Frequency = Weekly (manageable)

C. Feature Engineering (Spark):
   Data to process:       2 TB/day (raw events)
   Spark cluster:         50 nodes Ã— 32 cores = 1,600 cores
   Processing time:       ~2 hours for daily batch job

STEP 4: BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Event Ingestion:
â†’ Write BW = 100K events/sec Ã— 200 bytes = 20 MB/sec â‰ˆ 160 Mbps

Recommendation Serving:
â†’ Read BW  = 5K requests/sec Ã— 500 bytes (response) = 2.5 MB/sec â‰ˆ 20 Mbps

Model Downloads (deployment):
â†’ Model size: 5 GB per model Ã— 4 models = 20 GB
â†’ Deployment freq: Weekly
â†’ Bandwidth spike: 20 GB Ã· 3600 sec = ~5 MB/sec

STEP 5: CACHE (Redis)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Using 80-20 Rule:
â†’ Hot Users (20%) = 1B Ã— 0.2 = 200M users

Pre-computed recommendations per user:
â†’ Top 50 items Ã— 8 bytes (item ID) = 400 bytes per user
â†’ Cache Size = 200M Ã— 400 bytes = 80 GB

With metadata (scores, features):
â†’ Enhanced cache = 80 GB Ã— 2 = 160 GB

Practical Redis Cluster:
â†’ 3 masters + 3 replicas
â†’ 200 GB total (with overhead)
â†’ Cost: ~$500/month (ElastiCache)
```

---

## ğŸ§  PART 5: ML-Specific Mental Math Techniques

### **Technique 1: The Embedding Dimension Rule**
```
Common embedding dimensions:
â€¢ Small models: 32-64 dims (mobile, edge devices)
â€¢ Standard: 128 dims (most production systems)
â€¢ Large: 256-512 dims (research, high accuracy)

Memory estimation:
32 dims   â†’ ~128 bytes per entity (32 Ã— 4)
128 dims  â†’ ~512 bytes per entity (128 Ã— 4)
256 dims  â†’ ~1 KB per entity

Quick calc for 1B users with 128-dim embeddings:
1B Ã— 512 bytes = 512 GB
```

### **Technique 2: The Training Data Multiplier**
```
Training data expansion due to negative sampling:

Positive samples: P
Negative sampling ratio: N (typically 1:4 or 1:10)

Total training samples = P Ã— (1 + N)

Example:
10B interactions Ã— (1 + 4) = 50B training samples

Each sample = (user_id, item_id, label, features) â‰ˆ 1 KB
Storage = 50B Ã— 1 KB = 50 TB
```

### **Technique 3: The GPU Throughput Shortcut**
```
Training throughput (samples/sec):
â€¢ CPU (32 cores): ~1K samples/sec
â€¢ Single GPU (V100): ~50K samples/sec
â€¢ Single GPU (A100): ~100K samples/sec

Inference throughput:
â€¢ CPU: ~100 inferences/sec
â€¢ GPU (T4): ~500 inferences/sec
â€¢ GPU (V100): ~1000 inferences/sec

Example:
Train 10B samples on 8Ã— A100 GPUs:
â†’ Throughput = 8 Ã— 100K = 800K samples/sec
â†’ Time = 10B Ã· 800K = 12,500 sec = ~3.5 hours (single epoch)
```

### **Technique 4: The Feature Store Size Formula**
```
Feature Store Size =
    (Users Ã— User_Features + Items Ã— Item_Features) Ã— Bytes_Per_Feature

Typical feature sizes:
â€¢ Numerical: 4 bytes (float32)
â€¢ Categorical (embedding): 4-8 bytes (int32/int64)
â€¢ Text features (pre-processed): 512 bytes (128-dim embedding)

Example:
1B users Ã— 200 features Ã— 4 bytes = 800 GB (user features)
100M items Ã— 150 features Ã— 4 bytes = 60 GB (item features)
Total = 860 GB â†’ Compressed (Parquet) = ~400 GB
```

---

## ğŸ¯ PART 6: Real-World ML System Scale Benchmarks

| **System** | **DAU** | **Items** | **Events/day** | **Rec QPS** | **Training Freq** | **Storage** |
|------------|---------|-----------|----------------|-------------|-------------------|-------------|
| **Netflix** | 200M | 10K+ titles | 5B+ | 10K+ | Daily | 100+ PB |
| **YouTube** | 2B | 800M videos | 100B+ | 100K+ | Hourly | 1+ EB |
| **Amazon** | 300M | 500M products | 50B+ | 50K+ | Daily | 100+ PB |
| **Spotify** | 400M | 100M tracks | 20B+ | 20K+ | Daily | 50+ PB |
| **TikTok** | 1B | 1B+ videos | 200B+ | 200K+ | Hourly | 500+ PB |
| **LinkedIn** | 300M | 100M+ users | 10B+ | 15K+ | Daily | 50+ PB |
| **Our Design** | 200M | 100M items | 10B | 5-10K | Weekly | ~1 PB |

---

## ğŸ’¡ PART 7: ML System Scale Estimation - Practice Problems

### Problem 1: E-commerce Product Recommendations

```
Given:
- 500M registered users
- 100M DAU
- 50M products in catalog
- Each user views 20 products/day
- Each user requests recommendations 3 times/day
- Purchase rate: 2% of viewed products
- Item embeddings: 256 dimensions

Calculate:
1. Event Ingestion QPS
2. Recommendation Request QPS
3. Embedding Storage (users + items)
4. Training dataset size (with 1:5 negative sampling)
5. GPU hours for weekly training (assume A100)

[Try it yourself, then check answers below]
```

<details>
<summary>Answer</summary>

```
1. EVENT INGESTION QPS:
   - Product views/day = 100M Ã— 20 = 2B views
   - Purchases/day = 2B Ã— 0.02 = 40M purchases
   - Total events = 2B + 40M = 2.04B events/day
   - Event QPS = 2.04B Ã· 100K = ~20K events/sec
   - Peak QPS = 20K Ã— 2 = ~40K events/sec

2. RECOMMENDATION REQUEST QPS:
   - Requests/day = 100M Ã— 3 = 300M requests
   - Request QPS = 300M Ã· 100K = ~3K requests/sec
   - Peak QPS = 3K Ã— 2 = ~6K requests/sec

3. EMBEDDING STORAGE:
   - User embeddings: 500M Ã— 256 dims Ã— 4 bytes = 512 GB
   - Item embeddings: 50M Ã— 256 dims Ã— 4 bytes = 51.2 GB
   - Total: ~563 GB

4. TRAINING DATASET SIZE:
   - Positive samples (1 year): 2B views/day Ã— 365 = 730B samples
   - With 1:5 negative sampling: 730B Ã— 6 = 4.38 TB samples
   - Per sample size: ~500 bytes (user_id, item_id, features, label)
   - Storage: 4.38TB Ã— 500 bytes = ~2.2 PB raw
   - After feature engineering: ~100 TB (compressed)

5. GPU HOURS FOR TRAINING:
   - Training samples: 4.38 TB
   - Epochs: 5
   - Total: 4.38TB Ã— 5 = 21.9 TB samples
   - A100 throughput: ~100K samples/sec
   - GPU seconds: 21.9TB Ã· 100K â‰ˆ 60 hours (single GPU)
   - With 8 GPUs: 60 Ã· 8 = ~7.5 hours
```
</details>

---

### Problem 2: Video Streaming Recommendations (YouTube-like)

```
Given:
- 2B total users
- 500M DAU
- 800M videos in catalog
- 100K new videos uploaded/day
- Each user watches 10 videos/day (avg 10 min each)
- Each user gets recommendations 5 times/day
- Video embeddings: 128 dimensions
- Retention: 2 years

Calculate:
1. Video view event QPS
2. Storage for interaction events (2 years)
3. Embedding storage for videos
4. Feature store size (200 user features, 100 video features)
5. Daily batch processing time (Spark cluster: 100 nodes)

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. VIDEO VIEW EVENT QPS:
   - Views/day = 500M Ã— 10 = 5B video views
   - View event QPS = 5B Ã· 100K = ~50K events/sec
   - Peak QPS = 50K Ã— 2.5 = ~125K events/sec

2. STORAGE FOR INTERACTION EVENTS (2 years):
   - Events/day = 5B
   - Event size = 300 bytes (user_id, video_id, watch_time, context)
   - Daily storage = 5B Ã— 300 bytes = 1.5 TB/day
   - Yearly storage = 1.5 TB Ã— 365 = 547.5 TB/year
   - 2 years = 547.5 Ã— 2 = ~1.1 PB

3. EMBEDDING STORAGE FOR VIDEOS:
   - Video embeddings: 800M Ã— 128 dims Ã— 4 bytes = 409.6 GB
   - With metadata: ~500 GB

4. FEATURE STORE SIZE:
   - User features: 2B Ã— 200 features Ã— 4 bytes = 1.6 TB
   - Video features: 800M Ã— 100 features Ã— 4 bytes = 320 GB
   - Total: ~1.92 TB â†’ Compressed: ~900 GB

5. DAILY BATCH PROCESSING TIME:
   - Data to process: 1.5 TB/day
   - Spark cluster: 100 nodes Ã— 32 cores = 3,200 cores
   - Processing throughput: ~500 GB/hour (typical)
   - Time: 1.5 TB Ã· 500 GB/hour = ~3 hours
```
</details>

---

## ğŸš¨ ML-Specific Common Mistakes to Avoid

### Mistake 1: **Forgetting Model Update Frequency**
```
âœ— BAD:  "We train once and deploy"
âœ“ GOOD: "Daily incremental training + weekly full retraining"

Impact: Model staleness â†’ CTR degrades 10-20% per week without updates
```

### Mistake 2: **Underestimating Feature Store Size**
```
âœ— BAD:  "Feature store = user count Ã— feature count Ã— 4 bytes"
âœ“ GOOD: "Feature store includes:
         - Raw features (numerical, categorical)
         - Derived features (aggregations, time-windows)
         - Embedding features (high-dimensional)
         - Versioning overhead (2-3x for multiple versions)"

Actual size: 2-3x initial estimate
```

### Mistake 3: **Ignoring Negative Sampling Overhead**
```
âœ— BAD:  "Training data = number of positive interactions"
âœ“ GOOD: "Training data = positive Ã— (1 + negative_ratio)
         With 1:10 negative sampling:
         10B interactions â†’ 110B training samples"
```

### Mistake 4: **Not Accounting for A/B Testing Infrastructure**
```
âœ— BAD:  "One model in production"
âœ“ GOOD: "Multiple model variants:
         - Control (baseline): 50% traffic
         - Variant A (new model): 25% traffic
         - Variant B (experimental): 25% traffic

         Infrastructure overhead: 3Ã— model serving capacity"
```

### Mistake 5: **Underestimating Inference Latency**
```
âœ— BAD:  "Model inference = 10ms"
âœ“ GOOD: "End-to-end latency breakdown:
         - Feature retrieval: 20ms
         - Candidate generation (ANN): 30ms
         - Model inference: 10ms
         - Post-processing: 10ms
         - Network overhead: 20ms
         - Total: 90ms (P50), 150ms (P95)"
```

---

## ğŸ“ Your ML System Practice Template (Fill-in-the-Blank)

```
SYSTEM: ___________________

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Users (Total):            [____] M/B
Users (DAU):              [____] M
Items (Catalog):          [____] M
Interactions per user/day:[____]
Rec requests per user/day:[____]

Total Interactions/Day:   [____] B
Total Rec Requests/Day:   [____] M

â†’ Interaction QPS = [____] B Ã· 100K = [____] K
â†’ Rec Request QPS = [____] M Ã· 100K = [____] K
â†’ Peak QPS        = [____] K Ã— 2.5   = [____] K

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Raw Events:
   Events/day:            [____] B
   Bytes per event:       [____] bytes
   Retention:             [____] years

   â†’ Daily Storage  = [____] TB
   â†’ Yearly Storage = [____] TB

B. Embeddings:
   User embeddings:       [____] B users Ã— [____] dims Ã— 4 bytes
   Item embeddings:       [____] M items Ã— [____] dims Ã— 4 bytes

   â†’ User Emb  = [____] GB
   â†’ Item Emb  = [____] GB

C. Feature Store:
   User features:         [____] features
   Item features:         [____] features

   â†’ Feature Store = [____] TB

STEP 3: COMPUTE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
A. Inference Servers:
   Peak QPS:              [____] K
   Latency budget:        [____] ms
   Concurrent requests:   QPS Ã— Latency = [____]

   â†’ Servers (32 cores) = [____]
   â†’ GPUs (for serving) = [____]

B. Training (Weekly):
   Training samples:      [____] B
   GPU hours per epoch:   [____]
   Epochs:                [____]

   â†’ Total GPU hours = [____]
   â†’ With 8 GPUs     = [____] hours

STEP 4: CACHE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hot users (20%):          [____] M
Recs per user:            [____]
Bytes per rec:            [____]

â†’ Cache Size = [____] GB

SMELL TEST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ QPS reasonable? (compare to known ML systems)
â–¡ Storage in PB range? (typical for large-scale RecSys)
â–¡ Training time < 1 day? (for weekly updates)
â–¡ Cache size practical? (hundreds of GB for Redis cluster)
â–¡ Inference latency < 100ms? (P95 target)
```

---

## ğŸ Bonus: ML System Scale Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ML RECOMMENDATION SYSTEM SCALE CHEAT SHEET       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 1 Day      = 100K seconds
â€¢ 1 GB       = 1B bytes
â€¢ 1 TB       = 1000 GB
â€¢ 1 PB       = 1000 TB
â€¢ Float32    = 4 bytes
â€¢ Embedding  = Dimensions Ã— 4 bytes

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Event QPS       = Events/day Ã· 100K
Rec QPS         = Requests/day Ã· 100K
Embedding Size  = Count Ã— Dimensions Ã— 4 bytes
Training Data   = Positive Ã— (1 + Negative_Ratio)
GPU Hours       = Samples Ã· Throughput Ã· 3600
Feature Store   = (Users + Items) Ã— Features Ã— 4 bytes

TYPICAL RATIOS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ DAU:Total = 20-25% (active users)
â€¢ Peak:Avg = 2-3x (higher variability)
â€¢ Negative Sampling = 1:4 to 1:10
â€¢ Cache Hit Rate = 70-85% (recommendations)
â€¢ Feature Store Compression = 2-3x (Parquet)
â€¢ Model Update Freq = Daily to Weekly

QUICK ESTIMATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small RecSys:  1-10M users,   1-10M items,    <1 TB
Medium RecSys: 10-100M users, 10-100M items,  1-10 TB
Large RecSys:  100M-1B users, 100M-1B items,  10-100 TB
Huge RecSys:   >1B users,     >1B items,      100TB-1PB

ML INFERENCE BUDGET:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Feature Lookup:     10-20ms
Candidate Gen:      20-30ms (ANN search)
Model Inference:    10-20ms (per model)
Post-processing:    10-20ms
Network Overhead:   10-20ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total (P95):        <100ms target

ML TRAINING BUDGET:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small model (<100M params):  1-10 GPU hours
Medium model (100M-1B):      10-100 GPU hours
Large model (>1B params):    100-1000 GPU hours

With 8Ã— A100 GPUs in parallel: Divide by 6-8 (with overhead)

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Can serve 1K QPS with 10-20 servers? YES (with caching)
âœ“ Can store 1B embeddings in 1 TB? YES (128-dim, float32)
âœ“ Can train on 10B samples in 1 day? YES (with GPU cluster)
âœ“ Can cache 100M users in 100 GB Redis? YES (1 KB per user)
âœ“ Can achieve <100ms latency? YES (with optimization)

âœ— Can single GPU serve 10K QPS? NO (need 20+ GPUs or CPUs)
âœ— Can train on 1TB data in 1 hour? NO (need distributed training)
âœ— Can store all features in memory? NO (use Feature Store)
âœ— Can skip negative sampling? NO (critical for model quality)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final ML System Scale Challenge

Pick one of these ML systems and estimate full scale:

1. **Uber Rides** - Real-time location-based recommendations
2. **Airbnb** - Property recommendations with search constraints
3. **Pinterest** - Visual content recommendations
4. **Twitter/X** - Tweet recommendations in timeline
5. **OpenTable** - Restaurant recommendations

Use the blank template above and time yourself: **Can you complete it in 10 minutes?**

---

## ğŸ“š Additional ML System Resources

- **Papers**: "Deep Neural Networks for YouTube Recommendations" (Google)
- **Papers**: "Wide & Deep Learning for Recommender Systems" (Google)
- **Papers**: "Behavior Sequence Transformer" (Alibaba)
- **Books**: "Recommender Systems Handbook" by Ricci et al.
- **Courses**: Stanford CS246 (Mining Massive Datasets)
- **Tools**: TensorFlow Recommenders, PyTorch Geometric, FAISS

---

**Remember**:
> "In ML system design, understanding the data scale and model complexity trade-offs is more important than exact numbers."

**Now go build scalable recommendation systems!** ğŸš€

---

*Created with the POWER technique for ML Systems: Principles â†’ Order â†’ Write â†’ Estimate â†’ Round*
*Perfect for: FAANG interviews, ML System Design rounds, RecSys architecture discussions*
