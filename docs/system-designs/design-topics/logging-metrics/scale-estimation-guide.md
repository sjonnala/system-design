# ğŸ¯ Logging & Metrics System: Scale Estimation Masterclass

## The DELTA Technique for Observability Scale Math
**(D)ata volume â†’ (E)vents per second â†’ (L)atency targets â†’ (T)hroughput capacity â†’ (A)rchive requirements**

This mental framework applies specifically to data-intensive observability systems where **volume** is the primary driver.

---

## ğŸ“Š PART 1: System Scale Assumptions

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **Infrastructure** | Total Servers | 10,000 | Medium-large company (similar to Airbnb, Pinterest scale) |
| | Microservices | 500 | Distributed architecture |
| | Containers (K8s pods) | 50,000 | Avg 5 pods per service |
| **Logging** | Log lines per server/sec | 100 | Moderate logging |
| | Avg log line size | 512 bytes | JSON structured logs |
| | Daily Active Users | 100M | For user event tracking |
| **Metrics** | Metrics per server | 200 | CPU, mem, disk, network, app |
| | Scrape interval | 15 seconds | Prometheus default |
| | Metric cardinality | 10M | Total unique time series |
| **Retention** | Hot logs retention | 90 days | Searchable in Elasticsearch |
| | Cold logs retention | 1 year | S3 archive |
| | Metrics retention (raw) | 15 days | Prometheus local |
| | Metrics retention (long-term) | 5 years | Thanos/S3 with downsampling |

---

## ğŸ§® PART 2: The "Data Pipeline Calculator" - Mental Math for Observability

### Rule #1: **Events Per Second Foundation**
```
Key Anchor: 10,000 servers Ã— 100 logs/sec = 1 million log events/sec

Quick math:
â€¢ Small scale: 1,000 servers Ã— 100 logs/sec = 100K events/sec
â€¢ Medium scale: 10,000 servers Ã— 100 logs/sec = 1M events/sec
â€¢ Large scale: 100,000 servers Ã— 100 logs/sec = 10M events/sec
```

### Rule #2: **Storage Explosion Formula**
```
EMOTION TRIGGER: "Logs grow FAST - a million events per second is 40TB per day!"

Daily storage = Events/sec Ã— Seconds/day Ã— Size/event
             = 1M Ã— 86,400 Ã— 512 bytes
             = 44 TB/day raw logs

With compression (5x): 44TB Ã· 5 = ~9 TB/day
```

### Rule #3: **Metrics Are Small But Numerous**
```
Metrics calculation is simpler:
â€¢ 1 data point = ~16 bytes (timestamp + value)
â€¢ 10M metrics Ã— 4 samples/min Ã— 16 bytes = 640 MB/min = ~1TB/day

But cardinality explosion is the REAL challenge!
```

---

## ğŸ“ˆ PART 3: Scale Math Template for Observability

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ OBSERVABILITY NAPKIN MATH - Logging & Metrics       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: LOG VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Servers/Containers:      [____]
Log lines per server/sec:[____]
Avg log size:            [____] bytes

â†’ Log events/sec = Servers Ã— Lines/sec      = [____]
â†’ Peak events/sec = Avg Ã— 2                 = [____]
â†’ Daily volume = Events/sec Ã— 86,400 Ã— Size = [____] TB
â†’ With compression (5x) = Daily Ã· 5         = [____] TB

STEP 2: METRICS VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active metrics (cardinality):    [____] M
Scrape interval:                 [____] seconds
Data point size:                 16 bytes (fixed)

â†’ Samples/min = Metrics Ã— (60/interval)     = [____] M
â†’ Storage/min = Samples Ã— 16 bytes          = [____] MB
â†’ Daily storage = Storage/min Ã— 1440        = [____] GB
â†’ Yearly storage (raw) = Daily Ã— 365        = [____] TB

STEP 3: STORAGE WITH RETENTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Log retention (hot):     [____] days
Metrics retention (raw): [____] days

â†’ Total log storage = Daily Ã— Retention     = [____] TB
â†’ Total metric storage = Daily Ã— Retention  = [____] TB

STEP 4: PROCESSING CAPACITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Kafka partitions needed = Events/sec Ã· 50K  = [____]
â†’ ES shards needed = Total storage Ã· 50GB     = [____]
â†’ Prometheus instances = Metrics Ã· 1M         = [____]

STEP 5: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Ingest bandwidth = Events/sec Ã— Size       = [____] MB/s
â†’ Query bandwidth (estimate 10% of ingest)   = [____] MB/s
```

---

## ğŸ’¾ PART 4: Logging & Metrics System Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LOGGING & METRICS SYSTEM - NAPKIN MATH SOLUTION      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: LOG VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Servers/Containers:      10,000 servers + 50,000 containers = 60,000 sources
Log lines per source/sec: 100 (avg)
Avg log size:            512 bytes (JSON structured)

â†’ Log events/sec = 60,000 Ã— 100           = 6M events/sec
â†’ Peak events/sec = 6M Ã— 2                = 12M events/sec
â†’ Daily volume = 6M Ã— 86,400 Ã— 512 bytes  = 265 TB/day (raw)
â†’ With compression (5x) = 265TB Ã· 5       = 53 TB/day (compressed)

STEP 2: METRICS VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active metrics (cardinality):    10M time series
Scrape interval:                 15 seconds
Data point size:                 16 bytes (timestamp + float64)

â†’ Samples/min = 10M Ã— (60/15)             = 40M samples/min
â†’ Storage/min = 40M Ã— 16 bytes            = 640 MB/min
â†’ Daily storage = 640MB Ã— 1440            = 922 GB/day â‰ˆ 1 TB/day
â†’ Yearly storage (raw) = 1TB Ã— 365        = 365 TB/year

With Prometheus compression (~1.3 bytes/sample):
â†’ Daily storage = 40M samples/min Ã— 1440 Ã— 1.3 bytes = 75 GB/day
â†’ 15-day retention = 75GB Ã— 15            = ~1.1 TB (Prometheus local)

STEP 3: STORAGE WITH RETENTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Logs:
  Hot (Elasticsearch, 90 days):  53TB/day Ã— 90    = 4,770 TB â‰ˆ 5 PB
  Warm (ES, days 30-90):         53TB/day Ã— 60    = 3,180 TB â‰ˆ 3 PB
  Cold (S3 archive, 1 year):     53TB/day Ã— 365   = 19,345 TB â‰ˆ 19 PB

Metrics:
  Raw (Prometheus, 15 days):     75GB/day Ã— 15    = 1.1 TB
  Long-term (Thanos/S3, 5 years):
    - 5min avg (90 days):        15GB/day Ã— 90    = 1.4 TB
    - 1hr avg (1 year):          2GB/day Ã— 365    = 730 GB
    - 1day avg (5 years):        100MB/day Ã— 1825 = 183 GB
  Total long-term:                                = ~2.3 TB

STEP 4: PROCESSING CAPACITY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Kafka:
  â†’ Partitions needed = 6M events/sec Ã· 50K/partition = 120 partitions
  â†’ With 3x replication & 30 topics                   = 360 total partitions
  â†’ Kafka cluster size: 30-50 brokers (r5.2xlarge)

Elasticsearch:
  â†’ Shards needed = 5 PB Ã· 50GB/shard                 = 100,000 shards
  â†’ With 1 replica                                    = 200,000 total shards
  â†’ Index strategy: Daily indices (logs-2025.11.15)
  â†’ ES cluster size: 200-300 data nodes (i3.4xlarge)
    - Hot nodes (SSD): 50 nodes
    - Warm nodes (HDD): 100 nodes
    - Cold nodes (archive): 50 nodes

Prometheus:
  â†’ Instances needed = 10M metrics Ã· 1M/instance      = 10 instances
  â†’ With HA (2x)                                      = 20 instances
  â†’ Instance size: r5.4xlarge (16 vCPU, 128GB RAM)

Stream Processing (Flink):
  â†’ Task parallelism = 6M events/sec Ã· 100K/task      = 60 parallel tasks
  â†’ Flink cluster: 20 task managers (m5.2xlarge)

STEP 5: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Ingestion:
  â†’ Log bandwidth = 6M events/sec Ã— 512 bytes         = 3,072 MB/s â‰ˆ 24 Gbps
  â†’ Peak bandwidth = 24 Gbps Ã— 2                      = 48 Gbps

Query (estimate 10% of ingest):
  â†’ Query bandwidth                                   = 2.4 Gbps

Network requirements:
  â†’ Total bandwidth capacity needed                   = 50+ Gbps
  â†’ Use 100 Gbps network backbone
```

---

## ğŸ§  PART 5: Mental Math Techniques for Observability

### **Technique 1: The "Log Size Ladder"**
```
Memorize typical log sizes:
â€¢ Minimal (syslog):      ~200 bytes
â€¢ Structured JSON:       ~500 bytes
â€¢ Verbose app logs:      ~1 KB
â€¢ Stack traces:          ~5-10 KB

Pro Tip: Most production logs are 400-600 bytes (use 500 as default)
```

### **Technique 2: The "Compression Trick"**
```
ALWAYS apply compression in your calculations:

Text logs compress VERY well:
â€¢ gzip compression: 5-10x reduction
â€¢ Use 5x for estimation (conservative)

Example: 100TB raw logs â†’ 20TB compressed
```

### **Technique 3: The "Cardinality Explosion"**
```
Metrics cardinality kills systems!

Example explosion:
Base metric: http_requests_total
Labels:
  - service: 100 values
  - endpoint: 50 values
  - method: 4 values (GET, POST, PUT, DELETE)
  - status: 10 values (200, 201, 400, 404, 500, etc.)

Total cardinality = 100 Ã— 50 Ã— 4 Ã— 10 = 200,000 time series
From ONE metric type!

RULE: Limit high-cardinality labels (user_id, request_id are DANGEROUS!)
```

### **Technique 4: The "15-Second Rule"**
```
Prometheus default scrape = 15 seconds

Quick samples calculation:
â€¢ 1 minute = 4 samples
â€¢ 1 hour = 240 samples
â€¢ 1 day = 5,760 samples
â€¢ 15 days = 86,400 samples

For 10M metrics: 10M Ã— 86,400 = 864 billion data points!
But compression helps: 864B Ã— 1.3 bytes = 1.1 TB
```

### **Technique 5: The "Tier Memory Map"**
```
Storage tiers DRAMATICALLY reduce costs:

Hot tier (0-7 days):    SSD, full search, high IOPS    = EXPENSIVE
Warm tier (8-30 days):  HDD, reduced search, lower IOPS = MODERATE
Cold tier (31-90 days): Compressed, frozen, limited     = CHEAP
Archive (90+ days):     S3/Glacier, no search           = VERY CHEAP

Cost ratio: Hot:Warm:Cold:Archive = 10:3:1:0.1
```

---

## ğŸ¨ PART 6: The Visual Mind Map for Observability

```
                ğŸŒ OBSERVABILITY SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“ LOGS           ğŸ“Š METRICS        ğŸ” TRACES
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   Vol  Ret        Card  Scrape      Spans  Sample
   53TB  90d       10M   15s         1M     10%
```

**Memory Trigger**: Think **"L.M.T."** = Logs (Volume), Metrics (Cardinality), Traces (Sampling)

---

## ğŸš¨ PART 7: Common Mistakes in Observability Estimation

### Mistake 1: **Underestimating Log Volume**
```
âœ— BAD:  "1000 servers Ã— 10 logs/sec = 10K logs/sec"
âœ“ GOOD: "1000 servers Ã— 100 logs/sec + spikes = 200K logs/sec peak"

Logs spike during:
â€¢ Deployments (health checks)
â€¢ Errors (stack traces, retries)
â€¢ High traffic events
â†’ Always plan for 2-3x average
```

### Mistake 2: **Forgetting Compression**
```
âœ— BAD:  "100TB/day = $5000/day in S3"
âœ“ GOOD: "100TB/day compressed to 20TB = $1000/day in S3"

Compression is FREE and saves MASSIVE costs!
```

### Mistake 3: **Ignoring Cardinality Growth**
```
âœ— BAD:  "We have 100 metrics, that's easy"
âœ“ GOOD: "100 base metrics Ã— 1000 labels = 100K time series"

Cardinality explosion is the #1 Prometheus killer:
â€¢ Plan for 10K-1M active series (small/medium)
â€¢ 1M-10M for large deployments
â€¢ Beyond 10M: Federation/sharding required
```

### Mistake 4: **Not Planning for Retention Tiers**
```
âœ— BAD:  "Store all logs in Elasticsearch for 1 year"
âœ“ GOOD: "Hot (7d ES SSD) â†’ Warm (30d ES HDD) â†’ Cold (90d frozen) â†’ Archive (1yr S3)"

This reduces costs by 10-20x!
```

### Mistake 5: **Underestimating Query Load**
```
âœ— BAD:  "Ingestion is 10 Gbps, so 10 Gbps network is enough"
âœ“ GOOD: "Ingestion 10 Gbps + Query 5 Gbps + Replication 10 Gbps = 25 Gbps needed"

Queries can be MORE expensive than ingestion:
â€¢ Dashboard with 50 panels = 50 concurrent queries
â€¢ 100 users Ã— 50 panels = 5000 queries/min!
```

---

## ğŸ“ PART 8: Real-World Capacity Planning Examples

### Example 1: Startup (100 servers)
```
Infrastructure:
  - Servers: 100
  - Microservices: 20
  - Containers: 500

Logs:
  - Events/sec: 100 servers Ã— 100 = 10K events/sec
  - Daily volume: 10K Ã— 86,400 Ã— 500 bytes = 432 GB/day (raw)
  - Compressed: 432GB Ã· 5 = 86 GB/day
  - 90-day retention: 86GB Ã— 90 = 7.7 TB

Storage:
  - Single ES cluster: 5 nodes (i3.xlarge)
  - Kafka: 3 brokers (r5.large)
  - Prometheus: 2 instances (r5.xlarge)

Cost estimate: ~$5K/month
```

### Example 2: Mid-size Company (1,000 servers)
```
Infrastructure:
  - Servers: 1,000
  - Microservices: 100
  - Containers: 5,000

Logs:
  - Events/sec: 1K servers Ã— 100 = 100K events/sec
  - Daily volume: 100K Ã— 86,400 Ã— 500 = 4.3 TB/day (raw)
  - Compressed: 4.3TB Ã· 5 = 860 GB/day
  - 90-day retention: 860GB Ã— 90 = 77 TB

Metrics:
  - Cardinality: 1M time series
  - Daily storage: 10 GB/day (compressed)
  - 15-day retention: 150 GB

Storage:
  - ES cluster: 20 nodes (hot: 5 i3.2xlarge, warm: 10 i3.xlarge, cold: 5)
  - Kafka: 9 brokers (r5.xlarge)
  - Prometheus: 5 instances (r5.2xlarge)

Cost estimate: ~$30K/month
```

### Example 3: Large Enterprise (10,000 servers)
```
(This is our main example from Part 4)

Infrastructure:
  - Servers: 10,000
  - Microservices: 500
  - Containers: 50,000

Logs:
  - Events/sec: 6M events/sec
  - Daily volume: 53 TB/day (compressed)
  - 90-day retention: 5 PB

Metrics:
  - Cardinality: 10M time series
  - Daily storage: 75 GB/day (compressed)
  - 15-day retention: 1.1 TB

Storage:
  - ES cluster: 200-300 nodes (tiered architecture)
  - Kafka: 30-50 brokers (r5.2xlarge)
  - Prometheus: 20 instances (r5.4xlarge) with Thanos

Cost estimate: ~$500K-800K/month
```

---

## ğŸ”§ PART 9: Cost Optimization Strategies

### Strategy 1: **Sampling for Non-Critical Logs**
```
Not all logs need 100% capture:

â€¢ ERROR/FATAL: 100% (critical!)
â€¢ WARN: 100% (important)
â€¢ INFO: 10-50% (sample in production)
â€¢ DEBUG: 1% or OFF (dev only)

Savings: 50-70% reduction in log volume
```

### Strategy 2: **Metric Downsampling**
```
Raw retention: 15 days (full fidelity)
5-min avg: 90 days (20x reduction)
1-hr avg: 1 year (288x reduction)
1-day avg: 5 years (1440x reduction)

From 365 TB/year raw â†’ ~10 TB/year long-term
Savings: 97% storage reduction!
```

### Strategy 3: **Intelligent Retention Policies**
```
Service-based retention:

Production services:
  - ERROR logs: 1 year
  - INFO logs: 90 days
  - DEBUG logs: 7 days

Non-production:
  - All logs: 7 days

Metrics:
  - Critical SLIs: 5 years
  - Standard metrics: 1 year
  - Debug metrics: 30 days

Savings: 30-50% storage costs
```

### Strategy 4: **Compression & Encoding**
```
Logs:
  - Use structured logging (JSON)
  - Apply gzip/zstd compression
  - Savings: 5-10x

Metrics:
  - Use Prometheus TSDB (1.3 bytes/sample)
  - Enable compression in Thanos
  - Savings: 10-20x vs uncompressed
```

### Strategy 5: **Query Optimization**
```
Expensive:
  - Full-text search across all indices
  - Aggregations without time bounds
  - Wildcard queries

Cheap:
  - Time-bounded queries (last 1 hour)
  - Indexed field searches
  - Pre-computed dashboards (cached)

Use Redis caching for dashboards: 90% query reduction!
```

---

## ğŸ¯ PART 10: Interview-Ready Cheat Sheet

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      OBSERVABILITY SYSTEM SCALE CHEAT SHEET            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 1 server = ~100 log lines/sec
â€¢ 1 log line = ~500 bytes (structured JSON)
â€¢ 1 metric sample = ~16 bytes (or 1.3 with Prometheus compression)
â€¢ Compression ratio = 5x for logs, 10x for metrics
â€¢ Scrape interval = 15 seconds (Prometheus default)

QUICK FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Log volume/day = Servers Ã— 100 logs/sec Ã— 86,400 Ã— 500 bytes Ã· 5
Metric storage/day = Cardinality Ã— (86,400/15) Ã— 1.3 bytes
Kafka partitions = Events/sec Ã· 50K
ES shards = Storage Ã· 50GB
Bandwidth = Events/sec Ã— Event_size

TYPICAL SCALES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:  100 servers,    10K events/sec,   <1TB/day
Medium: 1K servers,     100K events/sec,  ~1TB/day
Large:  10K servers,    1M events/sec,    ~10TB/day
Huge:   100K+ servers,  10M+ events/sec,  100+ TB/day

RETENTION STRATEGIES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Logs:
  Hot (0-7d):    ES SSD, full search
  Warm (8-30d):  ES HDD, reduced replicas
  Cold (31-90d): ES frozen, S3 snapshot
  Archive (90d+): S3 Glacier, compliance

Metrics:
  Raw (15d):     Prometheus local
  5min (90d):    Thanos/S3
  1hr (1yr):     Downsampled
  1day (5yr):    Long-term trends

COST OPTIMIZATION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Sample non-critical logs (50-70% savings)
âœ“ Use tiered storage (10x cost reduction)
âœ“ Enable compression (5-10x savings)
âœ“ Downsample metrics (97% storage savings)
âœ“ Cache dashboard queries (90% query reduction)

ANTI-PATTERNS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ— Storing all logs in single ES tier
âœ— Ignoring cardinality explosion
âœ— Not compressing logs
âœ— Using user_id/request_id as metric labels
âœ— Full-text search without time bounds
âœ— Forgetting to plan for peak traffic (2-3x avg)

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ 1K servers â†’ ~100K events/sec (reasonable)
âœ“ 100TB/day logs â†’ $3-5K/month S3 (compressed)
âœ“ 1M time series â†’ ~75GB/day metrics (with compression)
âœ“ Elasticsearch can handle 10-20K writes/sec per node
âœ“ Kafka partition can handle 50-100K events/sec
âœ“ Prometheus can handle 1M active series

âœ— 100K time series â†’ 1TB/day (too high, check calculation!)
âœ— Single ES node for 10M events/sec (impossible!)
âœ— 50M cardinality in single Prometheus (need sharding!)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ PART 11: Practice Problems

### Problem 1: E-commerce Platform
```
Given:
- 5,000 backend servers
- 20,000 Kubernetes pods
- 500 microservices
- Peak traffic: 100K requests/sec
- Each request generates 5 log lines
- Metric cardinality: 5M time series

Calculate:
1. Log events per second
2. Daily log storage (compressed)
3. 90-day log retention storage
4. Metrics storage (15 days)
5. Kafka partition count
6. Elasticsearch cluster size

[Try it yourself, then check below]
```

<details>
<summary>Answer</summary>

```
1. LOG EVENTS/SEC:
   Background logs: 25K sources Ã— 100 logs/sec = 2.5M events/sec
   Request logs: 100K req/sec Ã— 5 logs = 500K events/sec
   Total = 2.5M + 500K = 3M events/sec
   Peak = 3M Ã— 2 = 6M events/sec

2. DAILY LOG STORAGE:
   Raw = 3M Ã— 86,400 Ã— 500 bytes = 130 TB/day
   Compressed = 130TB Ã· 5 = 26 TB/day

3. 90-DAY RETENTION:
   26TB/day Ã— 90 = 2,340 TB â‰ˆ 2.3 PB

4. METRICS STORAGE (15 days):
   Samples/min = 5M Ã— (60/15) = 20M samples/min
   Daily = 20M Ã— 1440 Ã— 1.3 bytes = 37 GB/day
   15-day retention = 37GB Ã— 15 = 555 GB

5. KAFKA PARTITIONS:
   3M events/sec Ã· 50K/partition = 60 partitions
   With 3x replication = 180 partition replicas
   Brokers needed: 20-30 (r5.2xlarge)

6. ELASTICSEARCH CLUSTER:
   2.3 PB Ã· 50GB/shard = 46,000 shards
   With 1 replica = 92,000 total shards
   Cluster size: 100-150 nodes (tiered)
```
</details>

---

### Problem 2: SaaS Platform
```
Given:
- 50,000 tenants
- 200 servers
- Each tenant: 10 metrics
- Each server: 1000 metrics
- Average request: 2 log lines
- Request rate: 10K req/sec

Calculate:
1. Total metric cardinality
2. Is this safe for single Prometheus?
3. Daily log volume
4. Estimated monthly AWS cost (rough)

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. METRIC CARDINALITY:
   Tenant metrics: 50K tenants Ã— 10 = 500K
   Server metrics: 200 servers Ã— 1000 = 200K
   Total = 500K + 200K = 700K time series

2. PROMETHEUS SAFETY:
   700K series is SAFE for single Prometheus instance
   (Prometheus handles up to 1M series comfortably)
   Use r5.2xlarge (8 vCPU, 64GB RAM)

3. DAILY LOG VOLUME:
   Events/sec = 10K req/sec Ã— 2 logs + 200 servers Ã— 100 = 40K events/sec
   Daily = 40K Ã— 86,400 Ã— 500 bytes = 1.7 TB/day (raw)
   Compressed = 1.7TB Ã· 5 = 350 GB/day

4. MONTHLY AWS COST (rough estimate):
   Elasticsearch (10 nodes i3.xlarge): $3,000
   Kafka (3 brokers r5.large): $500
   Prometheus (2 instances r5.2xlarge): $800
   S3 storage (10TB): $250
   Data transfer: $500
   Total: ~$5,000-6,000/month
```
</details>

---

## ğŸ“š PART 12: Additional Resources

**Books:**
- "Distributed Systems Observability" by Cindy Sridharan
- "Observability Engineering" by Charity Majors, Liz Fong-Jones
- "Site Reliability Engineering" by Google

**Industry Benchmarks:**
- Elasticsearch: 10-20K writes/sec per node
- Kafka: 50-100K events/sec per partition
- Prometheus: 1M active series per instance
- Fluentd: 10-20K events/sec per core

**Real-World Examples:**
- Uber: 100M+ metrics, 10PB logs/year
- Netflix: 2.5M metrics, 1 trillion events/day
- Datadog: Handles 1M+ metrics/sec for customers

---

**Remember:**
> "In observability systems, DATA VOLUME is the primary cost driver. Every optimization that reduces volume pays massive dividends."

**Key Insight:**
> "The 80-20 rule applies EVERYWHERE: 20% of logs contain 80% of value. Sample aggressively!"

---

*Created with the DELTA technique: Data â†’ Events â†’ Latency â†’ Throughput â†’ Archive*
*Perfect for: FAANG interviews, SRE roles, Platform Engineering, Observability teams*
