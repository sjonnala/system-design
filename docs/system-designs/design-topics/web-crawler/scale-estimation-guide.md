# ğŸ¯ Web Crawler System Design: Scale Estimation Masterclass

## The POWER Technique for Scale Math
**(P)rinciples â†’ (O)rder of magnitude â†’ (W)rite it down â†’ (E)stimate â†’ (R)ound ruthlessly**

This is a **mental framework** you can apply to ANY distributed system design problem.

---

## ğŸ“Š PART 1: Web Crawling Scale Estimation

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **Crawl Scope** | Total web pages (indexed) | 50 Billion | Similar to Google's index size |
| | New pages per day | 5 Million | Growing web content |
| | Re-crawl frequency (popular) | 1 day | News sites, trending content |
| | Re-crawl frequency (normal) | 7 days | Regular websites |
| | Re-crawl frequency (archive) | 30 days | Old, stable content |
| **Distribution** | Popular pages (daily crawl) | 10% | News, social, trending |
| | Normal pages (weekly crawl) | 60% | Regular content |
| | Archive pages (monthly crawl) | 30% | Old, stable pages |
| **Page Characteristics** | Average page size | 100 KB | HTML + text + metadata |
| | Average links per page | 100 | Typical web page |
| | Avg fetch latency | 500ms | Network + server response |

---

## ğŸ§® PART 2: The "Coffee Shop Calculator" - Your Mental Math Toolkit

### Rule #1: **The Power of 10 Ladder**
```
Remember these anchors:
â€¢ 1 Thousand = 10^3    (3 zeros)  â†’ "K for Thousand"
â€¢ 1 Million = 10^6     (6 zeros)  â†’ "M for Million"
â€¢ 1 Billion = 10^9     (9 zeros)  â†’ "B for Billion"
â€¢ 1 Day = ~100K seconds (86,400) â†’ "Day â‰ˆ 10^5 sec"
â€¢ 1 Week = ~600K seconds         â†’ "Week â‰ˆ 6 * 10^5"
â€¢ 1 Month = ~2.5M seconds        â†’ "Month â‰ˆ 2.5 * 10^6"
```

### Rule #2: **The Division Shortcut**
Instead of dividing by complex numbers:
```
âœ— BAD:  5,000,000 Ã· 86,400
âœ“ GOOD: 5M Ã· 100K = 50 pages/sec
        (Just subtract the zeros: 6 zeros - 5 zeros = 1 zero = 50)
```

### Rule #3: **The Parallel Scaling Trick**
When calculating throughput:
```
Don't think: "How fast can ONE crawler go?"
Instead think: "How many crawlers do I need for target throughput?"

Target: 1,500 pages/sec
Single crawler: 50 pages/sec
Crawlers needed: 1,500 Ã· 50 = 30 crawler instances
```

---

## ğŸ“ˆ PART 3: Quick Scale Math Template (COPY THIS!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ THE NAPKIN MATH TEMPLATE - Web Crawler Edition     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: CRAWL RATE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Pages to Maintain:   [____] B
Daily Recrawl (10%):       [____] M
Weekly Recrawl (60%):      [____] M/7 daily
Monthly Recrawl (30%):     [____] M/30 daily

â†’ Total Pages/Day = Daily + Weekly + Monthly = [____] M
â†’ Pages/Sec = Total Pages/Day Ã· 100K = [____] pages/sec
â†’ Peak Pages/Sec = Avg Ã— 2 = [____] pages/sec

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Avg Page Size:             [____] KB
Total Pages:               [____] B
Storage Format:            Compressed (3:1 ratio)

â†’ Raw Storage = Pages Ã— Size       = [____] TB
â†’ Compressed = Raw Ã· 3             = [____] TB
â†’ With Replicas (3x) = Compressed Ã— 3 = [____] PB

STEP 3: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Ingress (Download) = Pages/Sec Ã— Avg Size = [____] MB/s = [____] Gbps
â†’ Egress (Storage writes) = Download Ã· 3 (compressed) = [____] MB/s

STEP 4: COMPUTE (CRAWLER INSTANCES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Single Crawler Rate:       [____] pages/sec (limited by network)
Target Rate:               [____] pages/sec

â†’ Crawlers Needed = Target Ã· Single Rate = [____] instances
â†’ With 30% headroom = Crawlers Ã— 1.3 = [____] instances
```

---

## ğŸ’¾ PART 4: Web Crawler Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      WEB CRAWLER SYSTEM - NAPKIN MATH SOLUTION          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: CRAWL RATE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Pages to Maintain:   50 Billion
Daily Recrawl (10%):       5 Billion pages (every day)
Weekly Recrawl (60%):      30 Billion pages Ã· 7 = 4.3 Billion/day
Monthly Recrawl (30%):     15 Billion pages Ã· 30 = 0.5 Billion/day

â†’ Total Pages/Day = 5B + 4.3B + 0.5B = ~10 Billion pages/day
â†’ Pages/Sec = 10B Ã· 100K = 100,000 pages/sec
â†’ Peak Pages/Sec = 100K Ã— 2 = 200,000 pages/sec

Wait! This is TOO HIGH for real-world politeness constraints.

ADJUSTED (With Politeness):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Politeness: 1 req/sec per domain
Unique Domains: ~200 Million
Parallelizable Requests: 200M domains Ã— 1 req/sec = 200M req/sec (theoretical max)

Realistic Target: 1,500 pages/sec (with 200M domains, plenty of headroom)
This means: 1,500 Ã· 200M = 0.0000075 req/sec per domain (very polite!)

â†’ Pages/Day = 1,500 Ã— 100K = 150 Million pages/day
â†’ Pages/Month = 150M Ã— 30 = 4.5 Billion pages/month

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Avg Page Size:
  - HTML content:            80 KB
  - Extracted text:          15 KB
  - Metadata (URL, timestamp): 5 KB
  - Total per page:          100 KB

Total Pages:               50 Billion
Storage Format:            Compressed gzip (3:1 ratio)

â†’ Raw Storage = 50B Ã— 100KB    = 5,000 PB = 5 EB
â†’ Compressed = 5 EB Ã· 3        = 1.67 EB â‰ˆ 1.7 EB
â†’ With Replicas (3x) = 1.7 EB Ã— 3 = ~5 EB total

Reality Check: For a crawler like Google's scale
For smaller crawlers (1B pages): 100 TB compressed

STEP 3: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Crawl Rate: 1,500 pages/sec
Avg Page Size: 100 KB

â†’ Ingress (Download) = 1,500 Ã— 100KB  = 150 MB/s = 1.2 Gbps
â†’ Compressed writes = 150 MB/s Ã· 3    = 50 MB/s = 400 Mbps
â†’ Peak (2x) = 1.2 Gbps Ã— 2            = 2.4 Gbps

STEP 4: COMPUTE (CRAWLER INSTANCES)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Single Crawler Rate:       50 pages/sec (network limited, politeness)
Target Rate:               1,500 pages/sec

â†’ Crawlers Needed = 1,500 Ã· 50      = 30 instances
â†’ With 30% headroom = 30 Ã— 1.3      = 40 instances
â†’ CPU per instance: 4 cores, 16GB RAM
â†’ Total: 160 cores, 640GB RAM

STEP 5: URL FRONTIER ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Frontier Size (pending URLs): 50 Million URLs
URL Entry Size:
  - URL (avg 100 chars):     100 bytes
  - Priority score:          4 bytes
  - Timestamp:               8 bytes
  - Domain hash:             8 bytes
  - Total per URL:           120 bytes

â†’ Frontier Memory = 50M Ã— 120 bytes = 6 GB
â†’ Redis cluster (with overhead) = 10 GB

STEP 6: DEDUPLICATION STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total URLs (seen): 100 Billion (2x page count, including duplicates)
Storage per URL hash: 32 bytes (SHA-256 hash)

â†’ Hash Table = 100B Ã— 32 bytes = 3.2 TB
â†’ Bloom Filter (10x compression) = 320 GB

Using Bloom filter + Redis:
â†’ Bloom Filter: 320 GB (in-memory)
â†’ Exact hashes: 3.2 TB (distributed Redis shards)
```

---

## ğŸ§  PART 5: Mental Math Techniques You MUST Master

### **Technique 1: The "Powers of 2" Ladder**
*(For Storage Calculations)*
```
Memorize this progression:
2^10 = 1 KB   (1,024 bytes)
2^20 = 1 MB   (1 Million bytes)
2^30 = 1 GB   (1 Billion bytes)
2^40 = 1 TB   (1 Trillion bytes)
2^50 = 1 PB   (1 Quadrillion bytes)
2^60 = 1 EB   (1 Quintillion bytes)

Pro Tip: Each step is ~1000Ã— the previous
```

### **Technique 2: The "Pages Per Second" Shortcut**
*(For Throughput Calculations)*
```
EMOTION TRIGGER: "A day is almost 100K seconds!"

Pages per day â†’ Pages per second:
150M pages/day Ã· 100K = 1,500 pages/sec

Easy!
```

### **Technique 3: The "Parallel Domains" Insight**
*(For Understanding Politeness Constraints)*
```
KEY INSIGHT: With millions of unique domains, you can crawl
             1 page/sec from EACH domain simultaneously!

Domains: 200M
Rate per domain: 1 req/sec
Theoretical max: 200M req/sec

But realistic (with overhead): 1K - 100K pages/sec
```

### **Technique 4: The "Compression Multiplier"**
```
ğŸ¯ WEB CONTENT COMPRESSES ~3:1 with gzip

- Raw HTML: 100 KB
- Compressed: ~33 KB
- Storage needed: 1/3 of raw size

ALWAYS account for compression in storage estimates!
```

---

## ğŸ¨ PART 6: The Visual Mind Map Approach

```
                ğŸ•·ï¸ WEB CRAWLER SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“Š CRAWL          ğŸ’¾ STORAGE        ğŸ”§ COMPUTE
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  Rate   Domains   Pages   Size    Instances  Memory
 1500/s   200M      50B    1.7EB     40      640GB
```

**Memory Trigger**: Think **"C.S.C."** = Crawl rate, Storage, Compute

---

## ğŸ—ï¸ PART 7: Domain Model for Web Crawler

```python
# Think in terms of domain entities first!

@dataclass
class CrawledPage:
    # THINK: What's the STORAGE pattern?
    url: str                  # 100 bytes avg
    url_hash: str             # 32 bytes (SHA-256)
    html_content: str         # 80 KB avg (compressed: 27 KB)
    extracted_text: str       # 15 KB avg (compressed: 5 KB)
    metadata: dict            # timestamp, status, content_type
    outgoing_links: list[str] # 100 links Ã— 100 bytes = 10 KB

    # Scale Insight: 50B pages Ã— 100KB = 5 EB raw storage!

@dataclass
class URLFrontierEntry:
    # THINK: What's the QUEUE pattern?
    url: str                  # 100 bytes
    priority_score: float     # 4 bytes
    domain: str               # 50 bytes
    scheduled_time: datetime  # 8 bytes
    depth: int                # 4 bytes

    # Scale Insight: 50M URLs in frontier Ã— 166 bytes = 8.3 GB

@dataclass
class CrawlerWorker:
    # THINK: What's the THROUGHPUT pattern?
    worker_id: str
    assigned_domains: list[str]  # Domain partitioning
    fetch_rate: int = 50         # pages/sec per worker

    # Scale Insight: 1,500 pages/sec Ã· 50 = 30 workers minimum
```

---

## ğŸ¯ PART 8: The Interview Cheat Sheet (Print This!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYSTEM DESIGN SCALE ESTIMATION - 5 MIN RITUAL   â”‚
â”‚              WEB CRAWLER EDITION                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Clarify: Total pages? New pages/day? Re-crawl frequency?
[ ] 2. Calculate Pages/Sec: Pages/day Ã· 100K
[ ] 3. Adjust for Politeness: Limit by domains, not raw throughput
[ ] 4. Calculate Storage: Pages Ã— Size Ã— Compression
[ ] 5. Calculate Crawlers: Target rate Ã· Single crawler rate
[ ] 6. Smell Test: Does 1,500 pages/sec sound reasonable? YES!
        (Not 1M pages/sec - too high, politeness violated)
```

---

## ğŸš€ Key Metrics Summary Table

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Crawl Rate** | 1,500 pages/sec | Determines crawler fleet size |
| **Pages/Day** | 150 Million | Daily throughput capacity |
| **Total Pages** | 50 Billion | Index size (like Google) |
| **Storage** | 1.7 EB compressed | S3/HDFS sizing (for 50B pages) |
| **Bandwidth** | 1.2 Gbps ingress | Network capacity needed |
| **Crawler Instances** | 30-40 workers | Horizontal scaling |
| **URL Frontier** | 50M URLs, 10 GB | Redis/Kafka sizing |
| **Dedup Storage** | 320 GB Bloom + 3.2 TB hashes | Memory + distributed cache |

---

## ğŸ’¡ Pro Architect Tips

### **Tip 1: The Politeness Reality Check**
After throughput calculations, ask:
- "Can I actually crawl this fast without violating politeness?" â†’ Probably NOT!
- "With 200M domains at 1 req/sec each, what's my theoretical max?" â†’ 200M req/sec
- "What's realistic with overhead?" â†’ 1K-100K pages/sec

### **Tip 2: The Compression Savings**
Always remember:
- "HTML compresses ~3:1 with gzip"
- "5 EB raw â†’ 1.7 EB compressed â†’ HUGE cost savings!"

### **Tip 3: Start with Constraints**
Always ask first:
1. How many pages to index?
2. How often to re-crawl?
3. Politeness requirements? (req/sec per domain)
4. Storage retention period?

---

## ğŸ“ Professor's Final Wisdom

> **"In web crawler design, POLITENESS beats SPEED. A slow, respectful crawler is better than a fast, banned crawler!"**

Your interviewer wants to see:
1. âœ… Understanding of politeness constraints
2. âœ… Realistic throughput estimates (not theoretical max)
3. âœ… Storage compression awareness
4. âœ… Distributed system thinking (workers, sharding)

**NOT NEEDED:**
- âŒ Exact page counts
- âŒ Complex politeness algorithms
- âŒ Memorized web statistics

---

## ğŸ” Repetition Backed by Emotion (Your Power Principle!)

**REPEAT 3 TIMES OUT LOUD:**
1. *"Politeness limits my throughput - not bandwidth, not CPU!"*
2. *"Compression saves 3x on storage - always factor it in!"*
3. *"Distribute by domain - each worker owns specific domains!"*

**VISUALIZE:** You're at the whiteboard, the interviewer nods as you confidently say: "So with 200 million domains and 1 request/sec per domain, we have plenty of headroom for 1,500 pages/sec..."

---

## ğŸ“š Quick Reference: Crawler Scale Benchmarks

| **Crawler Type** | **Pages Indexed** | **Crawl Rate** | **Storage** | **Workers** |
|------------------|-------------------|----------------|-------------|-------------|
| Small (Focused) | 1-10 Million | 10-50 pages/sec | 1-10 TB | 5-10 |
| Medium (Enterprise) | 100M - 1B | 100-500 pages/sec | 10-100 TB | 20-50 |
| Large (Google-scale) | 10B - 100B | 1K-10K pages/sec | 1-10 EB | 100-1000 |
| Specialized (News) | 10-100 Million | 50-200 pages/sec | 5-50 TB | 10-30 |

---

## ğŸ”§ Practical Application: Adapting This Template

### For a **News Crawler** (high freshness requirement):
```
STEP 1: CRAWL RATE
- Focus: News sites (1M pages)
- Re-crawl: Every 15 minutes
- Pages/Day: 1M Ã— (24 Ã— 4) = 96M pages/day
- Pages/Sec: 96M Ã· 100K = 960 pages/sec

STEP 2: STORAGE
- News retention: 30 days
- Size: 1M Ã— 100KB Ã— 30 = 3 TB (uncompressed)
- Compressed: 1 TB

STEP 3: CRAWLERS
- Target: 960 pages/sec
- Single crawler: 50 pages/sec
- Needed: 20 crawlers
```

### For a **E-commerce Product Crawler**:
```
STEP 1: CRAWL RATE
- Products: 100M (Amazon, eBay, etc.)
- Re-crawl: Daily (prices change)
- Pages/Day: 100M
- Pages/Sec: 100M Ã· 100K = 1,000 pages/sec

STEP 2: STORAGE
- Product page: 50 KB (smaller than general web)
- Compressed: ~17 KB
- Total: 100M Ã— 17KB = 1.7 TB

STEP 3: PARSING
- Extract: Price, title, reviews, images
- Structured data (JSON) storage
```

### For a **Social Media Crawler** (Twitter, Reddit):
```
STEP 1: CRAWL RATE
- Public posts: Real-time stream (10K posts/sec)
- User profiles: 500M (weekly re-crawl)
- Pages/Day: (10K Ã— 86.4K) + (500M Ã· 7) = 864M + 71M = 935M

STEP 2: STORAGE
- Post: 1 KB (text, metadata)
- Daily: 864M Ã— 1KB = 864 GB/day
- 30-day retention: 25 TB
```

---

## ğŸ¯ Mental Math Practice Problems

### Problem 1: Academic Research Crawler
```
Given:
- Academic papers: 200M papers
- New papers/year: 5M
- Re-crawl: Quarterly (every 90 days)
- Paper size: 2 MB (PDF)
- Extract: Abstract, citations (50 KB)

Calculate:
1. Crawl rate (pages/sec)
2. Storage for PDFs (compressed)
3. Storage for extracted data
4. Number of crawler workers

[Try it yourself, then check answers below]
```

<details>
<summary>Answer</summary>

```
1. CRAWL RATE:
   - Total papers: 200M
   - Re-crawl frequency: Quarterly
   - Papers/Quarter: 200M
   - Pages/Day = 200M Ã· 90 = 2.2M/day
   - Pages/Sec = 2.2M Ã· 100K = 22 pages/sec

2. STORAGE (PDFs):
   - Size per PDF: 2 MB
   - Compression: ~2:1 for PDFs
   - Compressed size: 1 MB
   - Total: 200M Ã— 1MB = 200 TB

3. STORAGE (Extracted):
   - Per paper: 50 KB
   - Total: 200M Ã— 50KB = 10 TB

4. CRAWLERS:
   - Target rate: 22 pages/sec
   - Single crawler: 10 pages/sec (PDFs are slower)
   - Needed: 22 Ã· 10 = 3 crawlers (round to 5 with headroom)
```
</details>

---

### Problem 2: Government Website Archiver
```
Given:
- Government sites: 10,000 sites
- Total pages: 50M
- Re-crawl: Monthly (compliance)
- Avg page: 150 KB
- Retention: 10 years (for legal compliance)
- Must keep historical versions

Calculate:
1. Crawl rate
2. Monthly storage delta
3. Total storage after 10 years
4. Bandwidth requirements

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. CRAWL RATE:
   - Pages: 50M
   - Re-crawl: Monthly
   - Pages/Day = 50M Ã· 30 = 1.67M/day
   - Pages/Sec = 1.67M Ã· 100K = 17 pages/sec

2. MONTHLY STORAGE:
   - Pages: 50M
   - Size: 150 KB
   - Compressed (3:1): 50 KB
   - Monthly: 50M Ã— 50KB = 2.5 TB/month

3. TOTAL STORAGE (10 years):
   - Keeping all versions (12 Ã— 10 = 120 snapshots)
   - Total: 2.5 TB Ã— 120 = 300 TB
   - With deduplication (60% savings): 120 TB

4. BANDWIDTH:
   - Crawl: 17 pages/sec Ã— 150 KB = 2.55 MB/s = 20 Mbps
   - Storage writes: 2.55 MB/s Ã· 3 (compressed) = 850 KB/s = 7 Mbps
```
</details>

---

## ğŸš¨ Common Mistakes to Avoid

### Mistake 1: **Ignoring Politeness**
```
âœ— BAD:  "We can crawl 1M pages/sec with enough machines!"
âœ“ GOOD: "Politeness limits us to 1-5 req/sec per domain.
         With 200M domains, realistic max is 10K-50K pages/sec"
```

### Mistake 2: **Forgetting Compression**
```
âœ— BAD:  "50B pages Ã— 100KB = 5 EB storage needed"
âœ“ GOOD: "50B pages Ã— 100KB = 5 EB raw, but compressed
         (3:1) = 1.7 EB, still massive but 3x cheaper!"
```

### Mistake 3: **Not Considering Re-crawl Frequency**
```
âœ— BAD:  "Crawl 50B pages once, done!"
âœ“ GOOD: "Different pages have different freshness needs:
         - News: Daily (10%)
         - Normal: Weekly (60%)
         - Archive: Monthly (30%)"
```

### Mistake 4: **Over-precision with Domains**
```
âœ— BAD:  "Exactly 247,382,941 domains to crawl"
âœ“ GOOD: "Roughly 200-250 million unique domains, call it 200M"
```

### Mistake 5: **Forgetting URL Frontier Size**
```
âœ— BAD:  "Just crawl URLs from database"
âœ“ GOOD: "URL Frontier queue with 50M pending URLs,
         needs 10 GB RAM, distributed across Kafka/Redis"
```

---

## ğŸ“ Your Practice Template (Fill-in-the-Blank)

```
WEB CRAWLER: ___________________

STEP 1: CRAWL RATE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Pages to Index:     [____] M/B
New Pages/Day:            [____] M
Re-crawl Frequency:
  - Hot (daily):          [____]% = [____] pages
  - Normal (weekly):      [____]% = [____] pages
  - Archive (monthly):    [____]% = [____] pages

Total Pages/Day:          [____] M
â†’ Pages/Sec = [____] M Ã· 100K = [____] pages/sec

Politeness Check:
  - Unique domains:       [____] M
  - Rate per domain:      [____] req/sec
  - Theoretical max:      [____] req/sec
  - Realistic target:     [____] pages/sec âœ“

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Avg Page Size:            [____] KB
Total Pages:              [____] B
Compression Ratio:        3:1

â†’ Raw Storage = [____] B Ã— [____] KB = [____] TB/PB/EB
â†’ Compressed = [____] Ã· 3             = [____] TB/PB/EB
â†’ With Replicas (3x) = [____] Ã— 3     = [____] TB/PB/EB

STEP 3: BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Ingress = [____] pages/sec Ã— [____] KB = [____] MB/s = [____] Gbps
â†’ Egress (compressed) = [____] MB/s Ã· 3 = [____] MB/s

STEP 4: COMPUTE (CRAWLERS)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Single Crawler Rate:      [____] pages/sec
Target Rate:              [____] pages/sec

â†’ Crawlers = [____] Ã· [____]     = [____] instances
â†’ With headroom (1.3x) = [____]  = [____] instances

STEP 5: URL FRONTIER
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Frontier Size:            [____] M URLs
Size per URL:             120 bytes

â†’ Frontier Memory = [____] M Ã— 120B = [____] GB

STEP 6: DEDUPLICATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total URLs (seen):        [____] B
Hash size:                32 bytes

â†’ Hash table = [____] B Ã— 32B = [____] TB
â†’ Bloom filter = [____] TB Ã· 10 = [____] GB

SMELL TEST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Crawl rate respects politeness? (<1% of max)
â–¡ Storage reasonable? (TB for small, PB for large, EB for Google-scale)
â–¡ Bandwidth achievable? (Gbps range typical for medium crawlers)
â–¡ Crawler count realistic? (10s-100s for medium scale)
```

---

## ğŸ Bonus: Web Crawler Scale Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        WEB CRAWLER SCALE ESTIMATION CHEAT SHEET        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 1 Day      = 100K seconds (~86.4K)
â€¢ Avg Page   = 100 KB (HTML + text)
â€¢ Compression = 3:1 (gzip)
â€¢ Politeness = 1-5 req/sec per domain
â€¢ Unique Domains â‰ˆ 200M (global web)

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Pages/Sec = Pages/Day Ã· 100K
Storage = Pages Ã— Page_Size Ã· 3 (compression)
Crawlers = Target_Rate Ã· Single_Crawler_Rate
Frontier Memory = Pending_URLs Ã— 120 bytes

POLITENESS CONSTRAINTS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Rate Limit: 1-5 req/sec per domain (typical)
â€¢ Respect robots.txt: ALWAYS
â€¢ Crawl-delay: Honor directive
â€¢ Realistic Max: 1K-100K pages/sec (not millions!)

TYPICAL SCALES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:    1M-10M pages,    10-50 pages/sec,  1-10 TB
Medium:   100M-1B pages,   100-500 pages/sec, 10-100 TB
Large:    10B-100B pages,  1K-10K pages/sec,  1-10 EB

QUICK ESTIMATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1B pages:
  - Storage (compressed): 100 TB Ã· 3 = 33 TB
  - Re-crawl weekly: 1B Ã· 7 Ã· 100K = 1.4 pages/sec
  - Crawlers needed: 1.4 Ã· 50 = 1 crawler (start small!)

10B pages (Google-scale):
  - Storage: 1 EB Ã· 3 = 333 PB
  - Re-crawl (mixed): ~10B/day Ã· 100K = 100K pages/sec (theoretical)
  - Realistic (politeness): 1K-10K pages/sec
  - Crawlers: 1K Ã· 50 = 20-200 instances

INTERVIEW FLOW:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Clarify requirements (5 min)
   â†’ How many pages? Re-crawl frequency? Freshness needs?

2. High-level estimates (5 min)
   â†’ Pages/sec (with politeness!), Storage, Bandwidth
   â†’ "Let me do some napkin math..."

3. System design (20 min)
   â†’ URL Frontier, Crawlers, Storage, Deduplication
   â†’ "Since we need politeness, we'll use domain-based queues..."

4. Deep dives (10 min)
   â†’ Based on scale, what's critical?
   â†’ "At this scale, deduplication is crucial - Bloom filter + Redis..."

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Crawl rate < 1% of theoretical max (politeness)? YES
âœ“ Storage in TB-PB range (not GB or EB for normal scale)? YES
âœ“ Compression applied (3:1 for HTML)? YES
âœ“ Crawler count reasonable (10s-100s for medium)? YES

âœ— Crawling 1M pages/sec (without billions of domains)? NO
âœ— Storing uncompressed HTML? NO (wasteful!)
âœ— Single crawler handling 10K pages/sec? NO (network limits)
âœ— No URL deduplication (will crawl duplicates)? NO (critical!)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final Challenge: Apply This Template

Pick one of these crawler types and practice the full estimation:

1. **E-commerce Price Tracker** - Track prices across 1M products
2. **Social Media Archiver** - Archive public posts from Twitter/Reddit
3. **Academic Paper Crawler** - Index 100M research papers
4. **Job Board Aggregator** - Crawl job postings from 1000 sites
5. **News Aggregator** - Real-time crawling of news websites

Use the blank template above and time yourself: **Can you complete it in 5-7 minutes?**

---

## ğŸ“š Additional Resources

- **Books**:
  - "Web Crawling and Data Mining with Apache Nutch" by Zakir Laliwala
  - "Mining the Web: Discovering Knowledge from Hypertext Data" by Soumen Chakrabarti
- **Papers**:
  - "Mercator: A Scalable, Extensible Web Crawler" (Google)
  - "The Anatomy of a Large-Scale Hypertextual Web Search Engine" (Google)
- **Open Source**: Apache Nutch, Scrapy, Heritrix, Colly
- **Practice**: System Design Interview questions, real crawler implementations

---

**Remember**:
> "The goal isn't perfection - it's demonstrating systematic thinking, understanding of politeness constraints, and realistic capacity planning for distributed crawling at scale."

**Now go build that crawler!** ğŸ•·ï¸

---

*Created with the POWER technique: Principles â†’ Order â†’ Write â†’ Estimate â†’ Round*
*Perfect for: FAANG interviews, Distributed Systems rounds, Big Data Architecture discussions*
