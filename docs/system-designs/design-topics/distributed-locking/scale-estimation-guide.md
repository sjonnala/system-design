# ğŸ¯ Distributed Locking System: Scale Estimation Masterclass

## The LOCKS Technique for Lock System Capacity Planning
**(L)atency requirements â†’ (O)perations per second â†’ (C)luster sizing â†’ (K)ey metrics â†’ (S)torage calculation**

This is a **mental framework** specifically designed for distributed coordination systems.

---

## ğŸ“Š PART 1: Understanding Lock System Scale

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **Client Base** | Total Microservices | 500 | Medium-sized distributed system |
| | Active Instances | 2,000 | ~4 instances per service |
| | Concurrent Workers | 10,000 | Including background jobs |
| **Lock Patterns** | Locks/Instance/Min | 10 | Critical sections rate |
| | Lock Hold Duration | 5 seconds | Avg critical section time |
| | Lock Renewal Frequency | 2.5s | TTL/2 for 5s locks |
| | Read:Write Ratio | 3:1 | Check lock status vs acquire |
| **Temporal** | Peak Hour Multiplier | 3x | Business hours concentration |
| | Lock TTL (default) | 30 seconds | Safety margin |
| | Heartbeat Interval | 15 seconds | TTL/2 |

---

## ğŸ§® PART 2: The "Lock Math Calculator" - Mental Model

### Rule #1: **The Lock Operation Ladder**
```
Lock Operations Hierarchy:
â€¢ Acquire Lock    â†’ Expensive (consensus write)
â€¢ Renew Lock      â†’ Medium   (consensus write, but cached check)
â€¢ Check Lock      â†’ Cheap    (read from follower)
â€¢ Release Lock    â†’ Medium   (consensus write + cleanup)

Cost Ratio: Acquire:Renew:Check:Release = 10:5:1:5
```

### Rule #2: **The Consensus Tax**
Every write operation requires consensus (Raft/Paxos):
```
Consensus Overhead:
âœ— BAD THINKING:  "1 lock operation = 1 database write"
âœ“ GOOD THINKING: "1 lock operation = 1 leader write + N-1 follower replicates + quorum wait"

Actual work: 1 operation Ã— 5 nodes Ã— (write + network + ACK) = ~5Ã— base cost
```

### Rule #3: **The Heartbeat Multiplier**
Long-running locks generate continuous heartbeat traffic:
```
If lock held for 5 minutes with 15s heartbeat interval:
â†’ 1 acquire + (300s / 15s) renewals + 1 release
â†’ 1 + 20 + 1 = 22 operations per lock lifecycle

Heartbeat traffic = Acquire rate Ã— (Avg hold duration / Heartbeat interval)
```

---

## ğŸ“ˆ PART 3: Distributed Locking Scale Math Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”’ NAPKIN MATH TEMPLATE - Distributed Lock System          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: LOCK OPERATION ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active Instances:        [____]
Locks per Instance/Min:  [____]
Avg Lock Hold Duration:  [____] seconds

â†’ Acquire QPS  = Instances Ã— Locks/min Ã· 60     = [____]
â†’ Renew QPS    = Acquire QPS Ã— (Duration / Heartbeat) = [____]
â†’ Release QPS  = Acquire QPS (same as acquire)  = [____]
â†’ Total Write QPS = Acquire + Renew + Release   = [____]

STEP 2: CONSENSUS CLUSTER SIZING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write QPS per Node:      [____] (Total / Leader capacity)
Quorum Size:             [____] (N/2 + 1 for N nodes)
Failure Tolerance:       [____] nodes (N - Quorum)

â†’ Cluster Size = Max(5 nodes, TotalQPS / NodeCapacity)
â†’ Leader capacity (etcd): ~10K QPS
â†’ Leader capacity (ZK):   ~5K QPS

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lock Entry Size:         [____] bytes
  - Resource name (100B) + Owner (50B) + Metadata (50B) = ~200B
Active Locks (peak):     [____]
Lock History Retention:  [____] days

â†’ Active Lock Storage = Active Ã— 200B     = [____] MB
â†’ History per Day = QPS Ã— 86400 Ã— 200B    = [____] GB/day
â†’ Total Storage = Active + (History Ã— Days) = [____] GB

STEP 4: NETWORK BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Write BW (leader) = Write QPS Ã— 200B Ã— 5 (replication) = [____] MB/s
â†’ Read BW (followers) = Read QPS Ã— 200B    = [____] MB/s

STEP 5: LATENCY BUDGET
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Target P99 Lock Acquisition: [____] ms (typically <10ms)

Breakdown:
  - Network RTT (client â†’ leader): 1-2ms
  - Leader log append: 1ms
  - Replication to followers: 2-3ms (parallel)
  - Quorum wait: 2-3ms
  - Response: 1-2ms
â†’ Total: ~7-12ms (healthy cluster, same datacenter)
```

---

## ğŸ’¾ PART 4: Distributed Locking Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     DISTRIBUTED LOCK SYSTEM - NAPKIN MATH SOLUTION          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: LOCK OPERATION ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active Instances:        2,000
Locks per Instance/Min:  10
Avg Lock Hold Duration:  300 seconds (5 minutes)
Heartbeat Interval:      15 seconds

â†’ Acquire QPS  = 2000 Ã— 10 Ã· 60         = 333 QPS
â†’ Renewals per Lock = 300 / 15          = 20 renewals
â†’ Renew QPS    = 333 Ã— 20               = 6,660 QPS
â†’ Release QPS  = 333                    = 333 QPS
â†’ Total Write QPS = 333 + 6660 + 333    = 7,326 QPS

Peak Multiplier (3x): 7,326 Ã— 3 = ~22,000 QPS

STEP 2: CONSENSUS CLUSTER SIZING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Target: Handle 22K QPS writes (peak)

Using etcd (leader capacity: ~10K QPS):
â†’ Need 3 leaders (sharding) â†’ 3 clusters OR
â†’ Single cluster with read offloading

Recommended: 5-node cluster (tolerates 2 failures)
  - 1 leader handles writes (up to 10K QPS)
  - 4 followers handle reads
  - If exceeds 10K QPS â†’ Shard by resource hash

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lock Entry: 200 bytes
  - Resource name: "payment:order:abc123" â†’ 100B
  - Owner: "payment-service-pod-42" â†’ 50B
  - Metadata: {fencingToken, expiresAt, acquiredAt} â†’ 50B

Active Locks (peak):
  - Avg hold: 300s, acquire rate: 333/s
  - Active = 333 QPS Ã— 300s = 100,000 concurrent locks

â†’ Active Lock Storage = 100K Ã— 200B = 20 MB

Lock History (30 days retention):
  - Operations/day = 7326 QPS Ã— 86,400s = 632M ops/day
  - Storage/day = 632M Ã— 200B = 126 GB/day
  - 30-day storage = 126GB Ã— 30 = 3.8 TB

Total Storage: 20 MB (active) + 3.8 TB (history) â‰ˆ 3.8 TB

STEP 4: NETWORK BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write QPS: 7,326 (average), 22K (peak)

â†’ Leader Write BW = 7326 QPS Ã— 200B Ã— 5 nodes = 7.3 MB/s avg, 22 MB/s peak
â†’ Follower Read BW = (Check QPS) Ã— 200B
  - If 1/3 checks â†’ 2,442 QPS Ã— 200B = 0.5 MB/s

Total Network: ~25 MB/s peak (well within 1 Gbps NIC)

STEP 5: LATENCY ANALYSIS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Target P99: <10ms

Same-datacenter 5-node cluster:
  - Network RTT: 1ms
  - Leader append: 1ms
  - Replication (parallel): 2ms
  - Quorum ACK (3/5): 2ms
  - Response: 1ms
â†’ Total: 7ms P99 âœ…

Cross-region cluster (NOT recommended for locks):
  - Network RTT: 50ms (US-East to US-West)
  - Total: ~55ms P99 âŒ Too slow for locks
```

---

## ğŸ§  PART 5: Mental Math Techniques for Lock Systems

### **Technique 1: The Concurrent Lock Estimator**
```
Active Locks = Acquire Rate (QPS) Ã— Avg Hold Duration (seconds)

Example:
  - 100 locks/sec acquired
  - Average hold: 60 seconds
  - Active locks = 100 Ã— 60 = 6,000 concurrent locks

MEMORY TRIGGER: "Locks are like cars in a parking lot.
  Arrival rate Ã— parking duration = cars in lot at any time"
```

### **Technique 2: The Heartbeat Tax Calculator**
```
Renewal overhead = (Lock hold duration / Heartbeat interval) - 1

Example:
  - 5-minute lock (300s)
  - 15s heartbeat
  - Renewals = (300 / 15) = 20 renewals per lock
  - If 100 locks/sec acquired â†’ 100 Ã— 20 = 2,000 renewals/sec

RULE OF THUMB: Renew QPS â‰ˆ 10Ã— to 20Ã— Acquire QPS (for long locks)
```

### **Technique 3: The Quorum Safety Check**
```
Failure Tolerance = (Cluster Size / 2) - 1 (rounded down)

3 nodes â†’ tolerates 1 failure (quorum = 2)
5 nodes â†’ tolerates 2 failures (quorum = 3)
7 nodes â†’ tolerates 3 failures (quorum = 4)

WHY NOT MORE? Consensus latency increases with cluster size!
  - More nodes = more network hops
  - Diminishing returns beyond 5-7 nodes
  - Use sharding instead of giant clusters
```

### **Technique 4: The Contention Detector**
```
Lock Contention Rate = Failed Acquires / Total Attempts

Low Contention (<5%):   Fine-grained locks, good design âœ…
Medium (5-20%):         Acceptable, monitor trends
High (>20%):            Coarse locks, redesign needed âŒ

REMEDY: Finer lock granularity
  - âŒ Lock entire "orders" table
  - âœ… Lock specific order "orders:12345"
```

---

## ğŸ¨ PART 6: Visual Lock Capacity Model

```
                ğŸ”’ DISTRIBUTED LOCK SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“Š LOAD          ğŸ’¾ STORAGE        âš–ï¸ CONSENSUS
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  QPS    Peak      Active  History   Nodes  Quorum
  7.3K   22K       20MB    3.8TB     5      3
```

**Memory Trigger**: Think **"L.S.C."** = Load, Storage, Consensus

---

## ğŸ—ï¸ PART 7: Lock System Domain Model

```python
# Think in terms of lock lifecycle states

class Lock:
    resource_name: str      # "payment:user:123"
    owner_id: str           # "payment-service-pod-42"
    fencing_token: int      # Monotonically increasing (prevent stale ops)
    acquired_at: int        # Unix timestamp (ms)
    expires_at: int         # acquired_at + ttl
    ttl: int                # 30,000 ms (30 seconds)
    renewal_count: int      # Track heartbeats
    state: LockState        # ACQUIRED, EXPIRED, RELEASED

# Scale Insight: Every lock generates:
# - 1 acquire (write)
# - N renewals (writes) where N = (hold_duration / heartbeat_interval)
# - 1 release (write)
# Total writes per lock = 2 + N

# For 5-minute lock with 15s heartbeat:
# Writes = 2 + (300 / 15) = 22 writes per lock!
# This drives our 7.3K QPS calculation.
```

---

## ğŸ¯ PART 8: The Interview Cheat Sheet for Locks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DISTRIBUTED LOCK ESTIMATION - 5 MIN RITUAL      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Clarify lock usage pattern:
    - How many clients? (instances, workers)
    - Lock frequency? (per instance, per second)
    - Lock duration? (seconds, minutes)
    - Renewal frequency? (TTL / 2 typical)

[ ] 2. Calculate lock operations:
    - Acquire QPS = clients Ã— frequency
    - Renew QPS = acquire Ã— (duration / heartbeat)
    - Total writes = acquire + renew + release

[ ] 3. Size consensus cluster:
    - Leader capacity: ~10K QPS (etcd), ~5K (ZK)
    - Cluster size: 5 nodes (tolerates 2 failures)
    - Sharding: If QPS > leader capacity

[ ] 4. Storage & bandwidth:
    - Active locks = acquire_qps Ã— hold_duration
    - Storage = active Ã— 200B + history
    - Bandwidth = write_qps Ã— 200B Ã— replicas

[ ] 5. Latency check:
    - Same DC: <10ms P99 âœ…
    - Cross-region: >50ms âŒ (not suitable)
```

---

## ğŸš€ Key Metrics Summary Table

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Acquire QPS** | 333 (avg), 1K (peak) | Determines new lock rate |
| **Renew QPS** | 6,660 (avg), 20K (peak) | Dominant write load for long locks |
| **Total Write QPS** | 7,326 (avg), 22K (peak) | Cluster sizing |
| **Active Locks** | 100,000 | Memory/storage for lock table |
| **Lock Entry Size** | 200 bytes | Minimal (key-value pair) |
| **Cluster Size** | 5 nodes | Tolerates 2 failures |
| **P99 Latency** | <10ms | Same-datacenter consensus |
| **Storage (30d)** | 3.8 TB | History for auditing |

---

## ğŸ’¡ Pro Architect Tips

### **Tip 1: The Lock Lifecycle Smell Test**
After calculations, ask:
- "Do 100K concurrent locks seem reasonable?" â†’ YES (if 2K instances)
- "Is 7K QPS within etcd's 10K limit?" â†’ YES âœ…
- "Can we handle 22K peak?" â†’ Maybe (sharding or read offloading)

### **Tip 2: The Heartbeat Efficiency Check**
```
If renewals dominate (>80% of writes):
  âŒ Problem: Heartbeat overhead too high
  âœ… Solutions:
     - Increase TTL (longer locks, fewer renewals)
     - Batch renewals (renew multiple locks in one call)
     - Use lease-based systems (etcd leases)
```

### **Tip 3: The Consistency vs Availability Trade-off**
Locks are inherently **CP (Consistency + Partition Tolerance)**:
```
During network partition:
  - Minority partition: Cannot acquire locks (unavailable)
  - Majority partition: Continues normal operation
  - Why: Safety > Availability (prevent split-brain)

Use Case Suitability:
  âœ… Financial transactions (need strong consistency)
  âœ… Leader election (only one leader allowed)
  âœ… Distributed cron (no duplicate job execution)
  âŒ Best-effort rate limiting (use AP system like Redis)
```

---

## ğŸ“ Professor's Wisdom: Lock System Edition

> **"In distributed locking, CORRECTNESS beats PERFORMANCE"**

Interviewer priorities:
1. âœ… Safety guarantees (fencing tokens, quorum)
2. âœ… Failure handling (what if leader crashes?)
3. âœ… Latency awareness (same DC vs cross-region)
4. âœ… Scalability bottlenecks (leader write limit)

**NOT NEEDED:**
- âŒ Exact QPS numbers
- âŒ Specific database choices
- âŒ Code implementation details

---

## ğŸ” Repetition Backed by Emotion (Lock Systems!)

**REPEAT 3 TIMES OUT LOUD:**
1. *"Active locks = Acquire rate Ã— Hold duration - simple as that!"*
2. *"Renewals dominate writes - 10x to 20x acquire rate!"*
3. *"Consensus needs quorum - Majority or bust for safety!"*

**VISUALIZE:** You're at the whiteboard, the interviewer nods as you say:
"So with 333 lock acquisitions per second and a 5-minute hold time,
we have about 100,000 concurrent locks at any moment..."

---

## ğŸ“š Quick Reference: Lock System Scale Benchmarks

| **System Type** | **Instances** | **Acquire QPS** | **Total Write QPS** | **Cluster Size** |
|-----------------|---------------|-----------------|---------------------|------------------|
| Small Startup | 100 | 10 | 200 | 3 nodes |
| Medium SaaS | 2,000 | 333 | 7K | 5 nodes |
| Large Enterprise | 10,000 | 1,600 | 35K | 5 nodes Ã— 4 shards |
| Hyperscale | 100,000 | 16K | 350K | 5 nodes Ã— 40 shards |

---

## ğŸ”§ Practical Application: Capacity Planning Examples

### Example 1: **E-commerce Inventory Lock**
```
Scenario: Lock inventory items during checkout

Given:
- 100K items in catalog
- 1M daily orders
- Avg checkout time: 2 minutes
- Peak hour: 3x average

Calculate:
1. Order QPS: 1M / 86400 = 11.5 QPS avg, 35 QPS peak
2. Acquire QPS: 35 (one lock per order)
3. Hold duration: 120s
4. Heartbeat: 60s (TTL 120s â†’ renew at 60s)
5. Renew QPS: 35 Ã— (120 / 60) = 70 QPS
6. Total Write QPS: 35 (acq) + 70 (renew) + 35 (rel) = 140 QPS

Conclusion: Single 5-node etcd cluster (10K capacity) âœ…
Active locks: 35 Ã— 120 = 4,200 concurrent
```

### Example 2: **Distributed Cron Scheduler**
```
Scenario: 1000 cron jobs, prevent duplicate execution

Given:
- 1000 jobs running every minute
- Each job acquires leader lock
- Lock held for entire job duration (avg 30s)

Calculate:
1. Acquire QPS: 1000 / 60 = 16.6 QPS
2. Hold duration: 30s
3. Heartbeat: 15s
4. Renew QPS: 16.6 Ã— (30 / 15) = 33.2 QPS
5. Total: 16.6 + 33.2 + 16.6 = 66.4 QPS

Conclusion: Trivial load, single cluster handles easily âœ…
```

### Example 3: **Payment Processing Locks**
```
Scenario: Lock user accounts during payment

Given:
- 10M daily payments
- Avg payment processing: 5 seconds
- Peak: 5x average (Black Friday)

Calculate:
1. Payment QPS: 10M / 86400 = 115 avg, 575 peak
2. Hold duration: 5s (short!)
3. Heartbeat: 2.5s
4. Renew QPS: 575 Ã— (5 / 2.5) = 1,150 QPS
5. Total: 575 + 1150 + 575 = 2,300 QPS

Conclusion: Well within single cluster capacity âœ…
Short locks = low renewal overhead
```

---

## ğŸš¨ Common Capacity Planning Mistakes

### Mistake 1: **Forgetting Renewal Overhead**
```
âœ— BAD:  "We acquire 100 locks/sec, so 100 QPS writes"
âœ“ GOOD: "100 acq/s + renewals (10x) + 100 rel/s = ~2,100 QPS"
```

### Mistake 2: **Ignoring Lock Contention**
```
âœ— BAD:  "Lock the entire database during migrations"
âœ“ GOOD: "Lock specific tables/rows, minimize contention"

High contention = Failed acquisitions = Retry storms = Cluster overload
```

### Mistake 3: **Cross-Region Consensus**
```
âœ— BAD:  "Deploy 5-node Raft cluster across 5 continents"
âœ“ GOOD: "Single region for consensus, replicate state cross-region for reads"

Cross-region consensus latency: 100-500ms (unacceptable for locks!)
```

### Mistake 4: **Undersizing TTL**
```
âœ— BAD:  "Use 5-second TTL for all locks"
âœ“ GOOD: "TTL = 2Ã— max expected hold time (safety margin)"

If task takes 10s, use 20-30s TTL. Avoids premature expiry on slow ops.
```

### Mistake 5: **Single Leader Bottleneck**
```
âœ— BAD:  "All 100K QPS writes go through one leader"
âœ“ GOOD: "Shard locks by resource hash across multiple Raft clusters"

Each leader: ~10K QPS capacity
Need 100K? â†’ 10 shards Ã— 5 nodes = 50-node deployment
```

---

## ğŸ“ Your Practice Template (Fill-in-the-Blank)

```
LOCK SYSTEM: ___________________

STEP 1: WORKLOAD CHARACTERIZATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active Clients:           [____]
Locks per Client/Min:     [____]
Avg Lock Hold Duration:   [____] seconds
Heartbeat Interval:       [____] seconds (TTL/2)

STEP 2: QPS CALCULATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Acquire QPS  = [____] clients Ã— [____] locks/min Ã· 60 = [____]
â†’ Renewals per Lock = [____] duration Ã· [____] heartbeat = [____]
â†’ Renew QPS    = [____] acquire Ã— [____] renewals = [____]
â†’ Release QPS  = [____] (same as acquire)
â†’ Total Write QPS = [____] + [____] + [____] = [____]
â†’ Peak QPS (3x) = [____]

STEP 3: CLUSTER SIZING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Leader Capacity (etcd): 10,000 QPS

â†’ Clusters Needed = ceil([____] peak QPS / 10K) = [____]
â†’ Nodes per Cluster = 5 (tolerates 2 failures)
â†’ Total Nodes = [____] clusters Ã— 5 = [____]

STEP 4: STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Lock Entry Size = 200 bytes
â†’ Active Locks = [____] QPS Ã— [____] seconds = [____]
â†’ Active Storage = [____] locks Ã— 200B = [____] MB
â†’ History/Day = [____] QPS Ã— 86400 Ã— 200B = [____] GB
â†’ Total ([____] days) = [____] GB

SMELL TEST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ QPS within leader capacity? (<10K per cluster)
â–¡ Latency achievable? (<10ms same DC)
â–¡ Failure tolerance acceptable? (2 nodes for 5-node cluster)
â–¡ Storage reasonable? (GBs to TBs range)
```

---

## ğŸ Bonus: Lock System Capacity Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      DISTRIBUTED LOCK CAPACITY CHEAT SHEET             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Lock Entry:    ~200 bytes (resource + owner + meta)
â€¢ etcd Capacity: ~10K write QPS per leader
â€¢ ZK Capacity:   ~5K write QPS per leader
â€¢ P99 Latency:   <10ms (same DC), >50ms (cross-region)
â€¢ Cluster Size:  5 nodes (tolerates 2 failures)

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Acquire QPS = Clients Ã— Locks/min Ã· 60
Renew QPS = Acquire Ã— (Hold Duration / Heartbeat)
Active Locks = Acquire QPS Ã— Hold Duration (seconds)
Total Writes = Acquire + Renew + Release
Storage = Active Ã— 200B + (Daily QPS Ã— 86400 Ã— 200B Ã— Days)

TYPICAL RATIOS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Short Locks (<30s):  Renew ~2Ã— Acquire
â€¢ Medium Locks (5min): Renew ~20Ã— Acquire
â€¢ Long Locks (1hr):    Renew ~120Ã— Acquire
â€¢ Heartbeat:           TTL / 2 (typical)
â€¢ Peak:Average:        3:1 (business hours)

CLUSTER SIZING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:   <1K QPS    â†’ 3 nodes
Medium:  1K-10K QPS â†’ 5 nodes
Large:   >10K QPS   â†’ 5 nodes Ã— N shards

LATENCY BUDGET (Same DC):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Network RTT:         1-2ms
Leader Log Append:   1ms
Replication:         2-3ms (parallel to followers)
Quorum ACK:          2-3ms
Response:            1-2ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total P99:           7-12ms âœ…

FAILURE TOLERANCE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
3-node cluster: Tolerates 1 failure  (quorum = 2)
5-node cluster: Tolerates 2 failures (quorum = 3)
7-node cluster: Tolerates 3 failures (quorum = 4)

WHY NOT MORE? Consensus latency increases with cluster size!

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Can 5-node etcd handle 5K QPS? YES
âœ“ Can same-DC cluster do <10ms? YES
âœ“ Can lock survive 2 node failures? YES (5-node)
âœ“ Are renewals <90% of writes? YES (if hold <5min)

âœ— Can cross-region cluster do <10ms? NO
âœ— Can single leader handle 50K QPS? NO (shard it)
âœ— Can 3-node cluster tolerate 2 failures? NO (use 5)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final Challenge: Apply This Template

Pick one of these systems and estimate lock capacity:

1. **Kubernetes Scheduler** - Leader election for scheduler instances
2. **Distributed Cron** - 10,000 jobs, prevent duplicate execution
3. **Order Processing** - Lock inventory items during checkout
4. **Microservice Coordination** - Rate limiting per user account
5. **Database Migration** - Ensure single migrator across replicas

Use the blank template above and time yourself: **Can you complete it in 7 minutes?**

---

## ğŸ“š Additional Resources

- **Papers**: "In Search of an Understandable Consensus Algorithm" (Raft, Diego Ongaro)
- **Books**: "Designing Data-Intensive Applications" Chapter 9 (Consistency & Consensus)
- **Tools**: etcd, Consul, ZooKeeper benchmarking guides
- **Practice**: Design leader election, distributed cron, lock service

---

**Remember**:
> "Lock systems trade availability for correctness. In the face of uncertainty, safety comes first."

**Now go design rock-solid distributed coordination!** ğŸš€

---

*Created with the LOCKS technique: Latency â†’ Operations â†’ Cluster â†’ Key metrics â†’ Storage*
*Perfect for: System design interviews, Production capacity planning, Architecture reviews*
