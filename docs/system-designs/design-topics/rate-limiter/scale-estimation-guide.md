# ğŸ¯ Rate Limiter System Design: Scale Estimation Masterclass

## The RATE-CALC Technique for Rate Limiter Math
**(R)equests â†’ (A)lgorithm â†’ (T)hroughput â†’ (E)stimation â†’ (C)apacity â†’ (A)rchitecture â†’ (L)atency â†’ (C)osts**

This is a **mental framework** specifically for rate limiting systems at scale.

---

## ğŸ“Š PART 1: Understanding Rate Limiter Scale

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total API Clients | 1M | Enterprise SaaS platform |
| | Active Clients (DAU) | 200K | ~20% active daily |
| | Premium Clients | 20K | 10% paying customers |
| | Free Tier Clients | 180K | 90% free tier |
| **Request Patterns** | Avg Requests per Client | 1,000/day | API-heavy workload |
| | Read:Write Ratio | 80:20 | Typical API usage |
| | Peak Traffic Multiplier | 5x | Business hours spike |
| **Rate Limits** | Free Tier Limit | 100 req/min | Basic protection |
| | Premium Tier Limit | 1,000 req/min | Paid features |
| | Burst Allowance | 20% | Token bucket capacity |
| **System Scale** | Total Requests/Day | 200M | 200K Ã— 1,000 |
| | Rate Limit Checks/Sec | 10K-50K | Every request checked |

---

## ğŸ§® PART 2: Rate Limiter-Specific Mental Math

### Rule #1: **The Rate Limit Check Rule**
```
Every single API request = 1 rate limit check

If you have 10K API requests/sec â†’ 10K rate limit checks/sec
```

### Rule #2: **The Storage Key Pattern**
```
Storage keys = Unique identifiers Ã— Endpoints Ã— Algorithms

Example:
- 1M users
- 10 endpoints
- 2 algorithms (per-endpoint + global)
= 1M Ã— 10 Ã— 2 = 20M active keys in Redis
```

### Rule #3: **The Memory Calculation**
```
For Token Bucket:
  Per key: 32 bytes (tokens: 8B, last_refill: 8B, capacity: 8B, metadata: 8B)

For Sliding Window Counter:
  Per key: 16 bytes Ã— 2 windows = 32 bytes

For Sliding Window Log:
  Per key: 100 timestamps Ã— 8 bytes = 800 bytes (much higher!)

Rule of thumb: ~50 bytes per key (with overhead)
```

---

## ğŸ“ˆ PART 3: Rate Limiter Scale Math Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ RATE LIMITER NAPKIN MATH - Universal Template       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: REQUEST VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active API Clients:        [____] K
Requests per Client/Day:   [____]
Peak Hour Multiplier:      [____] x

â†’ Total Requests/Day    = [____] Ã— [____]     = [____] M
â†’ Average QPS           = Requests/Day Ã· 100K = [____]
â†’ Peak QPS              = Average Ã— [____]     = [____]
â†’ Rate Limit Checks/Sec = Peak QPS             = [____]

STEP 2: STORAGE ESTIMATION (Redis)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unique Identifiers:    [____] (users/API keys/IPs)
Endpoints Tracked:     [____]
Algorithms per Key:    [____] (global + per-endpoint)

â†’ Total Keys = IDs Ã— Endpoints Ã— Algorithms = [____] M
â†’ Bytes per Key = [____] bytes (algorithm dependent)
â†’ Total Memory = Keys Ã— Bytes = [____] GB
â†’ Add 50% overhead (Redis) = [____] GB

STEP 3: THROUGHPUT & LATENCY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Redis Ops/Sec = Rate Limit Checks = [____] K
â†’ Target Latency = <5ms (P99)
â†’ Network RTT = 1-2ms
â†’ Redis Query Time = <1ms

STEP 4: COORDINATION OVERHEAD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Config Updates/Day = [____]
â†’ ZooKeeper Writes/Sec = [____]
â†’ Cross-Region Sync Latency = [____] ms

STEP 5: ANALYTICS STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Throttled Requests/Day = [____] (estimate 5% reject rate)
â†’ Event Size = 200 bytes (JSON event)
â†’ Daily Storage = Throttled Ã— Event Size = [____] GB
â†’ Retention = 30 days
â†’ Total Analytics Storage = [____] TB
```

---

## ğŸ’¾ PART 4: Rate Limiter Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    RATE LIMITER SYSTEM - NAPKIN MATH SOLUTION           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: REQUEST VOLUME ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Active API Clients:        200 K
Requests per Client/Day:   1,000
Peak Hour Multiplier:      5 x

â†’ Total Requests/Day    = 200K Ã— 1,000     = 200 M
â†’ Average QPS           = 200M Ã· 100K      = 2,000 QPS
â†’ Peak QPS              = 2K Ã— 5           = 10,000 QPS
â†’ Rate Limit Checks/Sec = 10,000

STEP 2: STORAGE ESTIMATION (Redis)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unique Identifiers:    200K (API keys)
Endpoints Tracked:     20
Algorithms per Key:    3 (global + per-endpoint + burst)

â†’ Total Keys = 200K Ã— 20 Ã— 3 = 12 M keys
â†’ Bytes per Key = 50 bytes (Token Bucket)
â†’ Total Memory = 12M Ã— 50B = 600 MB
â†’ Add 100% overhead (Redis internal) = 1.2 GB

For Hot Keys (20% accessed frequently):
â†’ Hot Keys = 12M Ã— 0.2 = 2.4M
â†’ Cache Memory = 2.4M Ã— 50B = 120 MB

STEP 3: THROUGHPUT & LATENCY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Redis Ops/Sec = 10K (rate limit checks)
â†’ Target Latency = <5ms (P99)
  - Network RTT = 1ms
  - Redis Lua script execution = 0.5ms
  - Gateway processing = 2ms
  - Buffer = 1.5ms
â†’ Total = ~5ms âœ“

STEP 4: COORDINATION OVERHEAD
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Config Updates/Day = 100 (tenant limit changes)
â†’ ZooKeeper Writes/Sec = 0.001 (negligible)
â†’ Cross-Region Sync Latency = 50ms (acceptable)

STEP 5: ANALYTICS STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Total Requests/Day = 200M
â†’ Reject Rate = 5% (10M rejected per day)
â†’ Event Size = 200 bytes (JSON: tenant, endpoint, timestamp, limit)
â†’ Daily Storage = 10M Ã— 200B = 2 GB/day
â†’ Retention = 30 days
â†’ Total Analytics Storage = 2GB Ã— 30 = 60 GB
```

---

## ğŸ§  PART 5: Rate Limiter-Specific Mental Math Tricks

### **Technique 1: The "100K QPS Rule"**
*(Industry Standard)*
```
Single Redis instance: ~100K simple ops/sec
Our use case: 10K QPS â†’ Need only 1 Redis master + 2 replicas âœ“

If 1M QPS â†’ Need Redis cluster with 10 master nodes
```

### **Technique 2: The "5-Tier Limit Hierarchy"**
*(Enterprise Pattern)*
```
1. Global Limit:        1M req/day (all users combined)
2. Tenant Limit:        100K req/day (per enterprise customer)
3. API Key Limit:       10K req/day (per API key)
4. Endpoint Limit:      1K req/min (per endpoint, per key)
5. Burst Limit:         50 req/sec (spike protection)

Storage = Tiers Ã— Keys per Tier
```

### **Technique 3: The "Rejection Rate Formula"**
```
EMOTION TRIGGER: "Good rate limiters reject 3-10% of requests"

If rejection rate > 10% â†’ Limits too strict
If rejection rate < 1% â†’ Limits too loose (not protecting backend)

Target: 5% rejection rate for capacity planning
```

### **Technique 4: The "Redis vs Database Decision"**
```
Rate Limiter Data:
- High read/write frequency (every request!)
- Short-lived data (reset every minute/hour)
- Atomic operations critical

â†’ MUST use Redis (or Memcached), NOT traditional DB

Database is ONLY for:
- Limit configuration (low frequency reads)
- Historical analytics (write-once, read-many)
```

---

## ğŸ¨ PART 6: Visual Mind Map for Rate Limiter Scale

```
                 ğŸŒ RATE LIMITER SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“Š TRAFFIC        ğŸ’¾ STORAGE        ğŸ”§ COORDINATION
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   10K    5%        12M     1.2GB    ZooKeeper  Kafka
   QPS  Reject     Keys     Redis      Config   Events
```

**Memory Trigger**: Think **"T.S.C."** = Traffic, Storage, Coordination

---

## ğŸ—ï¸ PART 7: Algorithm-Specific Storage Calculations

```java
// Token Bucket - Most memory efficient
class TokenBucket {
    double tokens;        // 8 bytes
    long last_refill;     // 8 bytes
    double capacity;      // 8 bytes
    double refill_rate;   // 8 bytes
    // Total: 32 bytes per key
}

// Sliding Window Counter - Memory efficient
class SlidingWindowCounter {
    long current_window_count;   // 8 bytes
    long prev_window_count;      // 8 bytes
    long current_window_start;   // 8 bytes
    // Total: 24 bytes per key
}

// Sliding Window Log - Memory intensive
class SlidingWindowLog {
    List<Long> timestamps;  // 100 requests Ã— 8 bytes = 800 bytes
    // Total: 800+ bytes per key (not scalable!)
}

// Leaky Bucket
class LeakyBucket {
    Queue queue;          // Variable (avg 50 requests Ã— 8 bytes)
    long last_leak;       // 8 bytes
    // Total: ~400 bytes per key
}

RECOMMENDATION: Use Token Bucket or Sliding Window Counter for scale
```

---

## ğŸ¯ PART 8: The Interview Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RATE LIMITER SCALE ESTIMATION - 5 MIN RITUAL    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Ask: Number of API clients? Requests per client?
[ ] 2. Calculate: Total QPS = (Clients Ã— Req/day) Ã· 100K
[ ] 3. Estimate Keys: Clients Ã— Endpoints Ã— Tier levels
[ ] 4. Calculate Memory: Keys Ã— 50 bytes (Token Bucket)
[ ] 5. Check: Can Redis handle QPS? (100K ops/sec per instance)
[ ] 6. Consider: Multi-region? (Adds coordination overhead)
```

---

## ğŸš€ Key Metrics Summary Table

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Rate Limit Checks** | 10K QPS | Determines Redis throughput needed |
| **Peak QPS** | 50K QPS | Size Redis cluster (50K Ã· 100K = 1 master) |
| **Redis Memory** | 1.2 GB | Determines instance type (r6g.large) |
| **Active Keys** | 12M | Eviction policy (LRU if exceeds memory) |
| **Decision Latency** | <5ms | P99 latency budget |
| **Reject Rate** | 5% | Normal (10M rejected per day) |
| **Analytics Storage** | 60 GB | TimescaleDB sizing (30 days retention) |

---

## ğŸ’¡ Pro Architect Tips for Rate Limiters

### **Tip 1: The Redis Cluster Decision**
After calculations, ask:
- "Can single Redis handle 10K QPS?" â†’ YES (100K max)
- "Is 1.2GB memory reasonable?" â†’ YES (fits in r6g.large)
- "Do we need sharding?" â†’ NO (unless >100K QPS)

### **Tip 2: The Algorithm Decision Matrix**

| **Algorithm** | **Memory** | **Accuracy** | **Burst** | **Use Case** |
|---------------|------------|--------------|-----------|--------------|
| Token Bucket | Low (32B) | Good | YES âœ“ | **Recommended** for most cases |
| Sliding Window Counter | Low (24B) | Good | Partial | High QPS with smooth limits |
| Sliding Window Log | High (800B) | Perfect | YES | Low QPS, exact limits |
| Leaky Bucket | Medium (400B) | Good | NO | Traffic shaping |

**Interview Tip:** Default to Token Bucket unless interviewer asks for exact limits

### **Tip 3: The Distributed Coordination Cost**

```
Single Region:
- Rate limit check: 1ms (local Redis)
- Total latency: 5ms

Multi-Region (Global Quotas):
- Cross-region sync: 50-100ms (ZooKeeper/etcd)
- Trade-off: Partition quotas (10% over-quota acceptable)
```

---

## ğŸ“ Professor's Rate Limiter Wisdom

> **"Rate limiting is about FAIRNESS, not just PROTECTION"**

Your interviewer wants to see:
1. âœ… Understanding of distributed challenges
2. âœ… Algorithm trade-offs (memory vs accuracy)
3. âœ… Coordination vs latency decisions
4. âœ… Redis as default choice (not DB!)

**NOT NEEDED:**
- âŒ Exact Redis commands (high-level is fine)
- âŒ Lua script syntax (pseudocode works)
- âŒ ZooKeeper internals (know it's for coordination)

---

## ğŸ” Repetition Backed by Emotion

**REPEAT 3 TIMES OUT LOUD:**
1. *"Every request = 1 rate limit check - Redis is NON-NEGOTIABLE!"*
2. *"Token Bucket is the default - only change with good reason!"*
3. *"5ms latency budget - 1ms Redis, 2ms gateway, 2ms buffer!"*

**VISUALIZE:** You're at the whiteboard confidently saying: "At 10K QPS, single Redis handles all checks under 1ms..."

---

## ğŸ“š Quick Reference: Rate Limiter Benchmarks

| **Company** | **API QPS** | **Algorithm** | **Storage** | **Special Notes** |
|-------------|-------------|---------------|-------------|-------------------|
| Stripe | 10K | Token Bucket | Redis | Per-API-key limits |
| GitHub | 5K | Sliding Window | Redis | 5,000 req/hour |
| AWS API Gateway | 100K | Token Bucket | DynamoDB | 10K default, burst 5K |
| Twitter | 50K | Sliding Counter | Manhattan DB | 15 req/15min windows |
| Shopify | 20K | Leaky Bucket | Redis | 2 req/sec (smooth) |

---

## ğŸ”§ Practical Application: Different Scales

### For **Small SaaS** (1K QPS):
```
STEP 1: TRAFFIC
- API Clients: 10K
- Requests: 100/client/day
- QPS: 10K Ã— 100 Ã· 100K = 10 QPS

STEP 2: STORAGE
- Keys: 10K Ã— 5 endpoints Ã— 2 = 100K
- Memory: 100K Ã— 50B = 5 MB
- Redis: Single r6g.medium ($50/month)

STEP 3: DECISION
- Single Redis instance âœ“
- Token Bucket algorithm âœ“
- No coordination needed âœ“
```

### For **Enterprise Platform** (100K QPS):
```
STEP 1: TRAFFIC
- API Clients: 1M
- Requests: 10K/client/day
- QPS: 1M Ã— 10K Ã· 100K = 100K QPS

STEP 2: STORAGE
- Keys: 1M Ã— 20 endpoints Ã— 3 = 60M
- Memory: 60M Ã— 50B = 3 GB
- Redis: Cluster with 2 masters (sharding)

STEP 3: COORDINATION
- Multi-region: YES (3 regions)
- ZooKeeper for config sync
- Hierarchical quotas (region-level)

STEP 4: DECISION
- Redis Cluster (2 shards) âœ“
- Token Bucket with local cache âœ“
- Allow 5-10% over-quota during failover âœ“
```

---

## ğŸ¯ Mental Math Practice Problems

### Problem 1: Gaming API with Spikes
```
Given:
- 10M players (DAU)
- Average 50 API calls/player/day
- Game events cause 100x spikes (5 min duration)
- Limit: 10 calls/min per player

Calculate:
1. Average QPS
2. Peak QPS
3. Redis memory needed
4. Burst capacity (Token Bucket)

[Try it yourself, then check answer]
```

<details>
<summary>Answer</summary>

```
1. AVERAGE QPS:
   - Calls/day = 10M Ã— 50 = 500M
   - Average QPS = 500M Ã· 100K = 5,000 QPS

2. PEAK QPS:
   - Spike multiplier = 100x
   - Peak QPS = 5K Ã— 100 = 500,000 QPS
   - (This is VERY high - need aggressive rate limiting!)

3. REDIS MEMORY:
   - Keys = 10M players Ã— 1 endpoint Ã— 1 limit = 10M
   - Memory = 10M Ã— 50B = 500 MB
   - With overhead = 1 GB

4. BURST CAPACITY:
   - Limit: 10 calls/min = refill rate
   - Burst: Allow 20 calls (2Ã— limit)
   - Capacity: 20 tokens
```

**Architecture Decision:**
- Need Redis Cluster (500K QPS >> 100K single instance)
- Use 5-6 master nodes (500K Ã· 100K)
- Token Bucket with capacity=20 (handles spikes)
</details>

---

### Problem 2: Multi-Tenant B2B Platform
```
Given:
- 1,000 enterprise customers
- Each customer: 100 API keys
- Each key: 1,000 req/min limit
- Track per-endpoint (50 endpoints)
- 30-day analytics retention

Calculate:
1. Total unique keys in Redis
2. Memory needed
3. Analytics storage (assume 10% rejected)
4. Cost estimate (AWS)
```

<details>
<summary>Answer</summary>

```
1. TOTAL KEYS:
   - API Keys = 1,000 customers Ã— 100 = 100K
   - Per-endpoint tracking = 100K Ã— 50 = 5M keys
   - Global limits = 100K
   - Total = 5M + 100K â‰ˆ 5.1M keys

2. REDIS MEMORY:
   - Memory = 5.1M Ã— 50B = 255 MB
   - With overhead (2x) = 500 MB
   - Instance: r6g.large (16 GB) - plenty of headroom

3. ANALYTICS STORAGE:
   - Requests/day = 100K keys Ã— 1K req/min Ã— 60min Ã— 24hr
                  = 100K Ã— 1.44M = 144 Billion/day
   - Rejected (10%) = 14.4B events/day
   - Event size = 200 bytes
   - Daily storage = 14.4B Ã— 200B = 2.88 TB/day
   - 30-day retention = 2.88TB Ã— 30 = 86 TB
   - Compressed (TimescaleDB 10:1) = 8.6 TB

4. COST ESTIMATE (AWS):
   - Redis r6g.large: $150/month
   - TimescaleDB (RDS): $2,000/month (8.6 TB)
   - Network egress: $500/month
   - Total: ~$2,650/month
```
</details>

---

## ğŸš¨ Common Mistakes to Avoid

### Mistake 1: **Using Database Instead of Redis**
```
âœ— BAD:  "Store rate limit counters in PostgreSQL"
âœ“ GOOD: "Use Redis for counters (high throughput, atomic ops)"

Why? PostgreSQL max ~10K writes/sec vs Redis 100K ops/sec
```

### Mistake 2: **Ignoring Algorithm Memory Overhead**
```
âœ— BAD:  "Sliding Window Log for 10M users" (8 GB memory!)
âœ“ GOOD: "Token Bucket for 10M users" (500 MB memory)

10x memory difference!
```

### Mistake 3: **Forgetting Distributed Coordination**
```
âœ— BAD:  "Each region enforces global quota independently"
âœ“ GOOD: "Partition quota per region, sync hourly via ZooKeeper"

Avoids cross-region latency on every request
```

### Mistake 4: **Over-Engineering for Small Scale**
```
âœ— BAD:  "100 QPS, but let's use Redis Cluster + ZooKeeper + Kafka"
âœ“ GOOD: "100 QPS, single Redis instance + simple config file"

Start simple, scale when needed
```

---

## ğŸ“ Your Practice Template (Rate Limiter Specific)

```
SYSTEM: Rate Limiter for ___________________

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API Clients (DAU):         [____] K
Requests per Client/Day:   [____]
Peak Multiplier:           [____] x

â†’ Total Requests/Day = [____] M
â†’ Average QPS        = [____]
â†’ Peak QPS           = [____]

STEP 2: REDIS STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unique Identifiers:   [____] (API keys/users)
Endpoints Tracked:    [____]
Limit Tiers:          [____]

â†’ Total Keys   = [____] M
â†’ Algorithm    = Token Bucket / Sliding Window
â†’ Bytes/Key    = [____] bytes
â†’ Total Memory = [____] GB

STEP 3: ALGORITHM CHOICE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Token Bucket (default - allows bursts)
â–¡ Sliding Window Counter (smooth, memory efficient)
â–¡ Sliding Window Log (exact, high memory)
â–¡ Leaky Bucket (traffic shaping)

STEP 4: COORDINATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Single Region (no coordination)
â–¡ Multi-Region (ZooKeeper/etcd needed)
â–¡ Hierarchical Quotas (yes/no)

STEP 5: ANALYTICS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Reject Rate:          [____] %
Events/Day:           [____] M
Event Size:           [____] bytes
Retention:            [____] days
â†’ Total Storage = [____] TB

SMELL TEST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ QPS < 100K? (single Redis) OR QPS > 100K? (cluster)
â–¡ Memory < 10GB? (single instance) OR > 10GB? (sharding)
â–¡ Latency < 5ms? (achievable with Redis)
â–¡ Reject rate 3-10%? (healthy rate limiting)
```

---

## ğŸ Rate Limiter Scale Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      RATE LIMITER SCALE DESIGN CHEAT SHEET             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Redis: 100K simple ops/sec (single instance)
â€¢ Token Bucket: 32 bytes/key
â€¢ Sliding Window Counter: 24 bytes/key
â€¢ Sliding Window Log: 800 bytes/key (avoid at scale)
â€¢ Target Latency: <5ms (P99)

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Rate Limit Checks/Sec = API QPS
Redis Keys = Clients Ã— Endpoints Ã— Tiers
Memory = Keys Ã— Bytes_per_Algorithm
Redis Instances = QPS Ã· 100K (round up)

ALGORITHM DECISION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Default: Token Bucket (burst support, memory efficient)
Alternative: Sliding Window Counter (smooth rate)
Avoid: Sliding Window Log (high memory)

SCALING RULES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
< 10K QPS:     Single Redis, no coordination
10K-100K QPS:  Single Redis cluster, optional replicas
> 100K QPS:    Multi-node Redis, ZooKeeper coordination
> 1M QPS:      Regional Redis, hierarchical quotas

TYPICAL REJECT RATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
< 1%:  Limits too loose (not protecting backend)
3-10%: Healthy (normal rate limiting)
> 20%: Limits too strict (bad UX)

COST ESTIMATES (AWS):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small (1K QPS):     $50/month (r6g.medium)
Medium (50K QPS):   $300/month (r6g.xlarge + replicas)
Large (500K QPS):   $2K/month (cluster + coordination)

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Redis for counters? (NOT database)
âœ“ Latency <5ms? (achievable with local Redis)
âœ“ Memory <16GB? (single instance) OR sharding needed
âœ“ Reject rate 3-10%? (healthy protection)
âœ“ Algorithm = Token Bucket? (unless special need)

âœ— Using Postgres for rate limits? NO (too slow)
âœ— Sliding Window Log for >1M keys? NO (too much memory)
âœ— Global quota with cross-region sync? NO (use hierarchy)
âœ— No burst allowance? NO (causes bad UX)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final Challenge

Pick a real-world system and estimate rate limiting needs:

1. **Netflix API** - Video streaming controls
2. **GitHub API** - Git operations and webhooks
3. **Stripe API** - Payment processing
4. **Discord API** - Real-time messaging
5. **AWS API Gateway** - Multi-tenant serverless

Time yourself: **Can you complete the estimation in 5 minutes?**

---

**Remember**:
> "Rate limiters protect backends, but also ensure fairness. The goal is balance, not rejection."

**Now go design rate limiters at scale!** ğŸš€

---

*Created with the RATE-CALC technique: Requests â†’ Algorithm â†’ Throughput â†’ Estimation â†’ Capacity â†’ Architecture â†’ Latency â†’ Costs*
*Perfect for: Distributed systems interviews, API platform design, Enterprise SaaS architectures*
