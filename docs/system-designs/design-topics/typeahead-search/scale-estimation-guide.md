# ğŸ” Typeahead + Search System: Scale Estimation Masterclass

## The SEARCH Technique for Scale Math
**(S)cope â†’ (E)stimate Users â†’ (A)nalyze Patterns â†’ (R)ank Priorities â†’ (C)alculate Resources â†’ (H)euristics Check**

This framework helps you systematically approach search system capacity planning.

---

## ğŸ“Š PART 1: Users & Traffic Estimation

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 500M | Google-scale search |
| | Daily Active Users (DAU) | 100M | ~20% engagement |
| | Searches per DAU | 10 | Industry average |
| | Typeahead requests per search | 10 | User types ~10 chars |
| **Traffic Distribution** | Peak:Average Ratio | 3x | Lunch/evening spikes |
| | Read:Write Ratio | 1000:1 | Searches >> Indexing |
| **Query Characteristics** | Avg query length | 3-5 words | Typical search |
| | Unique queries per day | 500M | 50% unique |
| **Content Scale** | Total documents | 10B | Web-scale corpus |
| | New docs per day | 100M | Fresh content |
| | Avg document size | 5KB | Typical web page |

---

## ğŸ§® PART 2: The "Search Engineer's Calculator"

### Rule #1: **The Query Traffic Ladder**

```
Remember these conversions:
â€¢ 1 Search = 10 Typeahead requests
â€¢ 100M DAU Ã— 10 searches = 1B searches/day
â€¢ 1B searches Ã— 10 typeahead = 10B typeahead/day
â€¢ 1 Day â‰ˆ 100K seconds
```

### Rule #2: **The Index Size Formula**

```
Total Index Size = Docs Ã— Avg Size Ã— Expansion Factor

Expansion Factor:
- Inverted Index: 1.5x (tokenization overhead)
- With replicas (3x): 4.5x total
- With sharding metadata: 5x final

Example:
10B docs Ã— 5KB Ã— 5 = 250TB total storage
```

### Rule #3: **The Latency Budget**

```
Typeahead Latency Budget (100ms):
- Network: 20ms
- Load Balancer: 5ms
- Application Logic: 10ms
- Cache Lookup (Redis): 5ms
- Trie Traversal: 10ms
- Serialization: 10ms
- Buffer: 40ms

Search Latency Budget (500ms):
- Network: 50ms
- Application: 30ms
- Elasticsearch Query: 200ms
- Ranking Service: 100ms
- Aggregations: 70ms
- Buffer: 50ms
```

---

## ğŸ“ˆ PART 3: Capacity Planning Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ TYPEAHEAD + SEARCH NAPKIN MATH TEMPLATE             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Daily Active Users:         100 M
Searches per user:          10
Typeahead per search:       10

â†’ Searches/Day   = 100M Ã— 10       = 1B
â†’ Typeahead/Day  = 1B Ã— 10         = 10B

â†’ Search QPS     = 1B Ã· 100K       = 10,000 QPS
â†’ Typeahead QPS  = 10B Ã· 100K      = 100,000 QPS
â†’ Peak QPS       = 100K Ã— 3        = 300,000 QPS

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Documents:                  10 B
Avg document size:          5 KB
Index expansion:            5x (inverted index + replicas)

â†’ Raw Data       = 10B Ã— 5KB       = 50 TB
â†’ Index Size     = 50TB Ã— 5        = 250 TB
â†’ Query Logs     = 1B Ã— 100B Ã— 365 = 36.5 TB/year

Total Storage (1 year):              ~300 TB

STEP 3: CACHE SIZING (Typeahead)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Unique prefixes (1-3 chars):   62^3 = 238K
Top queries per prefix:        10
Avg query size:                50 bytes

â†’ Trie Size      = 238K Ã— 10 Ã— 50  = ~120 MB (fits in memory!)

For all prefixes (up to 5 chars):
â†’ Trie Size      = 62^5 Ã— 10 Ã— 50  = ~45 GB

Redis Cache:                        100 GB (includes metadata)

STEP 4: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Typeahead:
  Request:  100K QPS Ã— 50B         = 5 MB/s
  Response: 100K QPS Ã— 500B        = 50 MB/s

Search:
  Request:  10K QPS Ã— 200B         = 2 MB/s
  Response: 10K QPS Ã— 5KB          = 50 MB/s

â†’ Total Bandwidth                   = ~110 MB/s = 0.9 Gbps

STEP 5: COMPUTE RESOURCES
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Typeahead Servers:
  Each server: 10K QPS capacity
  Required: 300K Ã· 10K             = 30 servers (peak)
  With buffer (30%):                 40 servers

Elasticsearch Nodes:
  Each node: 2K QPS search capacity
  Required: 30K Ã· 2K               = 15 nodes
  With replicas (3x):                45 nodes

Total Servers:                       ~100 (all services)
```

---

## ğŸ’¾ PART 4: Detailed Breakdown by Component

### **A. Typeahead Service**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         TYPEAHEAD CAPACITY PLANNING         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Traffic: 100,000 QPS average, 300,000 peak

MEMORY REQUIREMENTS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Trie Structure:
  - Short prefixes (1-3 chars): 120 MB
  - Medium prefixes (4-5 chars): 45 GB
  - Long tail (6+ chars): Elasticsearch fallback

Redis Cache:
  - Hot queries (1M): 50 MB
  - Warm queries (10M): 500 MB
  - Total with overhead: 100 GB

Per-Server Memory: 16 GB RAM
  - Trie: 10 GB
  - Application: 2 GB
  - OS: 2 GB
  - Buffer: 2 GB

COMPUTE REQUIREMENTS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Single Server Capacity:
  - CPU: 8 cores
  - Throughput: 10,000 QPS
  - Latency: P95 < 50ms

Servers Needed (Peak):
  300K QPS Ã· 10K = 30 servers
  With HA (N+2): 32 servers
  With deployments (20% extra): 40 servers

NETWORK:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Per Server:
  Inbound: 300K Ã— 50B Ã· 40 = 375 KB/s
  Outbound: 300K Ã— 500B Ã· 40 = 3.75 MB/s
  Total: ~4 MB/s per server (~32 Mbps)
```

### **B. Search Service (Elasticsearch)**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ELASTICSEARCH CLUSTER PLANNING         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Traffic: 10,000 QPS average, 30,000 peak

INDEX SIZE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Documents:        10 Billion
Avg size:         5 KB
Raw data:         50 TB

Inverted Index:   50TB Ã— 1.5 = 75 TB
With metadata:    75TB Ã— 1.2 = 90 TB

Shard Size (Recommended): 50 GB per shard
Number of Shards: 90TB Ã· 50GB = 1,800 shards

Replicas: 2 (total 3 copies)
Total Storage: 90TB Ã— 3 = 270 TB

NODE SIZING:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Data Node Specs:
  - RAM: 64 GB (32 GB heap, 32 GB OS cache)
  - Disk: 6 TB SSD
  - CPU: 16 cores

Shards per Node: 6TB Ã· 50GB = ~100 shards

Total Data Nodes:
  Storage-based: 270TB Ã· 6TB = 45 nodes
  Performance-based: 1800 shards Ã· 100 = 18 nodes

  Choose max: 45 data nodes

Master Nodes: 3 (dedicated)
Coordinating Nodes: 5 (query routing)

Total Cluster: 53 nodes

MEMORY BREAKDOWN (per data node):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
JVM Heap:         32 GB (ES recommendation: â‰¤32GB)
  - Segment Memory: 20 GB
  - Field Cache: 5 GB
  - Filter Cache: 5 GB
  - Buffer: 2 GB

OS Page Cache:    32 GB (for Lucene segments)

QPS CAPACITY:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Per Data Node: 500 QPS
Cluster Total: 45 Ã— 500 = 22,500 QPS
Peak Capacity: 22.5K QPS < 30K (need more nodes OR caching!)

Solution: Add Redis cache layer (80% hit rate)
  Actual ES load: 30K Ã— 0.2 = 6K QPS âœ“
```

### **C. Redis Cache Layer**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          REDIS CLUSTER SIZING               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TYPEAHEAD CACHE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Hot Queries: 1M queries
Avg size: 500 bytes (query + top 10 suggestions)
Total: 1M Ã— 500B = 500 MB

SEARCH CACHE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Hot Searches: 10M unique queries/day
Cache 20%: 2M queries
Avg response: 5KB (top 20 results + metadata)
Total: 2M Ã— 5KB = 10 GB

TOTAL CACHE SIZE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Data: 10.5 GB
Overhead (30%): 3 GB
Total: ~15 GB

Redis Cluster:
  - Nodes: 3 masters + 3 replicas = 6 nodes
  - Memory per node: 32 GB
  - Total capacity: 96 GB (plenty of headroom)

THROUGHPUT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Single Redis Node: 100K ops/sec
Cluster (3 masters): 300K ops/sec
Required: 100K typeahead + 10K search = 110K ops/sec âœ“
```

### **D. Query Logging & Analytics**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        ANALYTICS STORAGE (ClickHouse)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Query Volume: 1B queries/day

LOG ENTRY SIZE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{
  query_id: 16 bytes (UUID)
  query_text: 50 bytes
  user_id: 8 bytes
  timestamp: 8 bytes
  latency_ms: 4 bytes
  results_count: 2 bytes
  clicked_doc: 20 bytes
  metadata: 30 bytes (location, device, etc.)
}
Total: ~140 bytes per query

STORAGE (1 year):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Raw: 1B Ã— 140B Ã— 365 = 51 TB/year

ClickHouse Compression (10x): 5.1 TB/year

With replicas (2x): 10 TB/year

INGEST RATE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1B logs/day Ã· 100K sec = 10,000 rows/sec
Peak (3x): 30,000 rows/sec

ClickHouse Capacity: 100K+ rows/sec per node âœ“

Cluster Size:
  - 3 nodes (sharded)
  - 4 TB storage per node
  - 32 GB RAM per node
```

---

## ğŸ§  PART 5: Mental Math Shortcuts for Search Systems

### **Shortcut 1: The "Character Set Rule"**
```
Prefix Space Calculation:

Alphanumeric: 62 characters (a-z, A-Z, 0-9)

1-char prefixes: 62^1 = 62
2-char prefixes: 62^2 = 3,844
3-char prefixes: 62^3 = 238,328
4-char prefixes: 62^4 = 14.7M
5-char prefixes: 62^5 = 916M

RULE OF THUMB:
- Store 1-3 char prefixes in memory (Trie): 238K entries
- Store 4-5 char prefixes in Redis: 15M entries
- Store 6+ char prefixes in Elasticsearch: Long tail
```

### **Shortcut 2: The "80-20 Cache Rule"**
```
For Search Systems:
- 20% of queries = 80% of traffic
- Cache the hot 20%

Example:
500M unique queries/day
Cache: 500M Ã— 0.2 = 100M queries
Avg response: 5KB
Cache size: 100M Ã— 5KB = 500 GB

BUT with TTL (1 hour):
Active queries in 1 hour: 1B Ã· 24 = 42M queries
Unique (50%): 21M queries
Cache size: 21M Ã— 5KB = 105 GB âœ“ (much better!)
```

### **Shortcut 3: The "Shard Size Rule"**
```
Elasticsearch Best Practice:
- Shard size: 30-50 GB (optimal)
- Shards per node: 20-100
- Heap memory: 50% of RAM (max 32GB)

Quick Calculation:
Index size: 90 TB
Shard size: 50 GB
Shards needed: 90,000 GB Ã· 50 GB = 1,800 shards

Nodes needed: 1,800 Ã· 40 shards/node = 45 nodes
```

### **Shortcut 4: The "QPS to Servers" Formula**
```
Generic Formula:
Servers = (Peak QPS Ã— Safety Factor) Ã· Server Capacity

Typeahead Example:
Peak: 300K QPS
Server capacity: 10K QPS
Safety factor: 1.3 (30% buffer)
Servers = (300K Ã— 1.3) Ã· 10K = 39 â†’ round to 40 servers

Search Example:
Peak: 30K QPS (after 80% cache hit)
  ES sees: 30K Ã— 0.2 = 6K QPS
ES node capacity: 500 QPS
Nodes = (6K Ã— 1.3) Ã· 500 = 16 â†’ round to 20 data nodes
```

---

## ğŸ¨ PART 6: Visual Capacity Map

```
                    ğŸ” TYPEAHEAD + SEARCH SYSTEM
                               |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                      |                      |
    ğŸ’» TYPEAHEAD           ğŸ” SEARCH             ğŸ“Š ANALYTICS
        |                      |                      |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”              â”Œâ”€â”€â”€â”´â”€â”€â”€â”             â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   QPS   MEM             QPS   Storage        Volume  Storage
  100K   45GB           10K    270TB          1B/day   10TB

  40     100GB          45      -              3       4TB
 Servers Redis        Nodes                  Nodes    /node
```

---

## ğŸ—ï¸ PART 7: Scaling Strategies

### **Horizontal Scaling**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          SCALING DECISION MATRIX                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

When to Scale Typeahead:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Trigger: P95 latency > 100ms OR CPU > 70%
Action: Add servers (horizontal scaling)
Cost: $500/month per server (c5.2xlarge)

When to Scale Elasticsearch:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Trigger: Disk > 85% OR Heap > 75% OR QPS degradation
Action:
  - Storage issue: Add data nodes
  - CPU issue: Add coordinating nodes
  - Memory issue: Increase heap (up to 32GB)
Cost: $2,000/month per data node (r5.4xlarge + 6TB SSD)

When to Scale Redis:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Trigger: Memory > 80% OR Hit rate < 70%
Action:
  - Memory issue: Add shards (resharding)
  - Hit rate: Increase TTL or cache size
Cost: $300/month per node (r5.xlarge)
```

### **Geographic Distribution**

```
Multi-Region Deployment:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Region 1 (US-East): 40% traffic
  - Typeahead: 16 servers
  - ES: 18 nodes
  - Redis: 3 masters

Region 2 (EU-West): 35% traffic
  - Typeahead: 14 servers
  - ES: 16 nodes
  - Redis: 3 masters

Region 3 (Asia): 25% traffic
  - Typeahead: 10 servers
  - ES: 11 nodes
  - Redis: 3 masters

Total Cost: 3x regional deployment + cross-region replication
```

---

## ğŸ¯ PART 8: Cost Analysis

### **Monthly Infrastructure Cost Breakdown**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AWS COST ESTIMATION (Monthly)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPUTE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Typeahead Servers (40 Ã— c5.2xlarge):
  $0.34/hr Ã— 40 Ã— 730hrs = $9,928/mo

Elasticsearch Data Nodes (45 Ã— r5.4xlarge):
  $1.008/hr Ã— 45 Ã— 730hrs = $33,082/mo

Elasticsearch Master/Coord (8 Ã— r5.xlarge):
  $0.252/hr Ã— 8 Ã— 730hrs = $1,472/mo

Redis Cluster (6 Ã— r5.xlarge):
  $0.252/hr Ã— 6 Ã— 730hrs = $1,104/mo

ClickHouse (3 Ã— r5.2xlarge):
  $0.504/hr Ã— 3 Ã— 730hrs = $1,104/mo

STORAGE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
EBS SSD (270 TB for ES):
  270TB Ã— $0.10/GB = $27,000/mo

S3 (Backups, 50TB):
  50TB Ã— $0.023/GB = $1,150/mo

NETWORK:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Data Transfer Out (10TB/month):
  10TB Ã— $0.09/GB = $900/mo

Load Balancer:
  ALB: $16.20/mo + $0.008/LCU
  ~$500/mo

TOTAL:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Compute:        $46,690/mo
Storage:        $28,150/mo
Network:        $1,400/mo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL:          ~$76,240/mo  (~$915K/year)

Cost per Query:
  $76,240 Ã· (1B searches Ã— 30) = $0.0025 per search
  $76,240 Ã· (10B typeahead Ã— 30) = $0.00025 per typeahead
```

---

## ğŸ’¡ PART 9: Optimization Opportunities

### **Cost Optimization Strategies**

```
1. RESERVED INSTANCES:
   Save 40-60% on compute
   Estimated savings: $20K/mo

2. SPOT INSTANCES (for batch indexing):
   Save 70% on indexing workers
   Estimated savings: $3K/mo

3. S3 INTELLIGENT TIERING:
   Auto-move cold data to Glacier
   Estimated savings: $500/mo

4. COMPRESSION:
   ClickHouse: 10x compression
   ES: Enable best_compression
   Estimated savings: Covered in calculations

5. CACHE WARMING:
   Preload hot queries at startup
   Reduces cold-start ES load
   Cost: $0 (operational improvement)

Total Potential Savings: ~$24K/mo (30% reduction)
Optimized Cost: ~$52K/mo (~$624K/year)
```

### **Performance Optimization ROI**

```
Investment: Add Redis Cache Layer ($1,104/mo)
Impact: Reduce ES load by 80%
  Before: 45 ES nodes
  After: 15 ES nodes (keep 30 for storage)

Savings: 30 nodes Ã— $735/mo = $22K/mo
ROI: $22K - $1.1K = $20.9K/mo saved!
```

---

## ğŸ“š PART 10: Quick Reference Cheat Sheet

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      TYPEAHEAD + SEARCH SCALE CHEAT SHEET             â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TRAFFIC RULES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 1 Search = 10 Typeahead requests
â€¢ Peak = 3Ã— Average (for search systems)
â€¢ Cache hit rate target: 80-90%

CAPACITY RULES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Typeahead:
  â€¢ Server capacity: 10K QPS per server
  â€¢ Memory: 45 GB for full Trie
  â€¢ Latency: P95 < 100ms

Elasticsearch:
  â€¢ Shard size: 30-50 GB
  â€¢ Shards per node: 20-100
  â€¢ Node capacity: 500 QPS
  â€¢ Heap: 32 GB max

Redis:
  â€¢ Node capacity: 100K ops/sec
  â€¢ Memory: 32 GB typical
  â€¢ Hit rate: Target 85%

STORAGE FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Index Size = Docs Ã— Size Ã— 1.5 (inverted index)
             Ã— 3 (replicas)
             = Docs Ã— Size Ã— 4.5

Trie Size = Prefixes Ã— Top_K Ã— Avg_Query_Size
          = 62^5 Ã— 10 Ã— 50 bytes
          = ~45 GB

LATENCY BUDGETS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Typeahead: 100ms total
  - Network: 20ms
  - Processing: 60ms
  - Buffer: 20ms

Search: 500ms total
  - Network: 50ms
  - Processing: 350ms
  - Buffer: 100ms

COST ESTIMATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:  1M QPS,    $10K/mo
Medium: 10M QPS,   $50K/mo
Large:  100M QPS,  $500K/mo

SCALING TRIGGERS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Scale Up:
  â€¢ Latency: P95 > target
  â€¢ CPU: > 70%
  â€¢ Memory: > 80%
  â€¢ Disk: > 85%

Scale Down:
  â€¢ All metrics < 50% for 7 days
  â€¢ Cost optimization opportunity
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ Professor's Final Wisdom

> **"In search systems, LATENCY is the king, RELEVANCE is the queen, and SCALE is the kingdom."**

### Key Principles:

1. **Two-Tier Optimization**:
   - Typeahead: Optimize for latency (<100ms)
   - Search: Optimize for relevance + latency (<500ms)

2. **Cache Aggressively**:
   - 80% cache hit = 5x cost reduction
   - Multi-tier: Trie â†’ Redis â†’ Elasticsearch

3. **Shard Smartly**:
   - ES shards: 30-50 GB sweet spot
   - Too many shards = overhead
   - Too few shards = hot spots

4. **Monitor Everything**:
   - Latency percentiles (P50, P95, P99)
   - Cache hit rates
   - Index lag time
   - Query relevance metrics

5. **Iterate on Ranking**:
   - Start with BM25
   - Add ML models incrementally
   - A/B test everything

---

## ğŸ”„ Practice Exercise

**Challenge**: Design capacity for a **Product Search** system

```
Given:
- 50M products in catalog
- 20M DAU
- 5 searches per user per day
- Average 8 typeahead requests per search

Calculate:
1. Typeahead QPS (peak)
2. Search QPS (peak)
3. Index storage size
4. Number of ES nodes needed
5. Monthly AWS cost

[Try it yourself using the templates above!]
```

<details>
<summary>Answer</summary>

```
1. TYPEAHEAD QPS:
   20M Ã— 5 Ã— 8 = 800M typeahead/day
   QPS = 800M Ã· 100K = 8,000 QPS
   Peak = 8K Ã— 3 = 24,000 QPS

2. SEARCH QPS:
   20M Ã— 5 = 100M searches/day
   QPS = 100M Ã· 100K = 1,000 QPS
   Peak = 1K Ã— 3 = 3,000 QPS

3. INDEX STORAGE:
   50M products Ã— 10KB (product data) = 500 GB
   With index: 500GB Ã— 1.5 = 750 GB
   With replicas: 750GB Ã— 3 = 2.25 TB

4. ES NODES:
   Shards: 2,250 GB Ã· 50 GB = 45 shards
   Nodes: 45 Ã· 20 shards/node = 3 data nodes
   With replicas distributed: 5 nodes total

5. COST:
   Typeahead: 4 servers Ã— $250 = $1K
   ES: 5 nodes Ã— $2K = $10K
   Redis: 3 nodes Ã— $300 = $900
   Storage: 2.25TB Ã— $100/TB = $225
   Total: ~$12K/month
```
</details>

---

**Remember**:
> "Scale estimation is an art + science. Be logical, state assumptions, and demonstrate systematic thinking!"

---

*Created with the SEARCH technique: Scope â†’ Estimate â†’ Analyze â†’ Rank â†’ Calculate â†’ Heuristics*
*Perfect for: FAANG interviews, System Design rounds, Capacity Planning*
