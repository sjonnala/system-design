# ğŸ¯ Messaging System Scale Estimation Masterclass

## The POWER Technique for Messaging Scale Math
**(P)rinciples â†’ (O)rder of magnitude â†’ (W)rite it down â†’ (E)stimate â†’ (R)ound ruthlessly**

This framework applies to ANY messaging system: WhatsApp, Messenger, Slack, Discord, etc.

---

## ğŸ“Š PART 1: Users & Scale Estimation

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 2 Billion | WhatsApp-scale |
| | Daily Active Users (DAU) | 500M | ~25% of total (messaging is sticky!) |
| | Peak Concurrent Users | 100M | ~20% of DAU online simultaneously |
| | Average Contacts per User | 50 | Typical contact list size |
| **Message Distribution** | Messages sent per user/day | 50 | Mix of heavy/light users |
| | Group messages ratio | 30% | 30% group, 70% 1-on-1 |
| | Average group size | 10 members | Smaller than max (256) |
| **Time Distribution** | Peak Hours | 6 hours/day | Morning & evening commutes |
| | Peak Traffic Multiplier | 3x | Triple the average load |
| **Media Content** | Text-only messages | 60% | Most messages are text |
| | Image messages | 30% | Photos dominate media |
| | Video/Audio messages | 10% | Voice notes, short videos |
| **Connection** | WebSocket connections | 100M concurrent | For real-time delivery |
| | Average message lifetime | 90 days | After that, archived/deleted |

---

## ğŸ§® PART 2: The "Messaging Calculator" - Your Mental Math Toolkit

### Rule #1: **The Message Volume Ladder**
```
Remember these anchors for messaging:
â€¢ 1 user Ã— 50 msgs/day = baseline
â€¢ 500M DAU Ã— 50 = 25 Billion messages/day
â€¢ 25B Ã· 100K seconds = 250K messages/sec average
â€¢ Peak: 250K Ã— 3 = 750K messages/sec
```

### Rule #2: **The WebSocket Math**
```
WebSocket connections are STATEFUL!
â€¢ 100M concurrent connections
â€¢ Each connection: ~10KB memory overhead
â€¢ Total RAM: 100M Ã— 10KB = 1TB RAM for connections
â€¢ Per server: 100K connections = 1GB RAM
â€¢ Need: 1,000 WebSocket servers minimum
```

### Rule #3: **The Storage Trick**
```
Messages are TIME-SERIES data:
â€¢ Average message: 1KB (text + metadata)
â€¢ Daily: 25B Ã— 1KB = 25TB/day
â€¢ 90 days retention: 25TB Ã— 90 = 2,250 TB â‰ˆ 2.3 PB
â€¢ With media (3x): ~7 PB total
```

---

## ğŸ“ˆ PART 3: Quick Scale Math Template (COPY THIS!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ MESSAGING NAPKIN MATH TEMPLATE - Universal Design  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Daily Active Users (DAU):   [____] M
Messages per user/day:      [____]
Total Messages/Day:         [____] B

â†’ Avg Message QPS = Messages/Day Ã· 100K = [____]
â†’ Peak Message QPS = Avg Ã— 3            = [____]
â†’ Group Message Fan-out = QPS Ã— Avg Group Size

STEP 2: CONNECTION ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Concurrent Users:           [____] M
WebSocket per user:         1

â†’ Total WS Connections = Concurrent Users  = [____] M
â†’ Memory per connection = 10KB
â†’ Total Connection RAM = Connections Ã— 10KB = [____] GB
â†’ Connections per server = 100K
â†’ Required WS Servers = Total / 100K        = [____]

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Message Size:
  - Text message avg:       500 bytes
  - Metadata (IDs, time):   300 bytes
  - Total per message:      ~1 KB

  - Image avg:              500 KB (compressed)
  - Video avg:              5 MB (short clips)
  - Voice note avg:         100 KB

Messages Created/Day:       [____] B
Retention Period:           [____] days

â†’ Daily Text Storage = Msgs Ã— 1KB         = [____] TB
â†’ Daily Media Storage = Msgs Ã— % Ã— AvgSize = [____] TB
â†’ Total Daily = Text + Media               = [____] TB
â†’ Retention Storage = Daily Ã— Days         = [____] PB

STEP 4: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Message Bandwidth = QPS Ã— Avg_Size = [____] GB/s
â†’ Media Bandwidth = QPS Ã— Media_% Ã— Media_Size = [____] GB/s

STEP 5: CACHE (HOT DATA)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Using recency principle (not Pareto):
â†’ Active conversations per user = 10
â†’ Messages per conversation = 100 (recent)
â†’ Cache per user = 10 Ã— 100 Ã— 1KB = 1MB
â†’ Total cache = 100M users Ã— 1MB = 100TB
```

---

## ğŸ’¾ PART 4: Messaging System Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      WHATSAPP-SCALE MESSAGING - NAPKIN MATH SOLUTION    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Daily Active Users (DAU):   500 M
Messages per user/day:      50
Total Messages/Day:         25 B (25 billion)

â†’ Avg Message QPS = 25B Ã· 100K  = 250,000 QPS
â†’ Peak Message QPS = 250K Ã— 3   = 750,000 QPS
â†’ Group Fan-out = 250K Ã— 30% Ã— 10 = 750K extra deliveries

STEP 2: CONNECTION ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Concurrent Users:           100 M
WebSocket per user:         1

â†’ Total WS Connections = 100M
â†’ Memory per connection = 10KB
â†’ Total Connection RAM = 100M Ã— 10KB = 1 TB
â†’ Connections per server = 100K
â†’ Required WS Servers = 100M / 100K = 1,000 servers

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Message Breakdown:
  - Text (60%): 15B messages Ã— 1KB        = 15 TB/day
  - Images (30%): 7.5B Ã— 500KB            = 3,750 TB/day
  - Video/Audio (10%): 2.5B Ã— 2MB         = 5,000 TB/day
  - Total Daily Storage:                  = ~8,800 TB â‰ˆ 9 TB/day

Retention Period:           90 days

â†’ Total Storage (90 days) = 9TB Ã— 90 = 810 TB â‰ˆ 1 PB

But with compression & deduplication:
â†’ Practical Storage = 1PB Ã— 0.7 = ~700 TB

STEP 4: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Text Bandwidth = 250K QPS Ã— 1KB     = 250 MB/s
â†’ Media Bandwidth = 250K Ã— 40% Ã— 1MB  = 100 GB/s

Total: ~100 GB/s (800 Gbps) - MASSIVE!
Solution: CDN for media delivery

STEP 5: CACHE (HOT DATA)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â†’ Active conversations per user = 10
â†’ Messages per conversation = 100
â†’ Cache per user = 10 Ã— 100 Ã— 1KB = 1MB
â†’ Total cache for 100M concurrent = 100M Ã— 1MB = 100 TB

Practical Redis deployment:
â†’ Distributed across 100 Redis nodes
â†’ Each node: 1TB RAM (cache + presence)
```

---

## ğŸ§  PART 5: Messaging-Specific Mental Math Techniques

### **Technique 1: The "Fan-Out Multiplier"**
*(For Group Messages)*
```
EMOTION TRIGGER: "Every group message multiplies work!"

1-on-1 message: 1 sender â†’ 1 recipient = 1 delivery
Group message: 1 sender â†’ 10 members = 10 deliveries

If 30% messages are groups (avg 10 members):
â†’ Effective delivery QPS = Base QPS Ã— (0.7 Ã— 1 + 0.3 Ã— 10)
â†’ Effective QPS = 250K Ã— (0.7 + 3) = 250K Ã— 3.7 = 925K deliveries/sec
```

### **Technique 2: The "Connection Cost"**
*(For WebSocket Servers)*
```
Key insight: Messaging is about CONNECTIONS, not just requests!

1 server handles 100K concurrent connections
Each connection: 10KB memory
â†’ 1 server needs: 100K Ã— 10KB = 1GB RAM just for connections

For 100M concurrent users:
â†’ Servers needed = 100M Ã· 100K = 1,000 servers
â†’ Just for maintaining connections!
```

### **Technique 3: The "Media Explosion"**
*(For Storage)*
```
Text is cheap, media is EXPENSIVE!

Text message: 1 KB
Image: 500 KB (500Ã— larger!)
Video: 5 MB (5,000Ã— larger!)

Even though only 40% messages have media:
â†’ Storage dominated by media (99% of total)
â†’ Bandwidth dominated by media (99.7% of total)

ALWAYS estimate media separately!
```

### **Technique 4: The "Presence Tax"**
*(For Real-Time Status)*
```
Every user has:
- Online/Offline status
- Last seen timestamp
- Typing indicator (ephemeral)

Status updates are FREQUENT:
â†’ User comes online: Notify all contacts (50 updates)
â†’ 100M users Ã— 1 status change/hour Ã— 50 contacts
â†’ = 5 Billion status updates/hour
â†’ = 1.4M status updates/sec

Solution: Aggregate & throttle status updates!
```

---

## ğŸ¯ PART 6: The Visual Mind Map Approach

```
                    ğŸ’¬ MESSAGING SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“Š SCALE          ğŸ’¾ STORAGE        ğŸ”§ COMPUTE
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   DAU    QPS       Size    Time    Servers  WS Conn
   500M   750K      1PB     90d      1K      100M
```

**Memory Trigger**: Think **"S.S.C."** = Scale, Storage, Compute

---

## ğŸ—ï¸ PART 7: Domain Model for Messaging

```java
// Think in terms of domain entities first!

@Entity
class Message {
    // THINK: What's the WRITE pattern?
    UUID messageId;       // TIMEUUID for ordering
    UUID chatId;          // Partition key for Cassandra
    UUID senderId;
    String content;       // Encrypted payload
    byte[] encryptedContent;
    MessageType type;     // TEXT, IMAGE, VIDEO, AUDIO
    Timestamp createdAt;

    // Scale Insight: This drives our 1PB storage calc!
    // Time-series nature â†’ Cassandra perfect fit
}

@Entity
class Conversation {
    UUID chatId;
    Set<UUID> participants;
    Timestamp lastMessageTime;
    int unreadCount;

    // Scale Insight: Denormalized for fast inbox queries
}

@Service
class MessageService {
    // THINK: What's the READ pattern?
    // Recent messages are HOT â†’ Cache heavily
    // 10 active chats Ã— 100 recent messages = 1MB cache per user
}
```

---

## ğŸ¯ PART 8: The Interview Cheat Sheet (Print This!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MESSAGING SCALE ESTIMATION - 5 MIN RITUAL       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Write down: DAU, Messages/user/day, Concurrent users
[ ] 2. Calculate QPS: Messages/day Ã· 100K Ã— 3 (peak)
[ ] 3. Calculate Connections: Concurrent Ã— 1, RAM = Ã— 10KB
[ ] 4. Calculate Storage: Messages Ã— 1KB (text) + Media
[ ] 5. Apply Fan-out: Group % Ã— Avg group size
[ ] 6. Double-check: Does 1,000 servers for 100M connections sound right? YES!
```

---

## ğŸš€ Key Metrics Summary Table

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Avg Message QPS** | 250K | Determines backend service capacity |
| **Peak Message QPS** | 750K | Size your infrastructure |
| **Concurrent Connections** | 100M | WebSocket server capacity (1K servers) |
| **Storage (90d)** | 1 PB | Database/archive sizing |
| **Cache Size** | 100 TB | Redis cluster sizing |
| **Media Bandwidth** | 100 GB/s | CDN bandwidth planning |
| **Connection RAM** | 1 TB | Memory for WebSocket servers |

---

## ğŸ’¡ Pro Architect Tips

### **Tip 1: The Smell Test**
After calculations, ask:
- "Can 1 server handle 100K WebSocket connections?" â†’ YES (with 1GB RAM)
- "Is 1PB storage reasonable for 90 days?" â†’ YES (with media)
- "Does 750K messages/sec sound right for WhatsApp?" â†’ YES

### **Tip 2: The Comparison Anchor**
Always compare to known systems:
- "WhatsApp: 100 Billion msgs/day (our calc: 25B) âœ“ Same order"
- "Slack: Millions of connections (our calc: 100M) âœ“ Scaled up"

### **Tip 3: Start with Constraints**
Always ask first:
1. How many users? (Active vs Total)
2. Messages per user per day?
3. What % are group messages?
4. Media vs text ratio?
5. How long to store messages?

---

## ğŸ“š Quick Reference: Common Messaging System Benchmarks

| **System Type** | **DAU** | **Messages/Day** | **Concurrent** | **Storage** |
|-----------------|---------|------------------|----------------|-------------|
| **WhatsApp** | 500M | 100B | 100M | 10+ PB |
| **Facebook Messenger** | 400M | 80B | 80M | 20+ PB |
| **Slack** | 20M | 5B | 5M | 500 TB |
| **Discord** | 150M | 15B | 20M | 5 PB |
| **Telegram** | 300M | 50B | 50M | 15 PB |

---

## ğŸ”§ Practical Application: Adapting This Template

### For a **1-on-1 Messaging App** (like Signal):
```
STEP 1: TRAFFIC
- No group messages (0% fan-out)
- Higher encryption overhead
- Ratio: 1:1 (no amplification)

STEP 2: STORAGE
- Ephemeral messages (auto-delete after 7 days)
- Lower retention = 10Ã— less storage

STEP 3: CONNECTIONS
- Same: 1 connection per active user
- Focus: Security over scale

STEP 4: CACHE
- Recent messages only (50 per chat)
- Smaller cache footprint
```

### For a **Team Collaboration Tool** (like Slack):
```
STEP 1: TRAFFIC
- Channel messages dominate (70% group)
- Larger average group size (50-100 members)
- Massive fan-out multiplier!

STEP 2: STORAGE
- Infinite retention (search history critical)
- Much higher storage needs
- Compression & archival strategy required

STEP 3: CONNECTIONS
- Desktop apps stay connected 24/7
- Higher connection persistence

STEP 4: CACHE
- Channel messages (not just 1-on-1)
- Unread message tracking per channel
```

### For a **Gaming Chat** (like Discord):
```
STEP 1: TRAFFIC
- Voice/Video > Text
- Server-based (not 1-on-1)
- Real-time voice: Different architecture!

STEP 2: STORAGE
- Text: Similar to messaging
- Voice: Ephemeral (not stored)
- Video: High quality, short retention

STEP 3: BANDWIDTH
- Voice: 64 kbps Ã— concurrent users
- Video: 2 Mbps Ã— concurrent streams
- Dominated by media streaming

STEP 4: CDN
- Critical for media delivery
- Regional edge servers
```

---

## ğŸ¯ Mental Math Practice Problems

### Problem 1: Telegram-Scale Messaging
```
Given:
- 300M DAU
- 60 messages/user/day
- 40% group messages, avg 15 members
- 50% text, 40% images (400KB), 10% video (3MB)
- 60 days retention

Calculate:
1. Peak message QPS
2. WebSocket servers needed (80M concurrent)
3. Storage after 60 days
4. Bandwidth requirements

[Try it yourself, then check answers below]
```

<details>
<summary>Answer</summary>

```
1. PEAK MESSAGE QPS:
   - Messages/day = 300M Ã— 60 = 18B messages
   - Avg QPS = 18B Ã· 100K = 180K QPS
   - Peak QPS = 180K Ã— 3 = 540K QPS

   - Delivery QPS (with fan-out):
     - 1-on-1: 60% Ã— 540K = 324K
     - Group: 40% Ã— 540K Ã— 15 = 3.24M
     - Total deliveries: 3.56M/sec

2. WEBSOCKET SERVERS:
   - Concurrent connections = 80M
   - Per server capacity = 100K
   - Servers needed = 80M Ã· 100K = 800 servers

3. STORAGE (60 days):
   - Text (50%): 9B Ã— 1KB = 9 TB/day
   - Images (40%): 7.2B Ã— 400KB = 2,880 TB/day
   - Video (10%): 1.8B Ã— 3MB = 5,400 TB/day
   - Daily total = 8,289 TB â‰ˆ 8.3 TB/day
   - 60 days = 8.3 TB Ã— 60 = 498 TB â‰ˆ 500 TB

4. BANDWIDTH:
   - Peak message delivery = 3.56M/sec
   - Text: 324K Ã— 1KB = 324 MB/s
   - Images: 216K Ã— 400KB = 86 GB/s
   - Video: 54K Ã— 3MB = 162 GB/s
   - Total: ~250 GB/s (needs CDN!)
```
</details>

---

### Problem 2: Enterprise Messaging (Slack-like)
```
Given:
- 20M DAU (enterprise users)
- 100 messages/user/day (work hours: 8 hours)
- 70% channel messages, avg 50 members
- 90% text, 10% files (2MB avg)
- Infinite retention
- 10M concurrent connections

Calculate:
1. Peak QPS during work hours
2. Fan-out delivery rate
3. Storage after 1 year
4. Redis cache size

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. PEAK QPS (WORK HOURS):
   - Messages/day = 20M Ã— 100 = 2B messages
   - Work hours: 8 hours = 28,800 seconds
   - QPS during work = 2B Ã· 28,800 = 69,444 QPS
   - Peak: 69K Ã— 2 = 138K QPS

2. FAN-OUT DELIVERY:
   - 1-on-1 (30%): 41K deliveries
   - Channel (70%): 97K Ã— 50 = 4.85M deliveries
   - Total: 4.89M deliveries/sec (HUGE!)

3. STORAGE (1 YEAR):
   - Text (90%): 1.8B Ã— 1KB = 1.8 TB/day
   - Files (10%): 200M Ã— 2MB = 400 TB/day
   - Daily total = 401.8 TB/day
   - 1 year = 402TB Ã— 365 = 146,730 TB â‰ˆ 147 PB

   (This shows why Slack is expensive at scale!)

4. CACHE SIZE:
   - Active channels per user = 20
   - Messages per channel = 100
   - Cache per user = 20 Ã— 100 Ã— 1KB = 2MB
   - Total cache = 10M Ã— 2MB = 20 TB
```
</details>

---

## ğŸš¨ Common Mistakes to Avoid

### Mistake 1: **Forgetting Fan-Out**
```
âœ— BAD:  "1M QPS message send = 1M deliveries"
âœ“ GOOD: "1M QPS Ã— 30% groups Ã— 10 members = 3M additional deliveries"
```

### Mistake 2: **Treating Messaging Like HTTP Requests**
```
âœ— BAD:  "Stateless servers, just add more"
âœ“ GOOD: "WebSocket servers are stateful, need connection distribution strategy"
```

### Mistake 3: **Underestimating Media**
```
âœ— BAD:  "1KB per message average"
âœ“ GOOD: "1KB for text, but 40% are media (500KB avg) â†’ Dominated by media"
```

### Mistake 4: **Not Considering Presence**
```
âœ— BAD:  "Just store and forward messages"
âœ“ GOOD: "Track online/offline, last seen, typing â†’ Extra infrastructure"
```

### Mistake 5: **Ignoring Message Ordering**
```
âœ— BAD:  "Use auto-increment IDs"
âœ“ GOOD: "Use TIMEUUID for distributed ordering, Cassandra for time-series"
```

---

## ğŸ Bonus: Messaging-Specific Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        MESSAGING SYSTEM SCALE CHEAT SHEET              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

MEMORY ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ 1 Message = 1 KB (text + metadata)
â€¢ 1 WebSocket = 10 KB RAM overhead
â€¢ 1 Server = 100K connections max
â€¢ 1 Group Message = Avg 10 deliveries (fan-out)
â€¢ Media dominates: 99% of storage

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Message QPS = (DAU Ã— Msgs/user/day) Ã· 100K Ã— Peak_Multiplier
Delivery QPS = Message QPS Ã— (1 + Group% Ã— Avg_Group_Size)
WS Servers = Concurrent_Users Ã· 100K
Storage = Messages/day Ã— Size Ã— Retention_Days
Connection RAM = Concurrent Ã— 10KB

TYPICAL RATIOS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Messages/user/day = 50 (consumer) to 100 (enterprise)
â€¢ Group% = 30% (consumer) to 70% (enterprise)
â€¢ Text:Media = 60:40 (consumer) to 90:10 (enterprise)
â€¢ Peak:Avg = 3x (time-of-day pattern)
â€¢ Concurrent:DAU = 20% (always-on apps)

QUICK ESTIMATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:  1M DAU,    50K QPS,   1M conn,    10 TB storage
Medium: 100M DAU,  500K QPS,  20M conn,   100 TB storage
Large:  500M DAU,  2.5M QPS,  100M conn,  1 PB storage
Huge:   2B DAU,    10M QPS,   500M conn,  10 PB storage

CRITICAL COMPONENTS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
WebSocket Servers: Stateful, sticky connections
Message DB: Cassandra (time-series), sharded by chat_id
Cache: Redis (recent messages, 100 per chat)
Media Storage: S3 + CDN (99% of bandwidth)
Presence: Redis (online/offline, TTL-based)
Push Notifications: FCM/APNS (offline users)

INTERVIEW FLOW:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Clarify requirements (5 min)
   â†’ DAU? Messages/user? Groups? Media?

2. High-level estimates (5 min)
   â†’ QPS, Connections, Storage, Bandwidth
   â†’ "Let me do some quick napkin math..."

3. System design (20 min)
   â†’ Use estimates to drive design decisions
   â†’ "With 100M connections, we need WebSocket cluster..."

4. Deep dives (10 min)
   â†’ Based on scale, what's critical?
   â†’ "At this scale, message ordering is critical..."

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Can 1 server handle 100K WebSockets? YES (1GB RAM)
âœ“ Can Cassandra handle 1M writes/sec? YES (distributed)
âœ“ Can Redis cache 100TB? YES (cluster with 100 nodes)
âœ“ Is 1PB storage for 90 days reasonable? YES (with media)

âœ— Can SQL handle 1M QPS writes? NO (need NoSQL)
âœ— Can single Redis handle 100M presence updates? NO (need cluster)
âœ— Can single server handle 1M connections? NO (max 100K)
âœ— Can ignore fan-out in groups? NO (multiplies deliveries 10Ã—)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final Challenge: Apply This Template

Pick one of these systems and practice the full estimation:

1. **iMessage** - Apple's messaging platform
2. **WeChat** - Multi-purpose messaging + payments
3. **Microsoft Teams** - Enterprise collaboration
4. **Snapchat** - Ephemeral messaging
5. **Twitch Chat** - Live streaming chat

Use the blank template above and time yourself: **Can you complete it in 5 minutes?**

---

**Remember**:
> "In messaging systems, CONNECTIONS matter more than requests. Think stateful, not stateless!"

**Now go crush those interviews!** ğŸš€

---

*Created with the POWER technique: Principles â†’ Order â†’ Write â†’ Estimate â†’ Round*
*Perfect for: FAANG interviews, Messaging System Design, Real-Time Architecture discussions*
