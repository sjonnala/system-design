# ğŸ¯ Google Maps Scale Estimation Masterclass

## The MAPS-SCALE Framework
**(M)ap â†’ (A)ssess â†’ (P)lan â†’ (S)hard â†’ (S)tore â†’ (C)ache â†’ (A)ggregate â†’ (L)ocalize â†’ (E)valuate**

This mental framework is specifically designed for **mapping platforms at global scale**.

---

## ğŸ“Š PART 1: Scale Assumptions for Google Maps

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 1B | Google Maps global reach |
| | Daily Active Users (DAU) | 200M | ~20% engagement |
| | Peak Concurrent Users | 20M | ~10% of DAU |
| **Map Sessions** | Sessions per DAU | 3 | Average daily usage |
| | Session Duration | 5 min | Typical map viewing |
| | Tiles per Session | 50 | Pan/zoom interactions |
| **API Requests** | Tile Requests/Day | 30B | 200M Ã— 3 Ã— 50 tiles |
| | Directions/Day | 100M | ~50% users navigate |
| | Geocoding/Day | 500M | Search + autocomplete |
| | Places API/Day | 300M | Business search |
| **Traffic Data** | GPS Updates/sec | 1M | Android location sharing |
| | Active Road Segments | 10M | Monitored for traffic |
| | Traffic Update Freq | 30 sec | Real-time granularity |
| **Map Coverage** | Total Map Data | 1.2 PB | Tiles + imagery |
| | Street View Panos | 50M | 360Â° images |
| | Road Network Size | 100M nodes | Intersections globally |

---

## ğŸ§® PART 2: The Napkin Math for Maps Platform

### Rule #1: **Tile Pyramid Explodes Exponentially**

```
Zoom level z â†’ 2^z Ã— 2^z tiles total

Zoom 0:  1 tile        (entire world)
Zoom 10: 1,048,576 tiles    (2^10 Ã— 2^10)
Zoom 15: 1,073,741,824 tiles (2^15 Ã— 2^15)
Zoom 22: 17,592,186,044,416 tiles (2^22 Ã— 2^22)

Storage (if fully rendered):
  - 17.5 trillion tiles Ã— 15 KB = 262 PB (!!)

Reality: Pre-render smartly
  - Zoom 0-15 globally: ~1 PB
  - Zoom 16-22 on-demand for cities only
  - Total stored: 1.1 PB
```

### Rule #2: **CDN is Mandatory (95% Cache Hit)**

```
Daily tile requests: 30B
QPS average: 30B / 86,400 = 350K requests/sec

Without CDN:
  - Origin servers: 350K QPS Ã— 15 KB = 5.25 GB/sec
  - Daily bandwidth: 450 TB/day
  - Cost: $0.09/GB Ã— 450,000 GB = $40K/day = $1.2M/month

With CDN (95% cache hit):
  - Origin: 5% Ã— 450 TB = 23 TB/day
  - CDN serves: 95% Ã— 450 TB = 427 TB/day
  - Origin cost: $2K/day
  - CDN cost: $10K/day (cheaper per GB at edge)
  - Total: $12K/day = $360K/month
  - Savings: $840K/month (70%)
```

### Rule #3: **Routing Requires Graph Preprocessing**

```
Road network:
  - Nodes: 100M (intersections)
  - Edges: 200M (road segments)

Naive Dijkstra:
  - Time: O(E + V log V) = O(200M + 100M log 100M)
  - Query time: ~10 seconds (too slow!)

With Contraction Hierarchies:
  - Preprocessing: 24 hours (one-time)
  - Query time: O(log V) = ~27 hops
  - Speedup: 1000Ã— faster
  - Typical query: 50-200ms âœ“
```

### Rule #4: **Traffic Updates are Write-Heavy**

```
GPS probes from Android devices:
  - 1M location updates/second
  - Each update: 100 bytes
  - Write throughput: 1M Ã— 100 bytes = 100 MB/sec

Daily writes: 100 MB/sec Ã— 86,400 = 8.4 TB/day

Time-series database:
  - 8.4 TB/day Ã— 90 days retention = 756 TB
  - Compressed 6Ã—: 126 TB actual storage
  - TimescaleDB chunks: 1-hour intervals
```

---

## ğŸ“ˆ PART 3: Complete Scale Estimation Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GOOGLE MAPS PLATFORM - NAPKIN MATH SOLUTION          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Map Tile Requests/Day:     30 B (200M DAU Ã— 3 sessions Ã— 50 tiles)
Directions API/Day:        100 M
Geocoding API/Day:         500 M
Places API/Day:            300 M
Traffic Updates/sec:       1 M (GPS probes)

â†’ Tile QPS      = 30B Ã· 86.4K     = 350K QPS (avg)
â†’ Tile QPS      = 350K Ã— 2        = 700K QPS (peak)
â†’ Directions QPS = 100M Ã· 86.4K   = 1,157 QPS
â†’ Geocoding QPS  = 500M Ã· 86.4K   = 5,787 QPS
â†’ Places QPS     = 300M Ã· 86.4K   = 3,472 QPS
â†’ Traffic Writes = 1M writes/sec  (sustained)

STEP 2: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Map Tiles (Object Storage):
  - Pre-rendered zoom 0-15:       1 PB
  - Hot tiles cache (cities):     100 TB
  - Subtotal                      = 1.1 PB

Road Network Graph:
  - 100M nodes Ã— 200 bytes        = 20 GB
  - 200M edges Ã— 500 bytes        = 100 GB
  - Contraction Hierarchies index = 50 GB
  - Subtotal                      = 170 GB

Places Database (Elasticsearch):
  - 200M businesses Ã— 5 KB        = 1 TB
  - 3 replicas                    = 3 TB
  - Search indexes                = 500 GB
  - Subtotal                      = 3.5 TB

Traffic Data (TimescaleDB):
  - Daily ingestion: 8.4 TB/day
  - Retention: 90 days
  - Compressed (6Ã—): 126 TB

Street View:
  - 50M panoramas Ã— 2 MB          = 100 TB

Total Storage:                    = ~1.2 PB

STEP 3: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Tile Requests (read):
  - 350K QPS Ã— 15 KB              = 5.25 GB/sec (avg)
  - Peak: 700K QPS                = 10.5 GB/sec
  - Daily: 5.25 GB/sec Ã— 86,400   = 454 TB/day

CDN Optimization:
  - CDN serves (95%): 431 TB/day
  - Origin (5%): 23 TB/day

API Responses:
  - Directions: 1,157 QPS Ã— 5 KB  = 5.8 MB/sec
  - Geocoding: 5,787 QPS Ã— 2 KB   = 11.6 MB/sec
  - Places: 3,472 QPS Ã— 3 KB      = 10.4 MB/sec
  - Total API: 27.8 MB/sec

Traffic Ingestion (write):
  - GPS probes: 1M/sec Ã— 100 bytes = 100 MB/sec

Total Bandwidth:
  - Ingress: 100 MB/sec (traffic data)
  - Egress (CDN): 5 GB/sec (tiles)
  - Egress (API): 30 MB/sec

STEP 4: MEMORY (CACHE) ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Redis Cluster (Multi-tier):

L1 - Hot Tiles:
  - Popular tiles (major cities): 100 TB
  - Cache hit rate: 95%
  - TTL: 1-24 hours

L2 - Route Cache:
  - Common routes: 1M Ã— 10 KB    = 10 GB
  - TTL: 5 minutes (traffic changes)
  - Hit rate: 70%

L3 - Geocode Cache:
  - Popular addresses: 10M Ã— 1 KB = 10 GB
  - TTL: 24 hours
  - Hit rate: 80%

L4 - Place Cache:
  - Business details: 1M Ã— 5 KB   = 5 GB
  - TTL: 1 hour
  - Hit rate: 75%

Total Redis Memory:                = 100 TB + 25 GB
Recommended setup:
  - Tile cache: Separate CDN edge storage
  - API cache: Redis cluster (100 GB)
```

---

## ğŸ’¾ PART 4: Geospatial-Specific Calculations

### **Tile Coordinate Math**

```python
# Convert lat/lon to tile coordinates
import math

def lat_lon_to_tile(lat, lon, zoom):
    """
    Convert geographic coordinates to tile coordinates
    """
    n = 2 ** zoom
    
    x = int((lon + 180.0) / 360.0 * n)
    
    lat_rad = math.radians(lat)
    y = int((1.0 - math.asinh(math.tan(lat_rad)) / math.pi) / 2.0 * n)
    
    return (x, y, zoom)

# Example
lat, lon = 37.7749, -122.4194  # San Francisco
x, y, z = lat_lon_to_tile(lat, lon, 15)
# Returns: (5242, 12666, 15)

tile_url = f"https://mt1.google.com/vt?x={x}&y={y}&z={z}"
```

### **Tiles Needed for Viewport**

```python
def tiles_for_viewport(center_lat, center_lon, zoom, screen_width, screen_height):
    """
    Calculate how many tiles needed for screen viewport
    """
    tile_size = 256  # pixels
    
    # Tiles needed in each direction
    tiles_x = math.ceil(screen_width / tile_size) + 1
    tiles_y = math.ceil(screen_height / tile_size) + 1
    
    # Center tile
    center_x, center_y, _ = lat_lon_to_tile(center_lat, center_lon, zoom)
    
    # Calculate tile range
    tiles = []
    for dx in range(-tiles_x//2, tiles_x//2 + 1):
        for dy in range(-tiles_y//2, tiles_y//2 + 1):
            tiles.append((center_x + dx, center_y + dy, zoom))
    
    return tiles

# Example: Mobile screen (375Ã—667)
tiles = tiles_for_viewport(37.7749, -122.4194, 15, 375, 667)
print(f"Tiles needed: {len(tiles)}")  # ~12 tiles
```

### **Storage Calculation by Zoom Level**

```python
def storage_per_zoom(zoom, tile_size_kb=15):
    """
    Calculate storage needed for one zoom level (entire world)
    """
    num_tiles = (2 ** zoom) ** 2
    storage_gb = (num_tiles * tile_size_kb) / (1024 * 1024)
    
    return {
        'zoom': zoom,
        'tiles': f"{num_tiles:,}",
        'storage_gb': f"{storage_gb:,.0f}",
        'storage_tb': f"{storage_gb/1024:,.2f}"
    }

# Calculate for key zoom levels
for zoom in [0, 5, 10, 15, 20, 22]:
    result = storage_per_zoom(zoom)
    print(f"Zoom {result['zoom']}: {result['tiles']} tiles = {result['storage_tb']} TB")

# Output:
# Zoom 0: 1 tiles = 0.00 TB
# Zoom 5: 1,024 tiles = 0.00 TB
# Zoom 10: 1,048,576 tiles = 15.26 TB
# Zoom 15: 1,073,741,824 tiles = 15,625.00 TB (15.6 PB)
# Zoom 20: 1,099,511,627,776 tiles = 16,000,000.00 TB (16 EB!)
# Zoom 22: 17,592,186,044,416 tiles = 256,000,000.00 TB (256 EB!)

# Lesson: Cannot pre-render everything! Use on-demand for high zooms.
```

---

## ğŸ¯ PART 5: Regional Capacity Planning

### **Traffic Distribution by Region**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GLOBAL MAPS INFRASTRUCTURE PLANNING                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Region           Tile Req/Day   Infra Cost/Month   Notes
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
US-West           6B             $120K             SF, LA, Seattle
US-East           7B             $140K             NYC, Boston, DC
US-Central        3B             $60K              Chicago, Dallas
Europe-West       6B             $120K             London, Paris
Europe-East       2B             $40K              Warsaw, Prague
Asia-Pacific      8B             $160K             Tokyo, Singapore
India             4B             $80K              Delhi, Mumbai
Latin America     2B             $40K              SÃ£o Paulo
Middle East       1B             $20K              Dubai
Africa            1B             $20K              Lagos, Cairo
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL             40B            $800K/month

Infrastructure per region (example: US-East):
  - CDN edge servers (100 locations):       $50K
  - Tile origin servers (20Ã— c5.4xlarge):   $30K
  - API servers (30Ã— c5.2xlarge):           $20K
  - Databases (RDS, Elasticsearch):         $25K
  - Load balancers + networking:            $10K
  - Monitoring + backup:                    $5K
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Total:                                    $140K/month
```

---

## ğŸ“Š PART 6: Database Sizing Worksheet

### **Graph Database (Road Network)**

```
PostgreSQL + PostGIS for road network:

Nodes Table:
  - Rows: 100M intersections
  - Row size: 200 bytes (id, lat, lon, metadata)
  - Total: 100M Ã— 200 bytes = 20 GB
  - Indexes (B-tree, GIST): 10 GB

Edges Table:
  - Rows: 200M road segments
  - Row size: 500 bytes (from, to, geometry, speed, etc.)
  - Total: 200M Ã— 500 bytes = 100 GB
  - Indexes: 30 GB

Contraction Hierarchies:
  - Shortcuts: 50M additional edges
  - Storage: 25 GB

Total Graph DB: 185 GB (fits in memory!)

Recommended Instance:
  - AWS RDS: db.r5.8xlarge (256 GB RAM)
  - Storage: 500 GB SSD
  - Cost: ~$6,000/month
```

### **Elasticsearch (Places)**

```
Places Index:
  - Documents: 200M businesses
  - Document size: 2 KB (indexed fields)
  - Total per index: 400 GB
  - Replicas (3Ã—): 1.2 TB

Cluster Configuration:
  - 12 data nodes: i3.2xlarge (8 vCPU, 61 GB RAM, 1.9 TB NVMe)
  - 3 master nodes: m5.large
  - Total cost: ~$18K/month

Performance:
  - Search QPS: 5K
  - Indexing rate: 10K docs/sec
  - Geo query latency: <50ms (P95)
```

### **TimescaleDB (Traffic)**

```
Traffic Speeds Table:
  - Inserts: 1M rows/sec
  - Row size: 100 bytes
  - Daily data: 8.4 TB
  - Retention: 90 days
  - Total: 756 TB (uncompressed)
  - Compressed (6Ã—): 126 TB

Hypertable Config:
  - Chunk interval: 1 hour
  - Compression after: 24 hours
  - Retention policy: DROP after 90 days

Cluster Setup:
  - 6Ã— r5.4xlarge (16 vCPU, 128 GB RAM)
  - Storage: 150 TB distributed
  - Cost: ~$30K/month
```

---

## ğŸš€ PART 7: Performance Benchmarks

### **Latency Budget Breakdown**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TILE REQUEST LATENCY (Target: <200ms P95)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Component                    P50      P95      P99
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Client â†’ CDN Edge            20ms     50ms     100ms
CDN Cache Lookup             2ms      5ms      10ms
CDN â†’ Origin (miss)          40ms     100ms    200ms
Origin Storage Fetch         30ms     80ms     150ms
Tile Compression             5ms      15ms     30ms
Response to Client           20ms     50ms     100ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TOTAL (Cache Hit)            42ms     105ms    210ms âœ“
TOTAL (Cache Miss)           117ms    300ms    590ms âš ï¸

Optimization: 95% cache hit â†’ P95 <150ms
```

---

## ğŸ¨ PART 8: Cost Optimization Strategies

### **Strategy 1: Selective Pre-rendering**

```
Smart pre-rendering strategy:
  - Zoom 0-10: Pre-render globally (small # of tiles)
  - Zoom 11-15: Pre-render major cities only
  - Zoom 16-22: On-demand rendering

Storage savings:
  - Full pre-render (all zooms): 262 PB
  - Smart pre-render: 1.1 PB
  - Savings: 99.6% reduction

On-demand rendering cost:
  - CPU cost: $0.0001 per tile
  - 5% cache misses: 30B Ã— 0.05 = 1.5B tiles/day
  - Cost: 1.5B Ã— $0.0001 = $150K/day
  - Still cheaper than storing 262 PB!
```

### **Strategy 2: Tiered CDN Caching**

```
Multi-tier cache TTL:
  - Zoom 0-5 (world/continents): 7 days
  - Zoom 6-10 (countries): 3 days
  - Zoom 11-15 (cities): 1 day
  - Zoom 16-22 (streets): 1 hour

Cache efficiency:
  - Low zooms: 99.9% hit rate (rarely change)
  - High zooms: 90% hit rate (frequent updates)
  - Average: 95% hit rate

Bandwidth savings: 
  - Without cache: 450 TB/day origin
  - With cache: 23 TB/day origin
  - Savings: 95% ($38K/day)
```

---

## ğŸ” PART 9: Practice Problems

### **Problem 1: Satellite Imagery Layer**

```
Given:
  - Google Earth satellite imagery
  - Multiple zoom levels (0-19)
  - Average satellite image tile: 50 KB (JPEG, compressed)
  - Update frequency: Yearly for most areas
  - High-resolution: Major cities updated monthly

Calculate:
1. Storage needed for global satellite tiles
2. Bandwidth for serving satellite view
3. Cost difference vs road map tiles
4. Update/ingestion bandwidth

[Try yourself, then check answer]
```

<details>
<summary>Answer</summary>

```
1. STORAGE:
   Satellite tiles are larger (50 KB vs 15 KB for vector)
   
   Zoom 0-15 (pre-rendered):
   - Tiles: 2^30 (1B tiles)
   - Size: 1B Ã— 50 KB = 50 TB
   
   Zoom 16-19 (major cities, ~10% of world):
   - Tiles: 2^38 Ã— 0.1 = 27B tiles
   - Size: 27B Ã— 50 KB = 1.35 PB
   
   Total: ~1.4 PB (vs 1.1 PB for road map)

2. BANDWIDTH:
   Satellite view usage: 20% of total map views
   - Tile requests: 30B Ã— 0.2 = 6B tiles/day
   - Data: 6B Ã— 50 KB = 300 TB/day
   
   With 95% CDN cache:
   - Origin: 15 TB/day
   - CDN: 285 TB/day

3. COST DIFFERENCE:
   Storage: 1.4 PB vs 1.1 PB = +300 TB
   - Extra cost: 300 TB Ã— $0.023/GB/month = $7K/month
   
   Bandwidth: 300 TB/day vs 454 TB/day (lower usage)
   - Satellite is less frequently used

4. UPDATE BANDWIDTH:
   Yearly updates for most areas:
   - 1.4 PB / 365 days = 3.8 TB/day ingestion
   
   Monthly updates for cities (10%):
   - 140 TB / 30 days = 4.7 TB/day
   
   Total ingestion: ~8.5 TB/day
```
</details>

---

### **Problem 2: Indoor Maps for Airports/Malls**

```
Given:
  - 1,000 major airports worldwide
  - 5,000 large shopping malls
  - Average building: 5 floors, 100K sqft per floor
  - Indoor positioning accuracy: 5 meters
  - Map detail level equivalent to zoom 19-20

Calculate:
1. Number of indoor tiles needed
2. Storage for indoor maps
3. Indoor positioning infrastructure
4. Real-time updates (people/shop locations)

[Try yourself]
```

<details>
<summary>Answer</summary>

```
1. INDOOR TILES:
   Each building converted to tiles:
   - Area: 500K sqft total = 46,500 sqm
   - At zoom 19, each tile â‰ˆ 600 sqm
   - Tiles per building: 46,500 / 600 = 78 tiles
   - Total buildings: 6,000
   - Total tiles: 6,000 Ã— 78 Ã— 5 floors = 2.34M tiles

2. STORAGE:
   Indoor tiles (detailed):
   - 2.34M tiles Ã— 50 KB = 117 GB
   - With 3D models: +500 GB
   - Total: ~650 GB (manageable!)

3. INDOOR POSITIONING:
   WiFi/Bluetooth beacons:
   - 1 beacon per 100 sqm
   - Total beacons: 6,000 Ã— 500K sqft / 100 = 280K beacons
   - Database: 280K Ã— 500 bytes = 140 MB
   
   Position updates:
   - 1M concurrent indoor users
   - Update frequency: 5 seconds
   - Write QPS: 1M / 5 = 200K writes/sec

4. REAL-TIME UPDATES:
   Store/restaurant locations:
   - 6,000 buildings Ã— 200 stores avg = 1.2M stores
   - Hours/status updates: 1.2M Ã— 1 KB = 1.2 GB
   - Update frequency: Daily
   
   Live occupancy (optional):
   - Track crowd density in real-time
   - Heatmap updates: Every 5 minutes
```
</details>

---

## ğŸ“ PART 10: Interview Cheat Sheet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOOGLE MAPS PLATFORM - INTERVIEW CHECKLIST         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CLARIFYING QUESTIONS (5 min):
  â–¡ Which features? (tiles, directions, traffic, all?)
  â–¡ Geographic scope? (global or specific regions)
  â–¡ Expected scale? (DAU, requests/day)
  â–¡ Latency requirements? (<200ms for tiles typical)
  â–¡ Consistency requirements? (eventual OK for traffic)
  â–¡ Offline support needed?

SCALE ESTIMATION (5-7 min):
  â–¡ Traffic: Map views â†’ tile requests
  â–¡ Storage: Tile pyramid math (exponential!)
  â–¡ Bandwidth: CDN mandatory (95%+ cache hit)
  â–¡ Cache: Hot tiles, routes, geocodes
  â–¡ Compute: Routing queries, traffic processing

DESIGN (20-25 min):
  â–¡ Tile system: Mercator projection, zoom 0-22
  â–¡ CDN architecture: Edge locations, cache strategy
  â–¡ Routing: Contraction Hierarchies algorithm
  â–¡ Geocoding: Elasticsearch fuzzy matching
  â–¡ Traffic: Streaming GPS â†’ map matching â†’ aggregation
  â–¡ Database sharding: By region/geohash

DEEP DIVES (10 min):
  â–¡ How to handle tile pyramid explosion?
  â–¡ Routing algorithm optimization (CH vs Dijkstra)
  â–¡ Real-time traffic aggregation pipeline
  â–¡ CDN cache invalidation strategy
  â–¡ Handling map data updates
  â–¡ Offline maps implementation

TRADE-OFFS:
  â–¡ Pre-rendering vs on-demand (storage vs compute)
  â–¡ Raster vs vector tiles (bandwidth vs flexibility)
  â–¡ Accuracy vs performance (routing)
  â–¡ Freshness vs cost (tile cache TTL)
  â–¡ Global vs regional deployment
```

---

## ğŸ BONUS: Quick Reference Card

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘    GOOGLE MAPS SCALE CHEAT SHEET                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

TYPICAL SCALE:
  DAU:                  200M
  Tile requests/day:    30B
  Directions/day:       100M
  Traffic updates/sec:  1M
  Tile QPS (avg):       350K (peak: 700K)
  Storage:              1.2 PB
  
TILE PYRAMID:
  Zoom 0:  1 tile
  Zoom 10: 1M tiles (2^20)
  Zoom 15: 1B tiles (2^30)
  Zoom 22: 17.5 trillion tiles (2^44)
  
  Storage per zoom level (15 KB tiles):
  Zoom 10: 15 GB
  Zoom 15: 15 TB
  Zoom 20: 16 PB
  Zoom 22: 256 PB (!)

DATABASE CHOICES:
  Tiles:          S3/GCS + CDN
  Road graph:     PostgreSQL + PostGIS
  Places:         Elasticsearch
  Traffic:        TimescaleDB
  Cache:          Redis Cluster + CDN edge

KEY ALGORITHMS:
  Routing:        Contraction Hierarchies (1000Ã— speedup)
  Map matching:   Hidden Markov Model (HMM)
  Geocoding:      Elasticsearch fuzzy match
  Traffic:        Median aggregation (30-sec windows)

LATENCY TARGETS:
  Tile load (CDN hit):   <100ms
  Directions:            <500ms
  Geocoding:             <300ms
  Place search:          <200ms
  
CDN STRATEGY:
  Cache hit rate:     95%
  Tile TTL:           1 hour (high zoom) to 7 days (low zoom)
  Edge locations:     200+
  Bandwidth savings:  95% (427 TB/day CDN vs 23 TB origin)

COST BREAKDOWN (monthly):
  Tile storage (S3):        $25K  (1.1 PB)
  CDN bandwidth:            $250K (13 PB/month egress)
  Compute (API servers):    $100K (500 instances)
  Databases:                $75K  (PostgreSQL, ES, Timescale)
  Traffic processing:       $50K  (Kafka, Flink)
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  TOTAL:                    ~$500K/month

SANITY CHECKS:
  âœ“ Can CDN cache 100TB of hot tiles? YES
  âœ“ Can serve 700K tile QPS? YES (with CDN)
  âœ“ Can PostgreSQL handle 100M node graph? YES (in-memory)
  âœ“ Can process 1M GPS updates/sec? YES (Kafka + Flink)
  âœ“ Can store 1.2 PB tiles? YES (object storage)
  
  âœ— Can pre-render all zoom levels? NO (256 PB needed)
  âœ— Can use Dijkstra for routing? NO (too slow)
  âœ— Can store tiles in database? NO (use object storage)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“ Final Wisdom for Maps Platforms

> **"In mapping systems, the tile pyramid grows exponentiallyâ€”plan accordingly"**

**Key Principles:**

1. **Selective Pre-rendering**: Cannot render everything, prioritize by usage
2. **CDN is Non-Optional**: 95%+ cache hit rate saves millions
3. **Graph Preprocessing**: Routing speed = precomputation quality
4. **Regional Sharding**: Geography determines data locality
5. **Streaming for Traffic**: Real-time aggregation, not batch

---

**Remember:**
> "Google Maps is one of the most complex distributed systems ever builtâ€”focus on understanding the core trade-offs rather than memorizing every detail."

**Now go crush those maps system design interviews!** ğŸ—ºï¸ğŸš€

---

*Created with the MAPS-SCALE framework: Map â†’ Assess â†’ Plan â†’ Shard â†’ Store â†’ Cache â†’ Aggregate â†’ Localize â†’ Evaluate*
