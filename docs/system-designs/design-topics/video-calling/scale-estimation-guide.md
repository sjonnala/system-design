# ğŸ¯ Video Calling System Scale Estimation Masterclass

## The POWER Technique for Video Calling Scale Math
**(P)rinciples â†’ (O)rder of magnitude â†’ (W)rite it down â†’ (E)stimate â†’ (R)ound ruthlessly**

This framework applies to ANY video calling system: Zoom, Google Meet, Microsoft Teams, WebEx, etc.

---

## ğŸ“Š PART 1: Users & Scale Estimation

### Core Assumptions Table

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 300 Million | Zoom-scale (300M daily participants) |
| | Peak Concurrent Meetings | 10 Million | ~3% of users in meetings simultaneously |
| | Average Meeting Size | 5 participants | Mix of 1-on-1 and small groups |
| | Large Meetings (50+ ppl) | 1% of meetings | Webinars, all-hands |
| **Meeting Distribution** | 1-on-1 calls | 40% | Most common type |
| | Small groups (3-10) | 50% | Team meetings |
| | Large groups (10-50) | 9% | Department meetings |
| | Webinars (50+) | 1% | Company-wide events |
| **Duration** | Average meeting length | 45 minutes | Typical business meeting |
| | Peak hours | 8 hours/day | Business hours (9am-5pm) |
| | Peak traffic multiplier | 2x | Start of hour (meetings begin) |
| **Quality Settings** | HD (720p) usage | 60% | Default for most users |
| | FHD (1080p) usage | 10% | Premium users |
| | SD (480p) usage | 30% | Low bandwidth users |
| **Recording** | Meetings recorded | 20% | Cloud recording feature |
| | Recording retention | 90 days | Default retention policy |

---

## ğŸ§® PART 2: The "Video Bandwidth Calculator" - Your Mental Math Toolkit

### Rule #1: **The Bandwidth Ladder**
```
Remember these anchors for video calling:
â€¢ Audio only: 50 Kbps (Opus codec, good quality)
â€¢ 360p video: 500 Kbps
â€¢ 480p (SD): 1 Mbps
â€¢ 720p (HD): 2.5 Mbps
â€¢ 1080p (FHD): 4 Mbps
â€¢ 4K: 15 Mbps (rarely used in conferencing)

Screen Share: 1-2 Mbps (lower framerate)
```

### Rule #2: **The P2P vs SFU Decision**
```
1-on-1 call (P2P - Peer to Peer):
  Upload: 2.5 Mbps (720p)
  Download: 2.5 Mbps (720p)
  Total: 5 Mbps symmetric
  Server cost: $0 (direct connection)

Group call with 10 people (SFU - Selective Forwarding Unit):
  Upload per person: 2.5 Mbps (1 stream to SFU)
  Download per person: 9 Ã— 2.5 Mbps = 22.5 Mbps (Gallery view)
  Server bandwidth: 10 Ã— 2.5 Mbps (in) + 90 Ã— 2.5 Mbps (out) = 250 Mbps

  Why SFU wins:
  âœ“ Upload stays constant (2.5 Mbps) regardless of group size
  âœ“ Server doesn't transcode (low CPU)
  âœ— High server bandwidth
```

### Rule #3: **The Recording Storage Formula**
```
Video recording storage per hour:
â€¢ 480p: ~200 MB/hour (H.264 compression)
â€¢ 720p: ~400 MB/hour
â€¢ 1080p: ~800 MB/hour

Audio recording: ~10 MB/hour (Opus)

Quick calc: 720p meeting â‰ˆ 400 MB/hour â‰ˆ 0.4 GB/hour
```

---

## ğŸ“ˆ PART 3: Quick Scale Math Template (COPY THIS!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ VIDEO CALLING NAPKIN MATH TEMPLATE - Universal     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: CONCURRENT PARTICIPANTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Users:                [____] M
Peak Concurrent %:          [____] %
Avg Meeting Size:           [____] participants

â†’ Concurrent Participants = Users Ã— Peak% = [____] M
â†’ Concurrent Meetings = Participants Ã· Avg Size = [____] M

STEP 2: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Video Quality:              [____] p (resolution)
Bitrate per stream:         [____] Mbps

For SFU-based (groups):
â†’ Upload per user = Bitrate                    = [____] Mbps
â†’ Download per user = (Size - 1) Ã— Bitrate     = [____] Mbps
â†’ SFU bandwidth = Participants Ã— Upload + 
                  Participants Ã— (Size-1) Ã— Bitrate = [____] Gbps

STEP 3: SERVER CAPACITY (SFU)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Participants per SFU:       [____] (typically 200-500)
Bandwidth per SFU:          [____] Gbps

â†’ SFU servers needed = Concurrent Ã· Participants_per_SFU = [____]

STEP 4: RECORDING STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meetings recorded:          [____] %
Avg meeting duration:       [____] minutes
Recording bitrate:          [____] MB/hour

â†’ Daily recordings = Meetings Ã— Recorded% Ã— Duration Ã— Bitrate
â†’ 90-day storage = Daily Ã— 90 = [____] PB

STEP 5: TURN SERVER BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TURN usage rate:            [____] % (typically 10-15%)
Bandwidth multiplier:       2Ã— (upload + download relay)

â†’ TURN bandwidth = Concurrent Ã— TURN% Ã— Bitrate Ã— 2 = [____] Gbps
```

---

## ğŸ’¾ PART 4: Zoom-Scale System Filled Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      ZOOM-SCALE VIDEO CALLING - NAPKIN MATH SOLUTION   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: CONCURRENT PARTICIPANTS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Users:                300 M daily participants
Peak Concurrent %:          3%
Avg Meeting Size:           5 participants

â†’ Concurrent Participants = 300M Ã— 3% = 10 M participants
â†’ Concurrent Meetings = 10M Ã· 5 = 2 M meetings

STEP 2: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Video Quality:              720p (HD, most common)
Bitrate per stream:         2.5 Mbps

Breakdown:
â€¢ 1-on-1 (40%): 4M participants â†’ 2M calls
  - Bandwidth: 2M Ã— 5 Mbps (P2P) = 10,000 Gbps (user bandwidth, not server)

â€¢ Small groups (50%): 5M participants â†’ 1M meetings (avg 5 ppl)
  - Upload per user: 2.5 Mbps
  - Download per user: 4 Ã— 2.5 = 10 Mbps
  - SFU bandwidth: 5M Ã— 2.5 Mbps (in) + 5M Ã— 10 Mbps (out)
    = 12.5 Gbps + 50 Gbps = 62.5 Gbps per region

â€¢ Large groups (10%): 1M participants â†’ 50K meetings (avg 20 ppl)
  - SFU bandwidth: More complex (simulcast + selective forwarding)
  - Estimated: ~20 Gbps per region

â†’ Total SFU Bandwidth (per region): ~85 Gbps
â†’ Multi-region (5 regions): 85 Ã— 5 = 425 Gbps global

STEP 3: SERVER CAPACITY (SFU)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Participants per SFU:       300 (conservative estimate)
Bandwidth per SFU:          10 Gbps NIC

â†’ SFU servers needed = 10M Ã· 300 = 33,333 SFU instances
â†’ With multi-region: ~6,700 per region (5 regions)
â†’ With auto-scaling + overhead: ~40,000 total servers

STEP 4: RECORDING STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Meetings recorded:          20%
Avg meeting duration:       45 minutes = 0.75 hours
Recording bitrate:          400 MB/hour (720p)

â†’ Daily recordings = 2M meetings Ã— 20% Ã— 0.75h Ã— 400MB
                   = 400,000 Ã— 0.75 Ã— 400MB
                   = 120,000 GB/day = 120 TB/day

â†’ 90-day storage = 120 TB Ã— 90 = 10,800 TB â‰ˆ 11 PB

With compression & lifecycle (move to Glacier):
â†’ Active (30 days): 3.6 PB (S3 Standard)
â†’ Archive (60 days): 7.2 PB (S3 Glacier)

STEP 5: TURN SERVER BANDWIDTH
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TURN usage rate:            12% (typical in enterprise)
Bandwidth multiplier:       2Ã— (relay traffic both ways)

â†’ TURN users = 10M Ã— 12% = 1.2M participants
â†’ TURN bandwidth = 1.2M Ã— 2.5 Mbps (upload) Ã— 2
                 = 1.2M Ã— 5 Mbps = 6,000 Gbps = 6 Tbps

This is HUGE! TURN is expensive.
Optimization: Use TURN only when P2P fails.

STEP 6: SIGNALING SERVERS
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Signaling is lightweight (WebSocket):
â†’ 10M concurrent connections
â†’ 100K connections per server (Node.js/Go)
â†’ Servers needed: 10M Ã· 100K = 100 signaling servers

Messages per second (join/leave/mute/etc.):
â†’ Assume 10 messages/min per participant
â†’ 10M Ã— 10 Ã· 60 = 1.6M messages/second
â†’ Easily handled by 100 servers

STEP 7: DATABASE & CACHE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PostgreSQL (Metadata):
â€¢ User accounts: 300M users Ã— 1KB = 300 GB
â€¢ Meeting history: 2M meetings/day Ã— 365 days Ã— 2KB = 1.5 TB/year
â€¢ Total: ~2 TB (easily fits in single DB with replicas)

Redis (Active meetings, presence):
â€¢ Active meetings: 2M meetings Ã— 10 KB = 20 GB
â€¢ User presence: 10M users Ã— 1 KB = 10 GB
â€¢ Total: ~30 GB (single Redis instance or small cluster)
```

---

## ğŸ§  PART 5: Video-Specific Mental Math Techniques

### **Technique 1: The "Group Size Multiplier"**
*(For Bandwidth)*
```
EMOTION TRIGGER: "Video bandwidth grows with NÂ²!"

1-on-1: 2 people Ã— 2.5 Mbps = 5 Mbps total traffic
4 people: 4 upload + 12 download = 16 Mbps total traffic
10 people: 10 upload + 90 download = 100 Mbps total traffic

Formula for N people:
- Total connections: N Ã— (N-1) 
- Total bandwidth: N Ã— (N-1) Ã— Bitrate

But with SFU:
- Upload: N Ã— Bitrate (linear growth!)
- Download: N Ã— (N-1) Ã— Bitrate
- Much more scalable
```

### **Technique 2: The "Simulcast Savings"**
*(For Bandwidth Optimization)*
```
Without Simulcast:
- 10 people, each sends 720p (2.5 Mbps)
- Download per user: 9 Ã— 2.5 = 22.5 Mbps
- Gallery view: All streams shown â†’ 22.5 Mbps needed

With Simulcast (3 layers):
- Each user sends: 360p (0.5 Mbps) + 480p (1 Mbps) + 720p (2.5 Mbps)
- Upload: 4 Mbps per user (slightly higher)
- SFU selects quality per receiver:
  - Gallery view: Send 360p for thumbnails â†’ 9 Ã— 0.5 = 4.5 Mbps
  - Speaker view: Send 720p for active speaker â†’ 2.5 Mbps + 8 Ã— 0.5 = 6.5 Mbps

Savings: 22.5 Mbps â†’ 6.5 Mbps (70% reduction!)
```

### **Technique 3: The "Recording Cost Calculator"**
*(For Storage)*
```
Key insight: Recordings compress well!

Live stream: 2.5 Mbps (for realtime)
Recorded file: 400 MB/hour (H.264 optimized)

Conversion:
2.5 Mbps Ã— 3600 seconds = 9000 Mb = 1125 MB/hour (live)
After compression: 400 MB/hour (recorded)
Compression ratio: 65% savings

Quick rule: 400 MB/hour â‰ˆ 0.4 GB/hour for 720p
```

### **Technique 4: The "Server Capacity Estimate"**
*(For SFU Sizing)*
```
Server capacity depends on:
1. Network bandwidth (10 Gbps NIC typical)
2. CPU (encode/decode if needed)

SFU doesn't transcode â†’ Network-bound, not CPU-bound

10 Gbps NIC:
- 10,000 Mbps Ã· 2.5 Mbps = 4,000 streams theoretical
- With overhead (50%): 2,000 streams practical
- Avg meeting size 5 â†’ 400 meetings per SFU
- Participants: 2,000 total per SFU

But Zoom uses 300-500 participants per SFU (conservative)
```

---

## ğŸ¯ PART 6: The Visual Mind Map Approach

```
                    ğŸ¥ VIDEO CALLING SYSTEM
                          |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                 |                 |
    ğŸ“Š USERS          ğŸŒ BANDWIDTH      â˜ï¸ SERVERS
        |                 |                 |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   DAU   Conc       Mbps   Tbps      SFU    TURN
   300M   10M       2.5    0.5       40K    1K
```

**Memory Trigger**: Think **"U.B.S."** = Users, Bandwidth, Servers

---

## ğŸ¯ PART 7: The Interview Cheat Sheet (Print This!)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VIDEO CALLING SCALE ESTIMATION - 5 MIN RITUAL  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

[ ] 1. Concurrent users (DAU Ã— peak %, typically 3-5%)
[ ] 2. Meeting size (1-on-1, small group, large group breakdown)
[ ] 3. Bandwidth per stream (720p = 2.5 Mbps)
[ ] 4. SFU vs P2P decision (P2P for 1-on-1, SFU for groups)
[ ] 5. SFU servers (10M concurrent Ã· 300/server = 33K servers)
[ ] 6. Recording storage (20% recorded Ã— 400MB/hour Ã— 90 days)
[ ] 7. TURN bandwidth (10-15% users Ã— 2Ã— relay = expensive!)
[ ] 8. Smell test: Does 40K servers for 300M users sound right? YES!
```

---

## ğŸš€ Key Metrics Summary Table

| **Metric** | **Value** | **Why It Matters** |
|------------|-----------|-------------------|
| **Concurrent Participants** | 10M | Size your infrastructure |
| **SFU Servers** | 40K | Server capacity planning |
| **SFU Bandwidth (total)** | 500 Gbps | Network backbone capacity |
| **TURN Bandwidth** | 6 Tbps | Most expensive component! |
| **Recording Storage (90d)** | 11 PB | S3/GCS storage costs |
| **Signaling Servers** | 100 | WebSocket connection handling |
| **Avg Bandwidth (per user)** | 10-15 Mbps | User internet requirement |

---

## ğŸ’¡ Pro Architect Tips

### **Tip 1: The Smell Test**
After calculations, ask:
- "Can 1 SFU server handle 300 participants?" â†’ YES (network-bound)
- "Is 11 PB storage for 90 days recordings reasonable?" â†’ YES (Zoom stores petabytes)
- "Does 6 Tbps TURN bandwidth sound expensive?" â†’ YES (this is why TURN is fallback only!)

### **Tip 2: The Comparison Anchor**
Always compare to known systems:
- "Zoom: 300M daily participants (our calc: same) âœ“"
- "Google Meet: Similar scale, integrated with Workspace"
- "Microsoft Teams: 250M+ users"

### **Tip 3: Start with Constraints**
Always ask first:
1. Expected concurrent users?
2. Average meeting size?
3. Video quality requirements (SD, HD, FHD)?
4. Recording needed? Retention period?
5. Geographic distribution (multi-region)?

---

## ğŸ“š Quick Reference: Common Video Calling Benchmarks

| **System** | **DAU** | **Concurrent** | **Bandwidth/user** | **Infrastructure** |
|------------|---------|----------------|--------------------|--------------------|
| **Zoom** | 300M | 10M | 10-15 Mbps | 40K+ servers, multi-region |
| **Google Meet** | 250M | 8M | 10-15 Mbps | Google Cloud infrastructure |
| **Microsoft Teams** | 250M | 8M | 10-15 Mbps | Azure datacenters |
| **WebEx** | 150M | 5M | 10-15 Mbps | Cisco infrastructure |
| **Discord** | 150M | 4M (voice heavy) | 5-10 Mbps | Self-hosted + AWS |

---

## ğŸ”§ Practical Application: Adapting This Template

### For a **Startup Video App** (1M users):
```
STEP 1: TRAFFIC
- 1M users Ã— 5% concurrent = 50K participants
- Avg meeting size: 3 people â†’ 17K meetings

STEP 2: BANDWIDTH
- 720p SFU: 50K Ã— 2.5 Mbps (up) + 50K Ã— 5 Mbps (down) = 375 Gbps
- Single region: Manageable with cloud providers

STEP 3: SERVERS
- SFU: 50K Ã· 300 = 167 servers
- Signaling: 50K Ã· 100K = 1 server
- Cost: ~$10K-20K/month (AWS/GCP)

STEP 4: STORAGE
- 20% recorded: 3,400 meetings Ã— 0.75h Ã— 400MB = 1 TB/day
- 90 days: 90 TB (S3 costs ~$2K/month)
```

### For an **Enterprise Solution** (10M users):
```
STEP 1: TRAFFIC
- 10M users Ã— 3% concurrent = 300K participants
- Avg meeting size: 8 people (larger teams) â†’ 37.5K meetings

STEP 2: BANDWIDTH
- Higher quality (1080p): 4 Mbps per stream
- SFU bandwidth: More complex (simulcast layers)
- Estimated: 1.5 Tbps (multi-region)

STEP 3: SERVERS
- SFU: 300K Ã· 300 = 1,000 servers (multi-region)
- Redundancy + auto-scaling: 1,500 servers
- Cost: ~$150K-200K/month

STEP 4: SPECIAL REQUIREMENTS
- SSO integration (OAuth, SAML)
- Compliance (HIPAA, SOC 2, GDPR)
- Dedicated infrastructure (VPC, private subnets)
- Advanced analytics and admin controls
```

---

## ğŸ¯ Mental Math Practice Problems

### Problem 1: Small Business Video Platform
```
Given:
- 5M registered users
- 10% daily active
- 5% in meetings at peak
- Average meeting: 4 people
- 720p default, 480p fallback for 30%
- 15% of meetings recorded
- 1-hour average meeting duration

Calculate:
1. Concurrent participants at peak
2. SFU servers needed (300 ppl/server)
3. Total SFU bandwidth
4. Recording storage per day
5. 30-day storage requirement

[Try it yourself, then check answers below]
```

<details>
<summary>Answer</summary>

```
1. CONCURRENT PARTICIPANTS:
   - DAU: 5M Ã— 10% = 500K daily active users
   - Peak concurrent: 500K Ã— 5% = 25K participants

2. SFU SERVERS:
   - Participants per server: 300
   - Servers needed: 25K Ã· 300 = 84 servers
   - With redundancy: ~100 servers

3. SFU BANDWIDTH:
   - Avg meeting size: 4 people
   - Upload per user: 2.5 Mbps (720p weighted avg)
   - Download per user: 3 Ã— 2.5 = 7.5 Mbps
   - Total upload: 25K Ã— 2.5 = 62.5 Gbps
   - Total download: 25K Ã— 7.5 = 187.5 Gbps
   - Total SFU: 250 Gbps

4. RECORDING STORAGE (PER DAY):
   - Meetings per day: 500K DAU Ã· 4 (avg size) = 125K meetings
   - Recorded: 125K Ã— 15% = 18,750 meetings
   - Duration: 1 hour
   - Size: 400 MB/hour (720p)
   - Total: 18,750 Ã— 400 MB = 7,500 GB = 7.5 TB/day

5. 30-DAY STORAGE:
   - 7.5 TB Ã— 30 = 225 TB
```
</details>

---

### Problem 2: Webinar Platform (Large Meetings)
```
Given:
- 50 simultaneous webinars at peak
- Average attendance: 500 people
- 1 presenter (1080p @ 4 Mbps)
- Viewers receive: 720p @ 2.5 Mbps
- 80% of webinars recorded
- Average duration: 1.5 hours

Calculate:
1. Total concurrent participants
2. Bandwidth from SFU to viewers
3. SFU servers needed
4. Daily recording storage
5. Why webinars are different from regular meetings

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. CONCURRENT PARTICIPANTS:
   - 50 webinars Ã— 500 people = 25,000 participants

2. BANDWIDTH (SFU â†’ Viewers):
   - Each viewer downloads 1 stream (presenter only)
   - 25,000 viewers Ã— 2.5 Mbps = 62,500 Mbps = 62.5 Gbps
   - Upload to SFU: 50 presenters Ã— 4 Mbps = 200 Mbps (negligible)
   - Total SFU egress: 62.5 Gbps

3. SFU SERVERS:
   - Webinars use different model (1-to-many)
   - Each webinar = 1 incoming + 500 outgoing
   - Server can handle ~10 webinars (network-bound)
   - Servers needed: 50 Ã· 10 = 5 servers
   
   (Much fewer servers than regular meetings because
    it's broadcast, not mesh!)

4. DAILY RECORDING STORAGE:
   - Assume 200 webinars per day
   - Recorded: 200 Ã— 80% = 160 webinars
   - Duration: 1.5 hours
   - Size: 800 MB/hour (1080p) Ã— 1.5 = 1.2 GB per webinar
   - Total: 160 Ã— 1.2 GB = 192 GB/day

5. WHY DIFFERENT:
   - Webinars are 1-to-many (broadcast model)
   - Viewers don't upload (except Q&A/chat)
   - CDN can be used for delivery (edge caching)
   - Much more efficient than mesh topology
   - Can scale to 10,000+ viewers per webinar with CDN
```
</details>

---

## ğŸš¨ Common Mistakes to Avoid

### Mistake 1: **Forgetting Simulcast**
```
âœ— BAD:  "10 people Ã— 2.5 Mbps download = 25 Mbps per user"
âœ“ GOOD: "With simulcast: 1 Ã— 2.5 Mbps (speaker) + 9 Ã— 0.5 Mbps (thumbnails) = 7 Mbps"
```

### Mistake 2: **Underestimating TURN Costs**
```
âœ— BAD:  "TURN is just a fallback, ignore it"
âœ“ GOOD: "10-15% of traffic uses TURN Ã— 2Ã— bandwidth = major cost driver!"
```

### Mistake 3: **Ignoring Peak Traffic**
```
âœ— BAD:  "Average of 5M concurrent users"
âœ“ GOOD: "Average 5M, but peak at hour start (2Ã—) = 10M. Size for peak!"
```

### Mistake 4: **Wrong Architecture Choice**
```
âœ— BAD:  "Use P2P for all calls (save server cost)"
âœ“ GOOD: "P2P for 1-on-1, SFU for groups (upload bandwidth limited on client)"
```

### Mistake 5: **Not Considering Regional Distribution**
```
âœ— BAD:  "Single datacenter with 40K servers"
âœ“ GOOD: "Multi-region: US (10K), EU (10K), APAC (10K), etc. for latency"
```

---

## ğŸ Bonus: Video Calling Cheat Sheet (1-Page)

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        VIDEO CALLING SCALE ESTIMATION CHEAT SHEET      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BANDWIDTH ANCHORS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Audio: 50 Kbps (Opus)
â€¢ 360p: 500 Kbps
â€¢ 480p (SD): 1 Mbps
â€¢ 720p (HD): 2.5 Mbps â† Most common
â€¢ 1080p (FHD): 4 Mbps
â€¢ Screen Share: 1-2 Mbps

FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Concurrent Users = DAU Ã— Peak%
SFU Upload = Participants Ã— Bitrate
SFU Download = Participants Ã— (Size - 1) Ã— Bitrate
SFU Servers = Concurrent Ã· Participants_per_Server
Recording Storage = Meetings Ã— Duration Ã— 400MB/hour

TYPICAL RATIOS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Peak Concurrent: 3-5% of DAU
â€¢ Avg Meeting Size: 3-5 people
â€¢ 1-on-1 Meetings: 40-50% of total
â€¢ Recording Rate: 20-30% of meetings
â€¢ TURN Usage: 10-15% (expensive!)
â€¢ Participants per SFU: 300-500

ARCHITECTURE DECISIONS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1-on-1: P2P (direct connection, no server)
3-10 people: SFU (selective forwarding)
10-50 people: SFU with simulcast
50+ people: SFU + MCU hybrid or webinar mode

QUICK ESTIMATES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Small:   100K users,  5K concurrent,   20 SFU servers
Medium:  5M users,    250K concurrent, 1K SFU servers
Large:   100M users,  5M concurrent,   20K SFU servers
Massive: 300M users,  10M concurrent,  40K SFU servers

COST DRIVERS (by $$):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. TURN bandwidth (ğŸ’°ğŸ’°ğŸ’°) - 10-15% users Ã— 2Ã— relay
2. SFU servers (ğŸ’°ğŸ’°) - 40K servers for Zoom scale
3. Recording storage (ğŸ’°) - 11 PB for 90 days
4. Signaling servers (ğŸ’µ) - Lightweight, cheap

INTERVIEW FLOW:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. Clarify requirements (5 min)
   â†’ Users? Quality? Group size? Recording?

2. High-level estimates (5 min)
   â†’ Concurrent users, Bandwidth, Servers
   â†’ "Let me do some quick napkin math..."

3. Architecture choice (10 min)
   â†’ P2P vs SFU vs MCU
   â†’ WebRTC, signaling, TURN/STUN
   â†’ "For this scale, we need SFU with simulcast..."

4. Deep dives (20 min)
   â†’ Recording, scaling, multi-region
   â†’ "TURN is expensive, so we optimize for P2P success..."

SANITY CHECKS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Can 1 SFU handle 300 participants? YES (network-bound)
âœ“ Is 10 Mbps enough for HD call? YES (2.5 upload + 7.5 download)
âœ“ Can use P2P for groups? NO (nÂ² connections, upload limited)
âœ“ Is 11 PB storage for recordings reasonable? YES (Zoom scale)

âœ— Can single server handle 10K participants? NO (need SFU cluster)
âœ— Can ignore TURN servers? NO (10-15% of users need it)
âœ— Is 1080p the default? NO (most use 720p to save bandwidth)
âœ— Can single region serve global users? NO (latency >200ms)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ Final Challenge: Apply This Template

Pick one of these systems and practice the full estimation:

1. **Online Education Platform** (like Udemy Live)
2. **Telemedicine System** (HIPAA-compliant video)
3. **Gaming Voice Chat** (like Discord voice channels)
4. **Virtual Events Platform** (conferences, exhibitions)
5. **Customer Support Video** (1-on-1 support calls)

Use the blank template above and time yourself: **Can you complete it in 5 minutes?**

---

**Remember**:
> "In video systems, BANDWIDTH is king. Everything else follows from the bits per second!"

**Now go crush those interviews!** ğŸš€

---

*Created with the POWER technique: Principles â†’ Order â†’ Write â†’ Estimate â†’ Round*
*Perfect for: FAANG interviews, Video Platform Design, WebRTC Architecture discussions*
