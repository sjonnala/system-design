# ğŸ¯ LinkedIn Connections: Scale Estimation Masterclass

## The GRAPH Technique for Social Network Scale Math
**(G)raph properties â†’ (R)elationships â†’ (A)ggregates â†’ (P)erformance â†’ (H)ardware**

This framework extends the POWER technique specifically for **graph-based systems** like social networks.

---

## ğŸ“Š PART 1: Graph-Specific Scale Estimation

### LinkedIn-Scale Assumptions

| **Category** | **Metric** | **Value** | **Justification** |
|--------------|------------|-----------|-------------------|
| **User Base** | Total Users | 800M | LinkedIn actual scale (2024) |
| | Daily Active Users (DAU) | 200M | ~25% of total (professional network) |
| | Power Users | 50M | ~25% create 75% of activity |
| | Avg Connections/User | 500 | Professional network (smaller than Facebook) |
| **Time Distribution** | Peak Hours | 6 hours/day | Business hours concentration |
| | Peak Traffic Multiplier | 2.5x | Higher peak than consumer apps |
| **Connection Operations** | Requests sent/user/month | 5 | Conservative professional networking |
| | Acceptance Rate | 60% | Higher than stranger-social networks |
| | Read:Write Ratio | 50:1 | Viewing >> connecting |
| **Graph Properties** | Max Connection Degree | 30,000 | LinkedIn limit (power connectors) |
| | Avg Degree Separation | 3 | "3 degrees of separation" |
| | Graph Density | 0.00125% | Sparse graph (not everyone connected) |

---

## ğŸ§® PART 2: Graph Math Foundations

### Rule #1: **Edge Count Formula**
```
For an undirected graph (bidirectional connections):

Total Edges = (Total Users Ã— Avg Connections per User) Ã· 2

Why Ã· 2?
- Each connection is an edge between two nodes
- A-B connection is same as B-A connection
- Counting both would double-count

Example:
800M users Ã— 500 connections Ã· 2 = 200 Billion edges
```

### Rule #2: **Graph Storage Calculation**
```
Storage per Edge:
- user_id_1 (8 bytes, BIGINT)
- user_id_2 (8 bytes, BIGINT)
- connected_at (8 bytes, TIMESTAMP)
- metadata (26 bytes: status, etc.)
Total: ~50 bytes per edge

Neo4j overhead: ~2x (indexes, pointers) = 100 bytes per edge

Total Graph Storage = Edges Ã— Storage per Edge
200B edges Ã— 100 bytes = 20 TB (graph database)
```

### Rule #3: **Degree Distribution** (Power Law)
```
Social graphs follow power law distribution:
- 80% of users have 50-500 connections (normal)
- 15% of users have 500-2000 connections (active networkers)
- 5% of users have 2000-30,000 connections (influencers, recruiters)

This affects:
- Cache sizing (influencers need more cache)
- Query performance (high-degree nodes are hotspots)
- Sharding strategy (need balanced distribution)
```

---

## ğŸ“ˆ PART 3: LinkedIn Connections Scale Template

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ LINKEDIN CONNECTIONS - NAPKIN MATH SOLUTION         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 1: USER & GRAPH SCALE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Users:             800 M
Daily Active Users:      200 M (25%)
Avg Connections/User:    500
Max Connections/User:    30,000 (cap for power users)

â†’ Total Edges = 800M Ã— 500 Ã· 2     = 200 Billion edges
â†’ Avg Degree Separation             = 3 hops
â†’ Graph Density = Edges Ã· (NodesÂ²)  = 0.00125%
  (Sparse graph - good for traversal!)

STEP 2: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Connection Requests:
- Per user: 5 requests/month
- Total: 800M Ã— 5 = 4 Billion requests/month
- Daily: 4B Ã· 30 = 133M writes/day
â†’ Write QPS = 133M Ã· 100K = 1,500 writes/sec
â†’ Peak: 1,500 Ã— 2.5 = 3,750 writes/sec

Read Operations (view connections, recommendations):
- Per DAU: 20 reads/day (view profile, scroll connections)
- Total: 200M Ã— 20 = 4B reads/day
â†’ Read QPS = 4B Ã· 100K = 46,000 reads/sec  
â†’ Peak: 46K Ã— 2.5 = 115,000 reads/sec

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Graph Database (Neo4j):
- Nodes (Users): 800M Ã— 1KB = 800 GB
- Edges (Connections): 200B Ã— 100 bytes = 20 TB
- Indexes: 20% overhead = 4 TB
â†’ Total Graph Storage = 25 TB

Relational Database (PostgreSQL):
- Users table: 800M Ã— 2KB = 1.6 TB
- Connection requests: 1B active Ã— 200 bytes = 200 GB
- User activities (ML features): 10B rows Ã— 100 bytes = 1 TB
â†’ Total Relational = 3 TB

Total Primary Storage: 25TB + 3TB = 28 TB

STEP 4: MEMORY (CACHE) ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hot Users (20% generate 80% traffic): 160M users
Cache per user:
- 1st degree connections: 500 connections Ã— 20 bytes = 10 KB
- Recommendations (pre-computed): 100 recs Ã— 50 bytes = 5 KB
- Total per user: 15 KB

â†’ Cache Size = 160M Ã— 15 KB = 2.4 TB (Redis cluster)

STEP 5: BANDWIDTH ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Write Bandwidth:
- 1,500 writes/sec Ã— 500 bytes (request payload) = 750 KB/sec
- Peak: 750 KB Ã— 2.5 = 1.875 MB/sec (negligible)

Read Bandwidth:
- 46K reads/sec Ã— 10 KB (avg response) = 460 MB/sec
- Peak: 460 MB Ã— 2.5 = 1.15 GB/sec

â†’ Network: Multi-Gbps (10 Gbps uplinks)

STEP 6: ML MODEL STORAGE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training Data:
- User features: 800M users Ã— 5 KB = 4 TB
- Pairwise features: 1B candidate pairs Ã— 1 KB = 1 TB
- Historical interactions: 50B events Ã— 200 bytes = 10 TB
â†’ Total Training Data = 15 TB (S3)

Model Artifacts:
- LightGBM model: ~500 MB
- Feature transformers: ~200 MB
- Model registry (versioning): ~5 GB
â†’ Total Model Storage = 5 GB (negligible)
```

---

## ğŸ§  PART 4: Graph-Specific Mental Math Techniques

### **Technique 1: The Edge Halving Rule**
```
ğŸ¯ EMOTION TRIGGER: "Every friendship is counted twice!"

When calculating total edges in an undirected graph:
- User A â†’ B connection
- User B â†’ A connection (same edge!)
- Always divide by 2

Example:
800M users Ã— 500 connections = 400B 
But each edge counted twice â†’ 400B Ã· 2 = 200B edges âœ“
```

### **Technique 2: Power Law Distribution**
```
ğŸ¯ "80-15-5 Rule for Social Graphs"

Don't assume uniform distribution!
- 80% of users: 50-500 connections (normal)
- 15% of users: 500-2K connections (active)
- 5% of users: 2K-30K connections (influencers)

Cache accordingly:
- Influencers (5%) need 60% of cache
- Normal users (80%) share 40% of cache
```

### **Technique 3: BFS Complexity Estimation**
```
ğŸ¯ "Degree * Degree * Degree = Explosion"

For calculating degrees of separation (BFS):

1st degree: 500 connections
2nd degree: 500 Ã— 500 = 250,000 users
3rd degree: 500 Ã— 500 Ã— 500 = 125,000,000 users!

This is why:
- Limit graph traversal to 3 degrees max
- Use bidirectional BFS (search from both ends)
- Cache popular paths aggressively
```

### **Technique 4: Graph Query Cost Estimation**
```
Query cost â‰ˆ Nodes visited Ã— Avg degree

1st degree query (direct connections):
- Nodes: 1
- Edges: 500
- Cost: O(500) = Very fast (10-50ms)

2nd degree query:
- Nodes: 1 + 500 = 501
- Edges: 500 Ã— 500 = 250K
- Cost: O(250K) = Moderate (50-200ms)

3rd degree query:
- Nodes: 1 + 500 + 250K = 250,501
- Edges: 250K Ã— 500 = 125M (EXPLOSION!)
- Cost: O(125M) = Expensive (500ms-2s)

â†’ Cache 2nd/3rd degree results aggressively!
```

---

## ğŸ¨ PART 5: Visual Graph Mind Map

```
                    ğŸ¤ LINKEDIN CONNECTIONS
                            |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                   |                   |
    ğŸ“Š GRAPH              ğŸ’¾ STORAGE          ğŸ”§ COMPUTE
        |                   |                   |
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”          â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  Nodes  Edges       Neo4j  Redis      Servers  ML
  800M   200B         25TB   2.4TB      200     Spark
```

**Memory Trigger**: **"N.E.S."** = Nodes, Edges, Storage

---

## ğŸ—ï¸ PART 6: Scale Math Practice Problems

### Problem 1: Facebook-Scale Social Graph

```
Given:
- 3 Billion users
- Average 300 friends per user
- 50 Billion posts per year
- Each post tagged with 3 friends avg
- Store posts for 10 years

Calculate:
1. Total graph edges (friendships)
2. Post storage requirements
3. Friend tag storage
4. Recommendation candidates (2nd degree)

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. GRAPH EDGES:
   - Total edges = 3B Ã— 300 Ã· 2 = 450 Billion edges
   - Storage (100 bytes/edge) = 45 TB

2. POST STORAGE (10 years):
   - Posts/year: 50B
   - Total posts: 50B Ã— 10 = 500B posts
   - Storage per post: 1KB (text, metadata)
   - Total: 500B Ã— 1KB = 500 TB

3. FRIEND TAG STORAGE:
   - Tags: 50B posts Ã— 3 tags = 150B tags/year
   - 10 years: 1.5 Trillion tags
   - Storage per tag: 20 bytes (post_id, user_id)
   - Total: 1.5T Ã— 20 bytes = 30 TB

4. RECOMMENDATION CANDIDATES (2nd degree):
   - Per user: 300 friends Ã— 300 = 90,000 2nd degree
   - But overlaps reduce to ~30,000 unique
   - Storage for candidates: 30K Ã— 50 bytes = 1.5 MB per user
   - For 100M active users: 150 TB (impractical!)
   
   â†’ Solution: Pre-compute top 100 recommendations only
   â†’ 100M Ã— 100 Ã— 50 bytes = 500 GB (reasonable!)
```
</details>

---

### Problem 2: Twitter-like Follow Graph (Directed)

```
Given:
- 500M users
- Average 500 followers per user
- Average 300 following per user
- Bi-directional (followers â‰  following)
- Track tweet impressions (who saw what)

Calculate:
1. Total edges (directed graph)
2. Asymmetry ratio
3. Storage for follower graph
4. Tweet fan-out calculation

[Try it yourself]
```

<details>
<summary>Answer</summary>

```
1. TOTAL EDGES (Directed Graph):
   - Followers: 500M Ã— 500 = 250 Billion edges
   - Following: 500M Ã— 300 = 150 Billion edges
   - Wait! These are the SAME edges (just different direction)
   - Total unique edges: 250B (undirected count)
   - But store both directions: 250B Ã— 2 = 500B directed edges
   - Storage: 500B Ã— 50 bytes = 25 TB

2. ASYMMETRY RATIO:
   - Avg followers: 500
   - Avg following: 300
   - Ratio: 500 Ã· 300 = 1.67
   - Interpretation: Users are followed more than they follow (influencer effect)

3. STORAGE FOR FOLLOWER GRAPH:
   - Adjacency list (out-edges): 250B Ã— 50 bytes = 12.5 TB
   - Reverse index (in-edges for fan-out): 12.5 TB
   - Total: 25 TB

4. TWEET FAN-OUT:
   - User tweets â†’ deliver to 500 followers avg
   - 1M tweets/sec (peak) Ã— 500 followers = 500M writes/sec!
   - This is why Twitter uses:
     â€¢ Fan-out on write (pre-compute timelines)
     â€¢ Hybrid approach for influencers (fan-out on read)
```
</details>

---

## ğŸ¯ PART 7: Graph Database Comparison

### Neo4j vs PostgreSQL: When to Use What?

| **Criteria** | **Neo4j (Graph DB)** | **PostgreSQL (Relational)** |
|--------------|----------------------|------------------------------|
| **Best For** | Relationship queries | Transactional data |
| **Query** | `MATCH (a)-[:CONNECTED*1..3]-(b)` | `SELECT ... FROM connections WHERE ...` |
| **1st Degree Query** | 10ms (native traversal) | 50ms (indexed lookup) |
| **2nd Degree Query** | 50ms (optimized traversal) | 500ms+ (multiple joins) |
| **3rd Degree Query** | 200ms (with caching) | Seconds (too slow!) |
| **Storage Overhead** | 2-3x (indexes, pointers) | 1x (normalized) |
| **Sharding** | Complex (cross-shard edges) | Easier (partition by user_id) |
| **ACID Compliance** | Yes (Causal Clustering) | Yes (standard) |
| **Use Case** | Graph traversal, recommendations | User profiles, metadata |

**Recommendation**: Use **both**!
- Neo4j: Social graph, degree calculations, mutual connections
- PostgreSQL: User data, connection requests, transactional state
- Redis: Caching layer for both

---

## ğŸš€ PART 8: Real-World Scale Numbers

### LinkedIn (Actual Numbers - Public Data)

| **Metric** | **Value** | **Calculation** |
|------------|-----------|-----------------|
| **Users** | 800M+ | - |
| **Connections** | 200B+ | 800M Ã— 500 Ã· 2 |
| **Profiles viewed/min** | 100K+ | - |
| **Connection requests/sec** | 1,000+ | Peak traffic |
| **Data storage** | 100+ PB | Includes posts, media, logs |
| **Graph DB size** | ~50 TB | Connection graph |
| **Hadoop cluster** | 10,000+ nodes | For analytics, ML |

### Facebook (For Comparison)

| **Metric** | **Value** | **Notes** |
|------------|-----------|-----------|
| **Users** | 3B+ | 4x LinkedIn |
| **Friendships** | 400B+ | 3B Ã— 300 Ã· 2 |
| **Daily active** | 2B | 67% DAU ratio |
| **Graph DB** | TAO (custom) | Distributed graph store |
| **Storage** | 1+ Exabyte | Includes photos, videos |

---

## ğŸ“ PART 9: Your Graph System Template

```
SYSTEM: ___________________

STEP 1: GRAPH TOPOLOGY
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total Users (Nodes):      [____] M
Avg Connections (Degree): [____]
Graph Type:               â–¡ Undirected (friendship)
                         â–¡ Directed (follow)
                         â–¡ Weighted (strength)

â†’ Total Edges = [____] M Ã— [____] Ã· 2 = [____] B

STEP 2: TRAFFIC ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DAU:                      [____] M
Operations per DAU:       [____]
Read:Write Ratio:         [____]:1

â†’ Write QPS = [____]
â†’ Read QPS  = [____]
â†’ Peak QPS  = [____]

STEP 3: STORAGE ESTIMATION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Graph Database:
- Nodes: [____] M Ã— [____] KB = [____] GB
- Edges: [____] B Ã— [____] bytes = [____] TB
- Indexes: [____]% overhead = [____] TB
â†’ Total Graph Storage = [____] TB

Relational Database:
- User metadata: [____] TB
- Transactions: [____] TB
â†’ Total Relational = [____] TB

STEP 4: CACHE SIZING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Hot Users (20%):          [____] M
Cache per user:           [____] KB
â†’ Cache Size = [____] M Ã— [____] KB = [____] TB

STEP 5: ML INFRASTRUCTURE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Training Data:            [____] TB
Model Size:               [____] GB
Prediction QPS:           [____]

STEP 6: GRAPH QUERY PERFORMANCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1st degree query:         < 50ms
2nd degree query:         < 200ms
3rd degree query:         < 500ms (with cache)

SMELL TEST:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¡ Edge count reasonable? (B to 100B range)
â–¡ Graph storage makes sense? (TB to 100TB)
â–¡ Cache hit rate achievable? (>80%)
â–¡ Query latencies realistic? (ms to sub-second)
```

---

## ğŸ PART 10: Graph Scale Cheat Sheet

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘         GRAPH SYSTEM DESIGN CHEAT SHEET                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GRAPH FORMULAS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Edges (undirected) = Nodes Ã— Avg_Degree Ã· 2
Graph Storage = Edges Ã— Storage_Per_Edge Ã— Overhead
BFS Explosion = Degree^Hops (exponential!)
Graph Density = Edges Ã· (Nodes Ã— (Nodes-1) Ã· 2)

TYPICAL GRAPH PROPERTIES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ Avg Degree: 100-1000 (social networks)
â€¢ Degree Distribution: Power law (80-15-5 rule)
â€¢ Avg Path Length: 3-6 hops (small world property)
â€¢ Graph Density: 0.001-0.01% (sparse)
â€¢ Clustering Coefficient: 0.1-0.3

STORAGE PER EDGE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Neo4j:        100 bytes (with overhead)
PostgreSQL:   50 bytes (normalized)
Redis:        20 bytes (cache, just IDs)

QUERY PERFORMANCE TARGETS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1st degree:   < 50ms  (direct neighbors)
2nd degree:   < 200ms (friends of friends)
3rd degree:   < 500ms (with aggressive caching)
Mutual:       < 100ms (intersection query)

SCALABILITY PATTERNS:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ“ Shard by user ID hash (consistent hashing)
âœ“ Replicate for reads (3-5x replicas)
âœ“ Cache hot paths aggressively (Redis)
âœ“ Denormalize for common queries (PostgreSQL backup)
âœ“ Async processing for heavy queries (Spark)

GRAPH DB ALTERNATIVES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Neo4j:       Most popular, Cypher query language
Dgraph:      GraphQL interface, distributed
AWS Neptune: Managed, Gremlin/SPARQL
JanusGraph:  Distributed, scalable
TigerGraph:  High-performance analytics

COMMON MISTAKES:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ— Forgetting to divide by 2 for undirected graphs
âœ— Not considering power law distribution
âœ— Underestimating 2nd/3rd degree explosion
âœ— Assuming uniform graph density
âœ— Not caching high-degree nodes
âœ— Trying 4+ degree queries (too expensive!)
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ”¥ PART 11: Advanced: Graph Partitioning Strategies

### Challenge: How to Shard a Graph?

**Problem**: Unlike tables, graph edges cross partition boundaries!

```
User A (Shard 1) --CONNECTED--> User B (Shard 2)
                  â†‘
           Cross-shard edge!
```

### Strategy 1: Hash Partitioning (Most Common)

```
Shard ID = hash(user_id) % num_shards

Pros:
âœ“ Even distribution
âœ“ Simple to implement
âœ“ Scales horizontally

Cons:
âœ— 80%+ queries hit multiple shards (cross-shard joins)
âœ— High network overhead
âœ— Complex transactions

Optimization:
- Most connections are "local" (friends of friends)
- Cache cross-shard edges aggressively
- Use async queries for non-critical paths
```

### Strategy 2: Community-Based Partitioning

```
Use graph clustering algorithms (Louvain, etc.) to group users:
- Users in same company â†’ same shard
- Users in same geography â†’ same shard
- Users with mutual connections â†’ same shard

Pros:
âœ“ 70-80% of queries stay within shard
âœ“ Better query performance

Cons:
âœ— Rebalancing is expensive
âœ— Hotspots (popular communities)
âœ— Complex to maintain

Used by: Facebook TAO, LinkedIn's graph DB
```

### Strategy 3: Hybrid Approach (Recommended)

```
1. Primary sharding: Hash-based (for even distribution)
2. Edge caching: Cache hot edges across shards (Redis)
3. Denormalization: Store critical edges in both directions
4. Routing layer: Smart query router minimizes cross-shard queries

Example:
- User's 1st degree connections: Stored in home shard + cache
- 2nd degree queries: Route to relevant shards in parallel
- Mutual connections: Pre-computed and cached
```

---

## ğŸ“š PART 12: ML-Specific Scale Considerations

### Feature Storage Explosion

```
Problem: Pairwise features for recommendations

Users: 800M
Candidate pairs: 800M Ã— 1000 (top candidates) = 800B pairs
Features per pair: 10 features Ã— 8 bytes = 80 bytes
Total: 800B Ã— 80 bytes = 64 TB!

Solutions:
1. Sparse representation (only non-zero features)
2. Feature hashing (reduce dimensionality)
3. On-demand computation (compute when needed, don't store)
4. Sampling (train on 10% sample, 6.4 TB)

LinkedIn approach: Combination of all 4
```

### Model Serving Latency

```
Requirement: Recommendations in <500ms

Calculation:
- Candidate generation: 100ms (graph query)
- Feature extraction: 150ms (100 candidates Ã— 1.5ms)
- Model inference: 50ms (LightGBM batch prediction)
- Ranking & formatting: 100ms
Total: 400ms âœ“ (fits budget!)

Optimizations:
- Pre-compute candidates (batch job)
- Cache feature vectors (Redis)
- Use fast model (LightGBM > Deep Learning)
- Batch inference (100 candidates at once)
```

---

## ğŸ¯ Final Challenge: Design Your Own Graph System

Pick one:
1. **Instagram**: Follower graph, feed generation, story viewers
2. **GitHub**: Code collaboration graph, stars, forks
3. **Spotify**: User-artist bipartite graph, playlist recommendations
4. **Stack Overflow**: User-question-answer tripartite graph

Use the template above and calculate:
- Graph topology (nodes, edges)
- Traffic (QPS)
- Storage (graph + cache)
- Query performance targets
- ML infrastructure needs

**Time yourself: Can you complete in 10 minutes?**

---

## ğŸ“– Key Takeaways

1. **Divide by 2**: Undirected graph edges counted twice
2. **Power Law**: 5% users = 60% cache needs
3. **BFS Explosion**: Degree^Hops grows exponentially
4. **Cache Aggressively**: Graph queries expensive, cache hot paths
5. **Hybrid Storage**: Graph DB + RDBMS + Cache = optimal
6. **Limit Traversal**: 3 degrees max, beyond that = too expensive
7. **ML is Expensive**: Feature storage >> model storage

---

**Remember**:
> "In graph systems, RELATIONSHIPS are first-class citizens. Design for edges, not just nodes!"

**Now go build the next LinkedIn!** ğŸš€

---

*Created with the GRAPH technique: Graph properties â†’ Relationships â†’ Aggregates â†’ Performance â†’ Hardware*
*Perfect for: Social networks, Recommendation systems, Knowledge graphs, Network analysis*
