# Consistent Hashing Deep Dive

## Contents

- [Consistent Hashing Deep Dive](#consistent-hashing-deep-dive)
    - [Core Mental Model](#core-mental-model)
    - [The Virtual Nodes Strategy (Load Balancing)](#2-the-virtual-nodes-strategy-load-balancing)
    - [Adding/Removing Nodes (Minimal Disruption)](#3-addingremoving-nodes-minimal-disruption)
    - [Replication & Fault Tolerance](#4-replication--fault-tolerance)
    - [Failure Modes & Edge Cases](#5-failure-modes--edge-cases)
    - [SYSTEM DESIGN INTERVIEW FRAMEWORK](#system-design-interview-framework)
        - [Requirements Clarification (RADIO: Requirements)](#1-requirements-clarification-radio-requirements)
        - [Capacity Estimation (RADIO: Scale)](#2-capacity-estimation-radio-scale)
        - [Data Model (RADIO: Data-Model)](#3-data-model-radio-data-model)
        - [High-Level Design (RADIO: Initial Design)](#4-high-level-design-radio-initial-design)
        - [Deep Dives (RADIO: Optimize)](#5-deep-dives-radio-optimize)
    - [MIND MAP: CONSISTENT HASHING CONCEPTS](#mind-map-consistent-hashing-concepts)

## Core Mental Model

```text
Problem: Naive Hashing
---------------------
server_index = hash(key) % N

Issue: When N changes (add/remove server), almost ALL keys remap!
- Add 1 server: (N â†’ N+1) â†’ ~100% keys move
- Remove 1 server: (N â†’ N-1) â†’ ~100% keys move

Solution: Consistent Hashing
---------------------------
Only K/N keys remap when servers change (K = total keys, N = server count)
- Add 1 server: Only ~1/N keys move
- Remove 1 server: Only ~1/N keys move
```

**The Key Insight:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Map both KEYS and SERVERS to same hash space (ring)     â”‚
â”‚ Each key walks clockwise to find its server             â”‚
â”‚ When server fails, only its keys move to next server    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mathematical Foundation:**
```
Hash Space: [0, 2^32 - 1] or [0, 2^64 - 1]
Ring Structure: Hash space wraps around (circular)

For key K and server S:
1. hash(K) â†’ position on ring
2. hash(S) â†’ position on ring
3. Assigned server = next server clockwise from hash(K)
```

**Visual Model:**
```text
         Hash Ring (0 to 2^32-1)

                  0/2^32
                    â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â•±                   â•²
    Node C                    Node A
    â†“                              â†“
   â•±                                â•²
  â”‚      Key1 â†’                      â”‚
  â”‚            Key2 â†’                â”‚
  â”‚                  Key3 â†’          â”‚
   â•²                                â•±
    â•²                              â•±
     Node B â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“

Key1 maps to Node A (next clockwise)
Key2 maps to Node A (next clockwise)
Key3 maps to Node B (next clockwise)
```

**Real World Implementation:**
```java
public class ConsistentHashRing<T> {

    private final TreeMap<Long, T> ring;  // Sorted map for O(log n) lookup
    private final HashFunction hashFunction;
    private final int virtualNodesPerServer;

    public ConsistentHashRing(int virtualNodesPerServer) {
        this.ring = new TreeMap<>();
        this.hashFunction = Hashing.murmur3_128();
        this.virtualNodesPerServer = virtualNodesPerServer;
    }

    /**
     * Add server with virtual nodes for better distribution
     */
    public void addServer(T server) {
        for (int i = 0; i < virtualNodesPerServer; i++) {
            // Create virtual node identifier
            String vnodeKey = server.toString() + "#" + i;
            long hash = hashFunction.hashString(vnodeKey, StandardCharsets.UTF_8)
                                   .asLong();
            ring.put(hash, server);
        }
    }

    /**
     * Remove server and its virtual nodes
     */
    public void removeServer(T server) {
        for (int i = 0; i < virtualNodesPerServer; i++) {
            String vnodeKey = server.toString() + "#" + i;
            long hash = hashFunction.hashString(vnodeKey, StandardCharsets.UTF_8)
                                   .asLong();
            ring.remove(hash);
        }
    }

    /**
     * Find server for a given key
     * Time: O(log n) where n = total virtual nodes
     */
    public T getServer(String key) {
        if (ring.isEmpty()) {
            return null;
        }

        long hash = hashFunction.hashString(key, StandardCharsets.UTF_8)
                               .asLong();

        // Find first server clockwise from hash
        Map.Entry<Long, T> entry = ring.ceilingEntry(hash);

        if (entry == null) {
            // Wrap around to first server
            entry = ring.firstEntry();
        }

        return entry.getValue();
    }

    /**
     * Get N replicas for a key (for fault tolerance)
     */
    public List<T> getReplicas(String key, int replicationFactor) {
        if (ring.isEmpty()) {
            return Collections.emptyList();
        }

        Set<T> replicas = new LinkedHashSet<>();
        long hash = hashFunction.hashString(key, StandardCharsets.UTF_8)
                               .asLong();

        // Walk clockwise collecting distinct servers
        NavigableMap<Long, T> tailMap = ring.tailMap(hash, true);

        for (T server : tailMap.values()) {
            replicas.add(server);
            if (replicas.size() == replicationFactor) {
                return new ArrayList<>(replicas);
            }
        }

        // Wrap around if needed
        for (T server : ring.values()) {
            replicas.add(server);
            if (replicas.size() == replicationFactor) {
                return new ArrayList<>(replicas);
            }
        }

        return new ArrayList<>(replicas);
    }
}
```

**Key Design Decision**: Use **TreeMap** (Red-Black tree) for O(log n) lookups instead of HashMap

---

## 2. **The Virtual Nodes Strategy** (Load Balancing)

ğŸ“ **PROFESSOR**: Without virtual nodes, you get **uneven distribution**:

```text
Problem: 3 servers, random hash positions
=========================================

         Ring with Real Nodes Only

          Node A owns 70% of ring!
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
         â•±          â•²
    Node C           Node A (70%)
    (10%)                â†“
   â•±                      â•²
  â”‚                        â”‚
  â”‚                        â”‚
   â•²                      â•±
    â•²                    â•±
     Node B â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        (20%)

Result: Massive load imbalance!
```

**Solution: Virtual Nodes (vnodes)**
```text
Each physical server â†’ 150-200 virtual nodes
These vnodes are evenly distributed on ring

         Ring with Virtual Nodes

          A#1  B#7  C#23  A#45  B#89
          â†“    â†“    â†“     â†“     â†“
         â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²
        â”‚   Evenly distributed     â”‚
        â”‚   across hash space      â”‚
         â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±

Result: Each server owns ~33% of ring
```

ğŸ—ï¸ **ARCHITECT**: Real-world numbers from DynamoDB:
```
Virtual Nodes Count Analysis:
- Too few (< 50): Poor distribution, hot spots
- Sweet spot (150-200): Good balance
- Too many (> 500): Memory overhead, slow rebalancing

DynamoDB uses 128 vnodes per physical node
Cassandra default: 256 vnodes per node
```

**Load Distribution Formula:**
```java
@Service
public class LoadBalancingAnalyzer {

    /**
     * Calculate standard deviation of load distribution
     * Lower is better (more uniform)
     */
    public double calculateLoadImbalance(ConsistentHashRing ring,
                                         List<String> keys) {
        Map<Server, Integer> loadCount = new HashMap<>();

        // Simulate key distribution
        for (String key : keys) {
            Server server = ring.getServer(key);
            loadCount.merge(server, 1, Integer::sum);
        }

        // Calculate standard deviation
        double mean = keys.size() / (double) loadCount.size();
        double variance = loadCount.values().stream()
            .mapToDouble(count -> Math.pow(count - mean, 2))
            .average()
            .orElse(0.0);

        return Math.sqrt(variance) / mean; // Coefficient of variation
    }

    /**
     * Test result:
     * - 0 vnodes: CV = 0.45 (45% imbalance) âŒ
     * - 50 vnodes: CV = 0.15 (15% imbalance) âš ï¸
     * - 150 vnodes: CV = 0.05 (5% imbalance) âœ…
     * - 500 vnodes: CV = 0.02 (2% imbalance) âœ… (but overhead!)
     */
}
```

**Interview talking point**: "Virtual nodes solve two problems: load balancing AND heterogeneous hardware. If server A has 2x capacity of server B, give it 2x virtual nodes."

---

## 3. **Adding/Removing Nodes** (Minimal Disruption)

ğŸ“ **PROFESSOR**: This is the **primary value proposition** of consistent hashing:

```text
Scenario: 4-node cluster, add 5th node
======================================

Before: Keys distributed among A, B, C, D
After: Only ~20% of keys move (1/5 of total)

Naive hashing: 80% keys move (almost everything!)
Consistent hashing: 20% keys move (only affected range)
```

**Visual: Adding a Node**
```text
BEFORE:
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â•±               â•²
    Node A              Node B
       â†“                   â†“
      â•±                     â•²
     â”‚   Keys mapped to      â”‚
     â”‚   A, B, C, D          â”‚
      â•²                     â•±
       â•²                   â•±
    Node D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Node C
       â†“                   â†“

AFTER: Add Node E between A and B
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â•±       E       â•²  â† New node
    Node A      â†“       Node B
       â†“                   â†“
      â•±                     â•²
     â”‚   Only keys in        â”‚
     â”‚   [A, E) move to E    â”‚  â† Minimal impact!
      â•²                     â•±
       â•²                   â•±
    Node D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Node C
```

**Production Implementation:**
```java
@Service
public class RingRebalancer {

    private final ConsistentHashRing<Server> ring;
    private final DataMigrationService migrationService;

    /**
     * Add server with gradual migration (zero downtime)
     */
    @Transactional
    public void addServerGradually(Server newServer) {
        // Step 1: Add to ring (starts receiving reads immediately)
        ring.addServer(newServer);

        // Step 2: Identify keys that need migration
        List<KeyRange> rangesToMigrate = calculateAffectedRanges(newServer);

        // Step 3: Background migration with rate limiting
        for (KeyRange range : rangesToMigrate) {
            migrationService.migrateRange(range, newServer,
                MigrationConfig.builder()
                    .rateLimit(1000)  // ops/sec
                    .verifyData(true)  // Double-write verification
                    .keepSourceCopy(true)  // Rollback safety
                    .build()
            );
        }

        // Step 4: Verify consistency
        migrationService.verifyMigration(newServer);

        // Step 5: Cleanup old copies
        migrationService.cleanupSourceData(rangesToMigrate);
    }

    /**
     * Remove server with zero data loss
     */
    @Transactional
    public void removeServerSafely(Server server) {
        // Step 1: Mark server as draining (no new writes)
        server.setState(ServerState.DRAINING);

        // Step 2: Find successor servers for each vnode
        Map<VirtualNode, Server> migrationPlan = new HashMap<>();
        for (VirtualNode vnode : server.getVirtualNodes()) {
            Server successor = ring.getNextServer(vnode);
            migrationPlan.put(vnode, successor);
        }

        // Step 3: Migrate data to successors
        migrationService.migrateToSuccessors(migrationPlan);

        // Step 4: Remove from ring only after migration complete
        ring.removeServer(server);

        // Step 5: Decommission
        server.setState(ServerState.DECOMMISSIONED);
    }

    /**
     * Calculate which key ranges are affected by new node
     */
    private List<KeyRange> calculateAffectedRanges(Server newServer) {
        List<KeyRange> ranges = new ArrayList<>();

        for (VirtualNode vnode : newServer.getVirtualNodes()) {
            long vnodeHash = vnode.getHash();

            // Find predecessor (who currently owns these keys)
            Server predecessor = ring.getPredecessorServer(vnodeHash);

            // Range from predecessor's position to new vnode position
            long predecessorHash = predecessor.getLastVnodeHash();
            ranges.add(new KeyRange(predecessorHash, vnodeHash, predecessor));
        }

        return ranges;
    }
}
```

**Real-world numbers (DynamoDB)**:
```
Adding 1 node to 100-node cluster:
- Keys to migrate: 1% of total dataset
- Migration time: ~2 hours for 1TB
- Impact: Zero downtime, reads continue normally
- Cost: Increased network I/O during migration
```

---

## 4. **Replication & Fault Tolerance**

ğŸ“ **PROFESSOR**: Consistent hashing naturally supports **N-way replication**:

```text
Replication Strategy: Walk clockwise N servers
=============================================

         Hash Ring

          Key X hashes here
               â†“
         â”Œâ”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”
        â•±             â•²
    Node A            Node B
    (Primary)         (Replica 1)
       â†“                   â†“
      â•±                     â•²
     â”‚                       â”‚
      â•²                     â•±
       â•²                   â•±
    Node D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Node C
  (Replica 3)         (Replica 2)
       â†“                   â†“

Key X stored on: A (primary), B, C, D (replicas)
Replication Factor = 4
```

ğŸ—ï¸ **ARCHITECT**: Real Cassandra implementation:
```java
@Service
public class ReplicatedConsistentHash {

    private final ConsistentHashRing<Node> ring;
    private final int replicationFactor;
    private final ConsistencyLevel readConsistency;
    private final ConsistencyLevel writeConsistency;

    /**
     * Write with tunable consistency
     */
    public WriteResult write(String key, String value) {
        List<Node> replicas = ring.getReplicas(key, replicationFactor);

        // Parallel write to all replicas
        List<CompletableFuture<Void>> futures = replicas.stream()
            .map(node -> CompletableFuture.runAsync(() ->
                node.write(key, value, System.currentTimeMillis())
            ))
            .collect(Collectors.toList());

        // Wait based on consistency level
        int requiredAcks = getRequiredAcks(writeConsistency);

        try {
            // Wait for N acks (where N = consistency requirement)
            CompletableFuture.anyOf(
                futures.subList(0, requiredAcks).toArray(new CompletableFuture[0])
            ).get(5, TimeUnit.SECONDS);

            return WriteResult.success();

        } catch (TimeoutException e) {
            // Write failed to meet consistency requirement
            return WriteResult.failure("Write timeout: only " +
                countCompleted(futures) + "/" + requiredAcks + " acks");
        }
    }

    /**
     * Read with tunable consistency
     */
    public ReadResult read(String key) {
        List<Node> replicas = ring.getReplicas(key, replicationFactor);
        int requiredReads = getRequiredReads(readConsistency);

        // Read from N replicas in parallel
        List<CompletableFuture<VersionedValue>> futures = replicas.stream()
            .limit(requiredReads)
            .map(node -> CompletableFuture.supplyAsync(() -> node.read(key)))
            .collect(Collectors.toList());

        try {
            List<VersionedValue> results = futures.stream()
                .map(f -> f.get(1, TimeUnit.SECONDS))
                .collect(Collectors.toList());

            // Return value with highest timestamp (last-write-wins)
            return results.stream()
                .max(Comparator.comparing(VersionedValue::getTimestamp))
                .map(ReadResult::success)
                .orElse(ReadResult.notFound());

        } catch (Exception e) {
            return ReadResult.failure("Read timeout");
        }
    }

    private int getRequiredAcks(ConsistencyLevel level) {
        return switch (level) {
            case ONE -> 1;
            case QUORUM -> (replicationFactor / 2) + 1;  // Majority
            case ALL -> replicationFactor;
        };
    }

    private int getRequiredReads(ConsistencyLevel level) {
        return switch (level) {
            case ONE -> 1;
            case QUORUM -> (replicationFactor / 2) + 1;
            case ALL -> replicationFactor;
        };
    }
}
```

**Consistency Trade-offs:**
```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Consistency Level vs Performance & Availability      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ONE:    Fast, high availability, may read stale      â”‚
â”‚ QUORUM: Balanced (recommended)                       â”‚
â”‚ ALL:    Slow, low availability, always consistent    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Formula: W + R > N guarantees strong consistency
Where: W = write replicas, R = read replicas, N = total replicas

Example: N=3, W=2, R=2 â†’ Strong consistency
         (2 + 2 = 4 > 3, guaranteed overlap)
```

**Interview gold**: "In distributed systems, we can't have both strong consistency AND high availability during partitions (CAP theorem). Consistent hashing with tunable consistency lets us choose per-operation."

---

## 5. **Failure Modes & Edge Cases**

ğŸ“ **PROFESSOR**: Real-world consistent hashing faces several challenges:

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Cascading Failures     â†’ Use circuit breakers    â”‚
â”‚ 2. Hot Spots (celebrity)  â†’ Shard hot keys          â”‚
â”‚ 3. Hash Collisions        â†’ Use high-quality hash   â”‚
â”‚ 4. Network Partitions     â†’ Vector clocks           â”‚
â”‚ 5. Slow Nodes             â†’ Adaptive timeouts       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**1. Cascading Failure Scenario:**
```text
Problem: Node failure â†’ keys move to next node â†’ overload â†’ failure
========

   Node A fails
       â†“
   Keys move to Node B
       â†“
   Node B overloaded (2x traffic)
       â†“
   Node B fails
       â†“
   Keys move to Node C
       â†“
   ENTIRE CLUSTER COLLAPSES! ğŸ’¥
```

**Solution: Circuit Breaker + Rate Limiting**
```java
@Service
public class ResilientConsistentHash {

    private final ConsistentHashRing<Node> ring;
    private final LoadingCache<Node, CircuitBreaker> circuitBreakers;

    /**
     * Get server with circuit breaker protection
     */
    public Node getServerSafely(String key) {
        Node primary = ring.getServer(key);
        CircuitBreaker breaker = circuitBreakers.get(primary);

        if (breaker.isOpen()) {
            // Primary is failing, try replica
            log.warn("Primary {} circuit open, using replica", primary);
            return ring.getNextServer(key);  // Fallback to replica
        }

        return primary;
    }

    /**
     * Write with automatic failover
     */
    @CircuitBreaker(name = "write", fallbackMethod = "writeToReplica")
    @RateLimiter(name = "write")  // Protect against overload
    public void write(String key, String value) {
        Node node = getServerSafely(key);

        try {
            node.write(key, value);
        } catch (Exception e) {
            // Circuit breaker will open after N failures
            throw new NodeUnavailableException("Write failed to " + node, e);
        }
    }

    /**
     * Fallback: write to all replicas
     */
    public void writeToReplica(String key, String value, Exception e) {
        log.warn("Primary write failed, attempting replicas", e);

        List<Node> replicas = ring.getReplicas(key, 3);
        for (Node replica : replicas) {
            try {
                replica.write(key, value);
                return;  // Success!
            } catch (Exception ex) {
                log.error("Replica write failed: {}", replica, ex);
            }
        }

        throw new AllReplicasFailedException("All replicas unavailable for " + key);
    }
}
```

**2. Hot Spot (Celebrity Problem):**
```text
Scenario: Viral tweet by Taylor Swift
Key: "tweet:12345" â†’ all traffic to one server

Solution: Shard hot keys
tweet:12345:shard:0  â†’ Server A
tweet:12345:shard:1  â†’ Server B
tweet:12345:shard:2  â†’ Server C
tweet:12345:shard:3  â†’ Server D
```

```java
public class HotKeyHandler {

    private final LoadingCache<String, Long> requestCounter;
    private final int hotKeyThreshold = 10_000;  // req/sec

    /**
     * Detect and shard hot keys
     */
    public String getKey(String originalKey) {
        long requestRate = requestCounter.get(originalKey);

        if (requestRate > hotKeyThreshold) {
            // Shard across multiple servers
            int shardCount = (int) (requestRate / hotKeyThreshold) + 1;
            int shard = ThreadLocalRandom.current().nextInt(shardCount);
            return originalKey + ":shard:" + shard;
        }

        return originalKey;
    }
}
```

**3. Network Partition (Split Brain):**
```text
Problem: Network splits cluster into two groups
=========

    Group A (minority)        Group B (majority)
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Node 1   â”‚             â”‚ Node 2   â”‚
    â”‚ Node 2   â”‚ â—„â”€â”€âœ—â”€â”€â–º     â”‚ Node 3   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚ Node 4   â”‚
                             â”‚ Node 5   â”‚
                             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Both groups think they're primary!
```

**Solution: Quorum-based writes**
```java
public class QuorumConsistentHash {

    /**
     * Only allow writes if we can reach quorum
     */
    public WriteResult writeWithQuorum(String key, String value) {
        List<Node> replicas = ring.getReplicas(key, 5);
        int quorum = (replicas.size() / 2) + 1;  // 3 out of 5

        List<Node> reachableNodes = replicas.stream()
            .filter(Node::isReachable)
            .collect(Collectors.toList());

        if (reachableNodes.size() < quorum) {
            // Cannot guarantee consistency, reject write
            throw new QuorumNotMetException(
                "Only " + reachableNodes.size() + "/" + quorum + " nodes reachable"
            );
        }

        // Proceed with write
        return writeToNodes(reachableNodes, key, value);
    }
}
```

---

## ğŸ¯ **SYSTEM DESIGN INTERVIEW FRAMEWORK**

When asked to *"Design a distributed cache"* or *"Design a key-value store"*, use consistent hashing:

### 1. Requirements Clarification (RADIO: Requirements)
```
Functional:
- Store/retrieve key-value pairs
- Support for distributed nodes
- Handle node failures gracefully
- Support dynamic scaling (add/remove nodes)

Non-Functional:
- High availability: 99.99%
- Low latency: P99 < 10ms
- Scale: 1M operations/sec
- Data durability: replicate across 3 nodes
```

### 2. Capacity Estimation (RADIO: Scale)
```
Traffic:
- 1M reads/sec, 100K writes/sec
- Read:Write ratio = 10:1

Storage:
- 1B keys
- Average value size: 1KB
- Total: 1TB raw data
- With 3x replication: 3TB total
- Per node (100 nodes): 30GB

Memory for hash ring:
- 100 physical nodes
- 150 vnodes per node = 15,000 vnodes
- TreeMap overhead: ~15,000 * 40 bytes = 600KB (negligible!)
```

### 3. Data Model (RADIO: Data Model)
```java
// Core domain model

@Entity
public class HashRingNode {
    private String nodeId;
    private String ipAddress;
    private int port;
    private NodeState state;  // ACTIVE, DRAINING, DOWN
    private long lastHeartbeat;
    private int virtualNodeCount;
}

@Entity
public class VirtualNode {
    private long hash;
    private String physicalNodeId;
    private KeyRange ownedRange;  // [startHash, endHash)
}

@Entity
public class StoredValue {
    private String key;
    private byte[] value;
    private long version;        // For conflict resolution
    private long timestamp;      // Last-write-wins
    private List<String> replicas;  // Where is this replicated
}

@Entity
public class ReplicationLog {
    private long sequence;
    private String key;
    private Operation operation;  // WRITE, DELETE
    private long timestamp;
    private String sourceNode;
}
```

### 4. High-Level Design (RADIO: Initial Design)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CLIENT                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Coordinator Node   â”‚  â† Any node can coordinate
      â”‚  (Consistent Hash)   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Determine target nodes    â”‚
    â”‚  using consistent hashing  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Replicate to N nodes      â”‚
    â”‚  (walk clockwise on ring)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Hash Ring (Virtual)        â”‚
    â”‚                                   â”‚
    â”‚  Node1#0  Node2#0  Node3#0        â”‚
    â”‚  Node1#1  Node2#1  Node3#1        â”‚
    â”‚  ...      ...      ...            â”‚
    â”‚  Node1#149 Node2#149 Node3#149    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Physical Storage Nodes   â”‚
    â”‚                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”       â”‚
    â”‚  â”‚Node 1â”‚  â”‚Node 2â”‚  ...  â”‚
    â”‚  â”‚ KV   â”‚  â”‚ KV   â”‚       â”‚
    â”‚  â”‚Store â”‚  â”‚Store â”‚       â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. Deep Dives (RADIO: Optimize)

**A. Hash Function Selection**
```java
// Bad: Simple modulo
int nodeIndex = key.hashCode() % nodeCount;  // âŒ Poor distribution

// Good: Cryptographic hash (but slower)
MessageDigest md5 = MessageDigest.getInstance("MD5");
byte[] hash = md5.digest(key.getBytes());

// Best: Fast non-crypto hash (MurmurHash, xxHash)
HashFunction murmur = Hashing.murmur3_128();
long hash = murmur.hashString(key, StandardCharsets.UTF_8).asLong();
```

**Performance:**
```
MD5:         ~200 MB/s
SHA-256:     ~150 MB/s
MurmurHash3: ~2500 MB/s  âœ… (12x faster!)
xxHash:      ~5000 MB/s  âœ… (25x faster!)
```

**B. Virtual Node Count Optimization**
```python
import numpy as np
import matplotlib.pyplot as plt

def simulate_load_distribution(num_servers, num_vnodes, num_keys=1_000_000):
    """
    Simulate key distribution with different vnode counts
    """
    ring = {}
    for server in range(num_servers):
        for vnode in range(num_vnodes):
            hash_val = hash(f"server_{server}_vnode_{vnode}")
            ring[hash_val] = server

    sorted_hashes = sorted(ring.keys())

    # Distribute keys
    server_load = [0] * num_servers
    for key in range(num_keys):
        key_hash = hash(f"key_{key}")
        # Find next server clockwise
        idx = np.searchsorted(sorted_hashes, key_hash)
        if idx == len(sorted_hashes):
            idx = 0
        server = ring[sorted_hashes[idx]]
        server_load[server] += 1

    # Calculate coefficient of variation
    mean = np.mean(server_load)
    std = np.std(server_load)
    cv = std / mean

    return cv

# Test different vnode counts
vnode_counts = [1, 10, 50, 100, 150, 200, 300, 500]
cvs = [simulate_load_distribution(100, vnodes) for vnodes in vnode_counts]

# Result: Sweet spot at 150-200 vnodes
# CV < 0.05 (5% imbalance)
```

**C. Monitoring & Observability**
```java
@Component
public class ConsistentHashMetrics {

    private final MeterRegistry registry;

    /**
     * Key metrics to track
     */
    public void recordMetrics(ConsistentHashRing ring) {
        // 1. Ring size
        registry.gauge("hash_ring.size", ring.getVirtualNodeCount());

        // 2. Load distribution (standard deviation)
        double loadImbalance = calculateLoadImbalance(ring);
        registry.gauge("hash_ring.load_imbalance", loadImbalance);

        // 3. Rebalancing events
        registry.counter("hash_ring.rebalance.total").increment();

        // 4. Key migration count
        registry.counter("hash_ring.keys_migrated", "reason", "node_added")
                .increment(migratedKeyCount);

        // 5. Lookup latency
        registry.timer("hash_ring.lookup.latency").record(() -> {
            ring.getServer(randomKey);
        });
    }

    /**
     * Alerts to configure:
     * - Load imbalance > 15% (too much variance)
     * - Node down for > 1 minute
     * - Migration taking > 4 hours
     * - Lookup latency P99 > 5ms
     */
}
```

---

## ğŸ§  **MIND MAP: CONSISTENT HASHING CONCEPTS**
```
           Consistent Hashing
                  |
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â†“           â†“           â†“
   CORE        VIRTUAL    REPLICATION
   CONCEPT      NODES
      |           |           |
  â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”´â”€â”€â”    â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  â†“       â†“   â†“     â†“    â†“       â†“
Ring   Minimal Load  Even Primary Replicas
      Movement Balance Dist.    (N-way)
  |       |      |      |        |
Hash   K/N    150-200  TreeMap  Quorum
Space  Keys   vnodes   O(log n) Writes
  |     Move           |         |
0-2^64  When     Murmur3    W+R>N
      Add/Remove  Hash   Consistency
```

### ğŸ’¡ **EMOTIONAL ANCHORS (For Subconscious Power)**

Create strong memories with these analogies:

1. **Consistent Hashing = Clock Face Seating ğŸ•**
   - 12 seats around a table (hash ring)
   - People arrive with assigned numbers (hash value)
   - Sit at next available seat clockwise
   - When someone leaves, only their neighbor shifts over
   - NOT everyone reshuffling chairs!

2. **Virtual Nodes = Multiple Placeholders ğŸ«**
   - You're at a buffet with friends
   - Instead of one person holding one spot in line
   - Each person puts 10 placeholders throughout the line
   - Ensures everyone gets fair share of each dish
   - NOT one person stuck at the salad section!

3. **Replication = Backup Singers ğŸ¤**
   - Main singer (primary) gets the spotlight
   - But 3 backup singers (replicas) know all the words
   - If main singer fails, backup takes over seamlessly
   - Audience doesn't notice (high availability!)

4. **Ring Rebalancing = Highway Lane Opening ğŸš—**
   - Traffic flows in circle (ring)
   - New lane opens (new node added)
   - Only cars near the merge point switch lanes
   - NOT entire highway rerouting!

---

## ğŸ“š **REAL-WORLD USAGE**

**Systems using Consistent Hashing:**

1. **Amazon DynamoDB**
   - Uses consistent hashing for partition distribution
   - Virtual nodes for load balancing
   - Quorum writes (W+R>N)

2. **Apache Cassandra**
   - 256 virtual nodes per physical node (default)
   - Tunable consistency levels
   - Token-based partitioning

3. **Memcached Clients**
   - ketama algorithm (consistent hashing variant)
   - Minimize cache miss on server addition

4. **CDN Edge Routing**
   - Content placement across edge nodes
   - Minimize cache invalidation on topology changes

5. **Load Balancers**
   - Session affinity without state
   - Graceful server addition/removal

---

## ğŸ¤ **INTERVIEW TALKING POINTS**

**When to mention consistent hashing:**
- "We need to distribute data across multiple servers"
- "We want to scale horizontally by adding nodes"
- "We need session affinity without sticky load balancers"
- "We want to minimize cache invalidation during scaling"

**Red flags to address:**
- "Naive modulo hashing causes massive reshuffling"
- "We need to handle hot spots (celebrity problem)"
- "Hash function quality matters (MurmurHash > MD5)"
- "Virtual nodes prevent load imbalance"
- "Replication strategy: walk clockwise for N replicas"

**Advanced points (senior level):**
- "We can use weighted virtual nodes for heterogeneous hardware"
- "Quorum writes (W+R>N) guarantee strong consistency"
- "Circuit breakers prevent cascading failures"
- "Monitor load distribution with coefficient of variation"
- "Consider bounded loads variant for tighter load bounds"
